{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjJCdMyNlW_S"
   },
   "source": [
    "# Test Harness Jupyter Notebook for [thesis name here]\n",
    "\n",
    "## *About*\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VsBm6eHlW_T"
   },
   "source": [
    "# Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest (DFC) 2018 Univerisity of Houston Dataset\n",
    "\n",
    "## *LiDAR*\n",
    "- **Lidar sensor**: Optech Titan MW (14SEN/CON340) w/ integrated camera (This is a multispectral lidar sensor)\n",
    "- **Ground Sampling Distance (GSD)**: 0.5 meters (for point clouds, multispectral intensity, DSM, and DEM)\n",
    "- **Lidar MS Channel #1 (C1)**: 1550nm wavelength (near infrared)\n",
    "- **Lidar MS Channel #2 (C2)**: 1064nm wavelength (near infrared)\n",
    "- **Lidar MS Channel #3 (C3)**: 532nm wavelength (green)\n",
    "- **DSM_C12**: a first surface model (DSM) generated from first returns detected on Titan channels 1 and 2. The elevations of the first returns were interpolated to a 50cm (0.5m) grid using Kriging with a search radius of 5 meters.\n",
    "- **DEM_C123_3msr**: a bare-earth digital elevation model (DEM) generated from returns classified as ground from all three Titan channels. The elevations of the first returns were interpolated to a 50cm (0.5m) grid using Kriging with a search radius of 3 meters. This model represents terrain with data voids within the footprints of buildings and other manmade structures.\n",
    "- **DEM_C123_TLI**: a bare-earth DEM generated from returns classified as ground from all three Titan channels. The elevations of the first returns were interpolated to a 50cm (0.5m) grid using a *triangulation with linear interpolation* algorithm. This model represents terrain where the voids of footprints of buildings and other manmade structures have been filled by the algorithm.\n",
    "- **DEM+B_C123**: a hybrid DEM that combines returns that were classified as coming from buildings and the ground detected in all three Titan channels. The elevations of the returns were interpolated to a 50cm (0.5m) grid using Kriging with a search radius of 5 meters.\n",
    "\n",
    "## *Hyperspectral*\n",
    "- **Hyperspectral Image (HSI) sensor**: ITRES CASI 1500\n",
    "- **Ground Sampling Distance (GSD)**: 1 meter\n",
    "- **Number of spectral bands**: 48\n",
    "- **Spectral range of data**: 380 - 1050nm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqxxA8MclW_U"
   },
   "source": [
    "# --- TO-DO List ---\n",
    "- Create thresholding filter for lidar intensity raster and DSM data\n",
    "    - In the contest paper \"Those pixel values that are greater than a threshold T are replaced with the minimum value in the data. We set T as 1*e*4 and 1*e*10 for LiDAR intensity raster data and DSM data, respectively\"\n",
    "- Create normalized DSM (NDSM) to feed to algorithm (NDSM = DSM - DEM)\n",
    "- For all data (Hyperspectral, MS LiDAR, NDSM) normalize each feature dimension into range of \\[0,1\\]\n",
    "- Contest winner conducted image partitioning, partitioning the main image into 40 sub-images with a size of 1202x300\n",
    "    - During the test phase, there was no need to restore the gradient of the network anymore so the full test image is partitioned into 15 sub-images with a size of 2404 x 600\n",
    "- Image blurring (smoothing) w/ Gaussian filter?\n",
    "- In EDA, do histograms of Hyperspectral and lidar values, make it interactive to do over all classes as well as overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZceZKzllW_U"
   },
   "source": [
    "# 1) Install/import required libraries and set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "nnLvLU2TzZlQ"
   },
   "outputs": [],
   "source": [
    "#@title Install GDAL, Rasterio, and Spectral libraries\n",
    "# Install GDAL\n",
    "#!apt install gdal-bin python-gdal python3-gdal\n",
    "\n",
    "# Install Rasterio\n",
    "#!pip install rasterio\n",
    "\n",
    "# Install Spectral (do I need git clone?)\n",
    "#!pip install spectral\n",
    "#!git clone https://github.com/spectralpython/spectral.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RUzAeV1e4Ma8"
   },
   "outputs": [],
   "source": [
    "#@title Import libraries to notebook\n",
    "### Standard Python Libraries ###\n",
    "import argparse\n",
    "import collections\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import gc\n",
    "import math\n",
    "from operator import truediv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "### Data Manipulation Libaries ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "### Other Library Imports ###\n",
    "import cpuinfo\n",
    "\n",
    "### Band Selection Libraries ###\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import cvxpy as cvx\n",
    "from munkres import Munkres\n",
    "import nimfa\n",
    "from numpy import linalg\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "from scipy.special import expit\n",
    "from scipy.sparse.linalg import svds\n",
    "from skfeature.function.sparse_learning_based import NDFS\n",
    "from skfeature.function.similarity_based import lap_score, SPEC\n",
    "from skfeature.utility import construct_W\n",
    "from skfeature.utility.sparse_learning import feature_ranking\n",
    "from sklearn import cluster\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.linear_model import orthogonal_mp_gram, OrthogonalMatchingPursuit\n",
    "from sklearn.metrics import accuracy_score, pairwise_distances\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.preprocessing import maxabs_scale, normalize\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "### Hyperspectral Libaries ###\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.windows import Window\n",
    "import spectral\n",
    "\n",
    "### Data Visualization Libraries ###\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import Jupyter NB widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "### Machine Learning Libraries ###\n",
    "import scipy.io as sio\n",
    "from sklearn import metrics, preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.callbacks as kcallbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateScheduler,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Add,\n",
    "    AveragePooling2D,\n",
    "    AveragePooling3D,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Conv3D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling2D,\n",
    "    GlobalAveragePooling3D,\n",
    "    Input,\n",
    "    MaxPooling2D,\n",
    "    MaxPooling3D,\n",
    "    Reshape,\n",
    "    Resizing,\n",
    ")\n",
    "from tensorflow.keras.models import (\n",
    "    Model,\n",
    "    Sequential,\n",
    ")\n",
    "from tensorflow.keras.optimizers import (\n",
    "    Adadelta,\n",
    "    Adagrad,\n",
    "    Adam,\n",
    "    Adamax,\n",
    "    Ftrl,\n",
    "    Nadam,\n",
    "    RMSprop,\n",
    "    SGD,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import (\n",
    "    Sequence,\n",
    "    to_categorical,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ttB0qX7Q5Y12"
   },
   "outputs": [],
   "source": [
    "#@title Environment Setup\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gfd_4UvJlW_W"
   },
   "source": [
    "# 2) Dataset Setup Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRCNnZsUlW_W"
   },
   "source": [
    "## 2.1) Dataset Setup Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Sux8XLUdlW_W"
   },
   "outputs": [],
   "source": [
    "#@title Pad Image\n",
    "def pad_img(img, pad_width, ignore_dims=[]):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    padding = [(pad_width,) if dim not in ignore_dims else (0,) for dim in range(img.ndim)]\n",
    "    img = np.pad(img, padding, mode='constant')\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "K7hiyMO4lW_W"
   },
   "outputs": [],
   "source": [
    "#@title Threshold Image\n",
    "def threshold_image(img, threshold):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    img = img[img > threshold] = img.min()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QQ7CLYTXlW_X"
   },
   "outputs": [],
   "source": [
    "#@title Normalize Image\n",
    "def normalize_image(img):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    img = img.astype(float, copy=False)\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1RowWJDMlW_X"
   },
   "outputs": [],
   "source": [
    "#@title Ground Truth Sampling\n",
    "def sample_gt(gt, train_size, mode='random'):\n",
    "    \"\"\"Extract a fixed percentage of samples from an array of labels.\n",
    "\n",
    "    Args:\n",
    "        gt: a 2D array of int labels\n",
    "        percentage: [0, 1] float\n",
    "    Returns:\n",
    "        train_gt, test_gt: 2D arrays of int labels\n",
    "\n",
    "    \"\"\"\n",
    "    indices = np.nonzero(gt)\n",
    "    X = list(zip(*indices)) # x,y features\n",
    "    y = gt[indices].ravel() # classes\n",
    "    train_gt = np.zeros_like(gt)\n",
    "    test_gt = np.zeros_like(gt)\n",
    "    if train_size > 1:\n",
    "       train_size = int(train_size)\n",
    "\n",
    "    if mode == 'random':\n",
    "       train_indices, test_indices = train_test_split(X, train_size=train_size, stratify=y)\n",
    "       train_indices = [list(t) for t in zip(*train_indices)]\n",
    "       test_indices = [list(t) for t in zip(*test_indices)]\n",
    "       train_gt[train_indices] = gt[train_indices]\n",
    "       test_gt[test_indices] = gt[test_indices]\n",
    "    elif mode == 'fixed':\n",
    "       print(f'Sampling {mode} with train size = {train_size}')\n",
    "       train_indices, test_indices = [], []\n",
    "       for c in np.unique(gt):\n",
    "           if c == 0:\n",
    "              continue\n",
    "           indices = np.nonzero(gt == c)\n",
    "           X = list(zip(*indices)) # x,y features\n",
    "\n",
    "           train, test = train_test_split(X, train_size=train_size)\n",
    "           train_indices += train\n",
    "           test_indices += test\n",
    "       train_indices = tuple([list(t) for t in zip(*train_indices)])\n",
    "       test_indices = tuple([list(t) for t in zip(*test_indices)])\n",
    "       train_gt[train_indices] = gt[train_indices]\n",
    "       test_gt[test_indices] = gt[test_indices]\n",
    "\n",
    "    elif mode == 'disjoint':\n",
    "        train_gt = np.copy(gt)\n",
    "        test_gt = np.copy(gt)\n",
    "        for c in np.unique(gt):\n",
    "            mask = gt == c\n",
    "            for x in range(gt.shape[0]):\n",
    "                first_half_count = np.count_nonzero(mask[:x, :])\n",
    "                second_half_count = np.count_nonzero(mask[x:, :])\n",
    "                try:\n",
    "                    ratio = first_half_count / (first_half_count + second_half_count)\n",
    "                    if ratio > 0.9 * train_size:\n",
    "                        break\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "            mask[:x, :] = 0\n",
    "            train_gt[mask] = 0\n",
    "\n",
    "        test_gt[train_gt > 0] = 0\n",
    "    else:\n",
    "        raise ValueError(f'{mode} sampling is not implemented yet.')\n",
    "    return train_gt, test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DqOVv_Z6lW_X"
   },
   "outputs": [],
   "source": [
    "#@title Get Valid Ground Truth Indicies\n",
    "def get_valid_gt_indices(gt, ignored_labels=[]):\n",
    "\n",
    "    mask = np.ones_like(gt)\n",
    "    for label in ignored_labels:\n",
    "        mask[gt == label] = 0\n",
    "\n",
    "    x_pos, y_pos = np.nonzero(mask)\n",
    "    indices = np.array([(x, y) for x, y in zip(x_pos, y_pos)])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JMYnoF03lW_X"
   },
   "outputs": [],
   "source": [
    "#@title Histogram Equalization\n",
    "def histogram_equalization(img):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # def _equalize_layer(layer):\n",
    "    #     \"\"\"\n",
    "    #     Internal function to equalize a single image channel slice.\n",
    "    #     \"\"\"\n",
    "    #     height, width = layer.shape\n",
    "    #     h, bin = np.histogram(layer.flatten(), 256, [0, 256])\n",
    "\n",
    "    #     cdf = np.cumsum(h)\n",
    "\n",
    "    #     cdf_m = np.ma.masked_equal(cdf,0)\n",
    "    #     cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\n",
    "    #     cdf_final = np.ma.filled(cdf_m,0).astype('uint8')\n",
    "\n",
    "    # if img.ndim == 2:\n",
    "    #     return cv2.equalizeHist(img)\n",
    "    # else:\n",
    "    #     for channel in range(img.shape[-1]):\n",
    "    #         img[:,:, channel] = cv2.equalizeHist(img[:,:,channel])\n",
    "    #     return img\n",
    "        #return cv2.equalizeHist(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XEhP9JUelW_Y"
   },
   "outputs": [],
   "source": [
    "#@title Image Filtering\n",
    "def filter_image(img, filter_type, filter_size=None, normalize=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    failure = False\n",
    "\n",
    "    if img.ndim == 2:\n",
    "        height, width = img.shape\n",
    "        channels = 1\n",
    "    else:\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "    if filter_type == 'median':\n",
    "        if filter_size is None: filter_size = (3, 3, channels)\n",
    "        img = median_filter(img, size=filter_size)\n",
    "    elif filter_type == 'gaussian':\n",
    "        img = gaussian_filter(img, 1)\n",
    "    else:\n",
    "        print(f'Bad filter argument {filter_type}! Image left alone...')\n",
    "        failure = True\n",
    "\n",
    "    if not failure and normalize:\n",
    "        img=normalize_image(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuU02ktqlW_Y"
   },
   "source": [
    "## 2.2) Hyperspectral Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cPrL_RDClW_Y"
   },
   "outputs": [],
   "source": [
    "#@title Hyperspectral Dataset Class (Extends Sequence)\n",
    "class HyperspectralDataset(Sequence):\n",
    "    def __init__(self, data, gt, shuffle=True, equal_class_distribution=False, **params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: 3D hyperspectral image\n",
    "            gt: 2D array of labels\n",
    "            shuffle: bool, set to True to shuffle data at each epoch\n",
    "            hyperparams: extra hyperparameters for setting up dataset\n",
    "        \"\"\"\n",
    "        # super(HyperspectralDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.input_channels = params['input_channels']\n",
    "        self.gt = gt\n",
    "        self.shuffle = shuffle\n",
    "        self.equal_class_distribution = equal_class_distribution\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.patch_size = params['patch_size']\n",
    "        self.supervision = params['supervision']\n",
    "        self.ignored_labels = set(params['ignored_labels'])\n",
    "        self.num_classes = params['n_classes']\n",
    "        self.loss = params['loss']\n",
    "        self.expand_dims = params['expand_dims']\n",
    "        self.flip_augmentation = params[\"flip_augmentation\"]\n",
    "        self.radiation_augmentation = params[\"radiation_augmentation\"]\n",
    "        self.mixture_augmentation = params[\"mixture_augmentation\"]\n",
    "        self.center_pixel = params[\"center_pixel\"]\n",
    "\n",
    "        if self.input_channels is not None:\n",
    "            self.multi_input = True\n",
    "        else:\n",
    "            self.multi_input = False\n",
    "\n",
    "        if self.supervision == \"full\":\n",
    "            mask = np.ones_like(gt)\n",
    "            for label in self.ignored_labels:\n",
    "                mask[gt == label] = 0\n",
    "        # Semi-supervised : use all pixels, except padding\n",
    "        elif self.supervision == \"semi\":\n",
    "            mask = np.ones_like(gt)\n",
    "        x_pos, y_pos = np.nonzero(mask)\n",
    "        num_neighbors = self.patch_size // 2\n",
    "        self.indices = np.array(\n",
    "            [\n",
    "                (x, y)\n",
    "                for x, y in zip(x_pos, y_pos)\n",
    "                if x > num_neighbors\n",
    "                    and x < data.shape[0] - num_neighbors\n",
    "                    and y > num_neighbors\n",
    "                    and y < data.shape[1] - num_neighbors\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.labels = np.array([gt[x, y] for x, y in self.indices])\n",
    "\n",
    "        self.sample_set = {label:[] for label in np.unique(self.gt) if label not in self.ignored_labels}\n",
    "\n",
    "        for index in self.indices:\n",
    "            self.sample_set[self.gt[tuple(index)]].append(tuple(index))\n",
    "\n",
    "        self.num_samples_to_select = None\n",
    "        for key, class_samples in self.sample_set.items():\n",
    "            num_samples = len(class_samples)\n",
    "            if self.num_samples_to_select is None or self.num_samples_to_select > num_samples:\n",
    "                self.num_samples_to_select = num_samples\n",
    "\n",
    "        # Run epoch end function to initialize dataset\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.equal_class_distribution:\n",
    "            # Randomly pick an equal number of indices from each class\n",
    "            # to be used in the training for this epoch\n",
    "            self.indices = np.array(\n",
    "                [index for key in self.sample_set\n",
    "                    for index in np.random.default_rng().choice(self.sample_set[key],\n",
    "                                                            self.num_samples_to_select,\n",
    "                                                            axis=0,\n",
    "                                                            replace=False)]\n",
    "            )\n",
    "\n",
    "            # Make sure to shuffle all of the different class indices\n",
    "            # around and apply the appropriate labels for those indices\n",
    "            np.random.shuffle(self.indices)\n",
    "            self.labels = np.array([self.gt[x, y] for x, y in self.indices])\n",
    "\n",
    "        elif self.shuffle:\n",
    "            # Shuffle the indices around and apply the appropriate\n",
    "            # labels for those indices\n",
    "            np.random.shuffle(self.indices)\n",
    "            self.labels = np.array([self.gt[x, y] for x, y in self.indices])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.indices) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.multi_input:\n",
    "            batch_data = [[] for _ in self.input_channels]\n",
    "        else:\n",
    "            batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        # Get all items in batch\n",
    "        for item in range(i*self.batch_size,(i+1)*self.batch_size):\n",
    "\n",
    "            # Make sure not to look for item id greater than number of\n",
    "            # indices\n",
    "            if item >= len(self.indices): break\n",
    "\n",
    "            # Get index tuple from indices\n",
    "            index = tuple(self.indices[item])\n",
    "\n",
    "            # Get data patch for the index\n",
    "            data, label= self.__get_patch(self.data,\n",
    "                                          self.gt,\n",
    "                                          index,\n",
    "                                          self.patch_size)\n",
    "\n",
    "            if self.flip_augmentation and self.patch_size > 1:\n",
    "                # Perform data augmentation (only on 2D patches)\n",
    "                data, label = self.flip(data, label)\n",
    "            if self.radiation_augmentation and np.random.random() < 0.1:\n",
    "                data = self.radiation_noise(data)\n",
    "            if self.mixture_augmentation and np.random.random() < 0.2:\n",
    "                data = self.mixture_noise(data, label)\n",
    "\n",
    "            # Extract the center label if needed\n",
    "            if self.center_pixel and self.patch_size > 1:\n",
    "                label = label[self.patch_size // 2, self.patch_size // 2]\n",
    "            # Remove unused dimensions when we work with invidual spectrums\n",
    "            elif self.patch_size == 1:\n",
    "                data = data[:, 0, 0]\n",
    "                label = label[0, 0]\n",
    "\n",
    "            # Add a fourth dimension for 3D CNN\n",
    "            if self.expand_dims and self.patch_size > 1:\n",
    "                # Make 4D data ((Batch x) Planes x Channels x Width x Height)\n",
    "                # E.g. adding a dimension for 'planes'\n",
    "                axis = len(data.shape) if K.image_data_format() == 'channels_last' else 0\n",
    "                data = np.expand_dims(data, axis)\n",
    "                # patch = tf.expand_dims(patch, 0)\n",
    "\n",
    "            # Break the data into inputs if needed\n",
    "            if self.multi_input:\n",
    "                data = [data.take(channels, axis=data.ndim-1) for channels in self.input_channels]\n",
    "\n",
    "            # If categorical cross-entropy, make sure labels are one-hot\n",
    "            # encoded\n",
    "            if self.loss == 'categorical_crossentropy':\n",
    "                label = to_categorical(label, num_classes = self.num_classes)\n",
    "\n",
    "            # Add data to lists\n",
    "            if self.multi_input:\n",
    "                for idx in range(len(batch_data)):\n",
    "                    batch_data[idx].append(tf.convert_to_tensor(data[idx], dtype='float32'))\n",
    "            else:\n",
    "                batch_data.append(tf.convert_to_tensor(data, dtype='float32'))\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        if self.multi_input:\n",
    "            for idx in range(len(batch_data)):\n",
    "                batch_data[idx] = tf.convert_to_tensor(batch_data[idx])\n",
    "            batch_data = (*batch_data,)\n",
    "        else:\n",
    "            batch_data = tf.convert_to_tensor(batch_data)\n",
    "\n",
    "        batch_labels = tf.convert_to_tensor(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_patch(data, gt, index, patch_size):\n",
    "        x, y = index\n",
    "        x1 = x - patch_size // 2    # Leftmost edge of patch\n",
    "        y1 = y - patch_size // 2    # Topmost edge of patch\n",
    "        x2 = x1 + patch_size        # Rightmost edge of patch\n",
    "        y2 = y1 + patch_size        # Bottommost edge of patch\n",
    "\n",
    "        patch = data[x1:x2, y1:y2]\n",
    "        label = gt[x1:x2, y1:y2]\n",
    "\n",
    "        # Copy the data into numpy arrays\n",
    "        patch = np.asarray(np.copy(patch), dtype=\"float32\")\n",
    "        label = np.asarray(np.copy(label), dtype='uint8')\n",
    "\n",
    "        return patch, label\n",
    "\n",
    "    @staticmethod\n",
    "    def flip(*arrays):\n",
    "        horizontal = np.random.random() > 0.5\n",
    "        vertical = np.random.random() > 0.5\n",
    "        if horizontal:\n",
    "            arrays = [np.fliplr(arr) for arr in arrays]\n",
    "        if vertical:\n",
    "            arrays = [np.flipud(arr) for arr in arrays]\n",
    "        return arrays\n",
    "\n",
    "    @staticmethod\n",
    "    def radiation_noise(data, alpha_range=(0.9, 1.1), beta=1 / 25):\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "        return alpha * data + beta * noise\n",
    "\n",
    "    def mixture_noise(self, data, label, beta=1 / 25):\n",
    "        alpha1, alpha2 = np.random.uniform(0.01, 1.0, size=2)\n",
    "        noise = np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "        data2 = np.zeros_like(data)\n",
    "        for idx, value in np.ndenumerate(label):\n",
    "            if value not in self.ignored_labels:\n",
    "                l_indices = np.nonzero(self.labels == value)[0]\n",
    "                l_indice = np.random.choice(l_indices)\n",
    "                assert self.labels[l_indice] == value\n",
    "                x, y = self.indices[l_indice]\n",
    "                data2[idx] = self.data[x, y]\n",
    "        return (alpha1 * data + alpha2 * data2) / (alpha1 + alpha2) + beta * noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEGnHgeolW_Z"
   },
   "source": [
    "## 2.3) Hyperspectral Dataset Getter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bHVGrPD6lW_Z"
   },
   "outputs": [],
   "source": [
    "#@title Get Hyperspectral Dataset\n",
    "def get_hyperspectral_dataset(data, gt, shuffle=True, **params):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    random_seed = params['random_seed']\n",
    "    input_channels = params['input_channels']\n",
    "    batch_size = params['batch_size']\n",
    "    patch_size = params['patch_size']\n",
    "    supervision = params['supervision']\n",
    "    ignored_labels = set(params['ignored_labels'])\n",
    "    num_classes = params['n_classes']\n",
    "    loss = params['loss']\n",
    "    expand_dims = params['expand_dims']\n",
    "\n",
    "    # Determine if this dataset is feeding a multi-input model\n",
    "    multi_input = True if input_channels is not None else False\n",
    "\n",
    "    # Get expected data shapes to make sure data is properly formatted\n",
    "    # if the Shape needs to be fixed\n",
    "    if multi_input:\n",
    "        x_shape = [[None, None, None, None, len(channels)] if expand_dims\n",
    "                        else [None, None, None, len(channels)]\n",
    "                        for channels in input_channels]\n",
    "    else:\n",
    "        x_shape = [None, None, None, None, data.shape[-1]] if expand_dims \\\n",
    "                    else [None, None, None, data.shape[-1]]\n",
    "\n",
    "    if loss == 'categorical_crossentropy':\n",
    "        y_shape = [None, num_classes]\n",
    "    else:\n",
    "        y_shape = [None, 1]\n",
    "\n",
    "\n",
    "    # Fully supervised : use all pixels with label not ignored\n",
    "    if supervision == \"full\":\n",
    "        mask = np.ones_like(gt)\n",
    "        for label in ignored_labels:\n",
    "            mask[gt == label] = 0\n",
    "    # Semi-supervised : use all pixels, except padding\n",
    "    elif supervision == \"semi\":\n",
    "        mask = np.ones_like(gt)\n",
    "    x_pos, y_pos = np.nonzero(mask)\n",
    "    num_neighbors = patch_size // 2\n",
    "    indices = np.array(\n",
    "        [\n",
    "            (x, y)\n",
    "            for x, y in zip(x_pos, y_pos)\n",
    "            if x > num_neighbors\n",
    "                and x < data.shape[0] - num_neighbors\n",
    "                and y > num_neighbors\n",
    "                and y < data.shape[1] - num_neighbors\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    labels = np.array([gt[x, y] for x, y in indices])\n",
    "\n",
    "    class HSDataset:\n",
    "        def __init__(self, data, gt, **params):\n",
    "\n",
    "            # Save parameters\n",
    "            self.data = data\n",
    "            self.gt = gt\n",
    "\n",
    "            self.input_channels = params['input_channels']\n",
    "            self.patch_size = params['patch_size']\n",
    "            self.num_classes = params['n_classes']\n",
    "            self.loss = params['loss']\n",
    "            self.expand_dims = params['expand_dims']\n",
    "\n",
    "            # Determine if this dataset is feeding a multi-input model\n",
    "            self.multi_input = True if self.input_channels is not None else False\n",
    "\n",
    "        def __call__(self, i):\n",
    "            i = tuple(i.numpy())\n",
    "\n",
    "            x, y = i\n",
    "            x1 = x - self.patch_size // 2    # Leftmost edge of patch\n",
    "            y1 = y - self.patch_size // 2    # Topmost edge of patch\n",
    "            x2 = x1 + self.patch_size        # Rightmost edge of patch\n",
    "            y2 = y1 + self.patch_size        # Bottommost edge of patch\n",
    "\n",
    "            patch = self.data[x1:x2, y1:y2]\n",
    "\n",
    "            # Copy the data into numpy arrays\n",
    "            patch = np.asarray(np.copy(patch), dtype=\"float32\")\n",
    "            # patch = tf.convert_to_tensor(patch, dtype=\"float32\")\n",
    "\n",
    "            if self.patch_size == 1:\n",
    "                patch = patch[:, 0, 0]\n",
    "\n",
    "            # Add a fourth dimension for 3D CNN\n",
    "            if self.expand_dims and self.patch_size > 1:\n",
    "                # Make 4D data ((Batch x) Planes x Channels x Width x Height)\n",
    "                # E.g. adding a dimension for 'planes'\n",
    "                axis = len(patch.shape) if K.image_data_format() == 'channels_last' else 0\n",
    "                patch = np.expand_dims(patch, axis)\n",
    "\n",
    "            if self.multi_input:\n",
    "                # Break the data into inputs\n",
    "                patch = [patch.take(channels, axis=data.ndim-1) for channels in self.input_channels]\n",
    "\n",
    "            sample = patch\n",
    "\n",
    "            # Get label for the patch\n",
    "            label = self.gt[i]\n",
    "\n",
    "            if self.loss == 'categorical_crossentropy':\n",
    "                label = to_categorical(label, num_classes = self.num_classes)\n",
    "\n",
    "\n",
    "            return sample, label\n",
    "\n",
    "    class FixShape:\n",
    "        def __init__(self, x_shape, y_shape, multi_input):\n",
    "            self.x_shape = x_shape\n",
    "            self.y_shape = y_shape\n",
    "            self.multi_input = multi_input\n",
    "\n",
    "        def __call__(self, x, y):\n",
    "            if self.multi_input:\n",
    "                _x = []\n",
    "                for index, data in enumerate(x):\n",
    "                    _x.append(data.set_shape(self.x_shape[index]))\n",
    "\n",
    "                x = tuple(_x)\n",
    "            else:\n",
    "                x.set_shape(self.x_shape)\n",
    "\n",
    "            y.set_shape(self.y_shape)\n",
    "\n",
    "            return x, y\n",
    "\n",
    "    hs_dataset = HSDataset(data, gt, **params)\n",
    "\n",
    "    output_signature = (tf.TensorSpec(shape=(2), dtype=tf.uint32))\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: indices,\n",
    "                                            output_signature=output_signature)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(indices), seed=random_seed,\n",
    "                                reshuffle_each_iteration=True)\n",
    "\n",
    "    if multi_input:\n",
    "        Tout = [tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.uint8)]\n",
    "    else:\n",
    "        Tout = [tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=None, dtype=tf.uint8)]\n",
    "    dataset = dataset.map(lambda i: tf.py_function(func=hs_dataset,\n",
    "                                                   inp=[i],\n",
    "                                                   Tout=Tout\n",
    "                                                   ),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # fixup_shape = FixShape(x_shape, y_shape, multi_input)\n",
    "    # dataset = dataset.batch(batch_size).map(fixup_shape)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_EZWrpClW_Z"
   },
   "source": [
    "## 2.4) Dataset Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pk3W27B-lW_Z"
   },
   "outputs": [],
   "source": [
    "#@title Create Datasets\n",
    "def create_datasets(data, train_gt, test_gt, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Get data from hyperparameters\n",
    "    patch_size = hyperparams['patch_size']  # N in NxN patch per sample\n",
    "    train_split = hyperparams['train_split']    # training percent in val/train split\n",
    "    split_mode = hyperparams['split_mode']\n",
    "\n",
    "    # Set pad length per dimension\n",
    "    pad = patch_size // 2\n",
    "\n",
    "    # Pad only first two dimensions\n",
    "    ignore_dims = [x for x in range(data.ndim) if x >= 2]\n",
    "\n",
    "    # Pad all images\n",
    "    data = pad_img(data, pad, ignore_dims=ignore_dims)\n",
    "    train_gt = pad_img(train_gt, pad)\n",
    "    test_gt = pad_img(test_gt, pad)\n",
    "\n",
    "    # Show updated padded dataset shapes\n",
    "    print(f'padded data shape: {data.shape}')\n",
    "    print(f'padded train_gt shape: {train_gt.shape}')\n",
    "    print(f'padded test_gt shape: {test_gt.shape}')\n",
    "\n",
    "    # Create validation dataset from training set\n",
    "    if train_split is not None and train_split != 1.0:\n",
    "        train_gt, val_gt = sample_gt(train_gt, train_split, mode=split_mode)\n",
    "\n",
    "    dataset_params = (\n",
    "        'input_channels',\n",
    "        'batch_size',\n",
    "        'patch_size',\n",
    "        'supervision',\n",
    "        'ignored_labels',\n",
    "        'n_classes',\n",
    "        'loss',\n",
    "        'expand_dims',\n",
    "        'flip_augmentation',\n",
    "        'radiation_augmentation',\n",
    "        'mixture_augmentation',\n",
    "        'center_pixel',\n",
    "    )\n",
    "\n",
    "    # Create dataset parameter subset from hyperparameters\n",
    "    params = {param: hyperparams[param] for param in dataset_params}\n",
    "\n",
    "    train_dataset = HyperspectralDataset(data, train_gt, equal_class_distribution=True, **params)\n",
    "\n",
    "    # Don't use augmentation for validation and test sets\n",
    "    params['flip_augmentation'] = False\n",
    "    params['radiation_augmentation'] = False\n",
    "    params['mixture_augmentation'] = False\n",
    "    if train_split is not None and train_split != 1.0:\n",
    "        val_dataset = HyperspectralDataset(data, val_gt, shuffle=False, **params)\n",
    "    else:\n",
    "        val_dataset = None\n",
    "\n",
    "    # If postprocessing is going to occur, change supervision parameter\n",
    "    # to 'semi' so all pixels are used (so we can predict the full\n",
    "    # image, the prediction then being used for postprocessing)\n",
    "    if not hyperparams['skip_data_postprocessing']:\n",
    "        params['supervision'] = 'semi'\n",
    "\n",
    "    # Don't use augmentation for test set\n",
    "    params['flip_augmentation'] = False\n",
    "    params['radiation_augmentation'] = False\n",
    "    params['mixture_augmentation'] = False\n",
    "    test_dataset = HyperspectralDataset(data, test_gt, shuffle=False, **params)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "H0o9sBurlW_a"
   },
   "outputs": [],
   "source": [
    "#@title Create Datasets (Version 2)\n",
    "def create_datasets_v2(data, train_gt, test_gt, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Get data from hyperparameters\n",
    "    patch_size = hyperparams['patch_size']  # N in NxN patch per sample\n",
    "    train_split = hyperparams['train_split']    # training percent in val/train split\n",
    "    split_mode = hyperparams['split_mode']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "\n",
    "    # Set pad length per dimension\n",
    "    pad = patch_size // 2\n",
    "\n",
    "    # Pad only first two dimensions\n",
    "    ignore_dims = [x for x in range(data.ndim) if x >= 2]\n",
    "\n",
    "    # Pad all images\n",
    "    data = pad_img(data, pad, ignore_dims=ignore_dims)\n",
    "    train_gt = pad_img(train_gt, pad)\n",
    "    test_gt = pad_img(test_gt, pad)\n",
    "\n",
    "    # Show updated padded dataset shapes\n",
    "    print(f'padded data shape: {data.shape}')\n",
    "    print(f'padded train_gt shape: {train_gt.shape}')\n",
    "    print(f'padded test_gt shape: {test_gt.shape}')\n",
    "\n",
    "    # Create validation dataset from training set\n",
    "    train_gt, val_gt = sample_gt(train_gt, train_split, mode=split_mode)\n",
    "\n",
    "    dataset_params = (\n",
    "        'random_seed',\n",
    "        'input_channels',\n",
    "        'batch_size',\n",
    "        'patch_size',\n",
    "        'supervision',\n",
    "        'ignored_labels',\n",
    "        'n_classes',\n",
    "        'loss',\n",
    "        'expand_dims',\n",
    "        'flip_augmentation',\n",
    "        'radiation_augmentation',\n",
    "        'mixture_augmentation',\n",
    "        'center_pixel',\n",
    "    )\n",
    "\n",
    "    # Create dataset parameter subset from hyperparameters\n",
    "    params = {param: hyperparams[param] for param in dataset_params}\n",
    "\n",
    "    train_dataset, train_labels = get_hyperspectral_dataset(data, train_gt, **params)\n",
    "    val_dataset, val_labels = get_hyperspectral_dataset(data, val_gt, **params)\n",
    "\n",
    "    # If postprocessing is going to occur, change supervision parameter\n",
    "    # to 'semi' so all pixels are used (so we can predict the full\n",
    "    # image, the prediction then being used for postprocessing)\n",
    "    if not hyperparams['skip_data_postprocessing']:\n",
    "        params['supervision'] = 'semi'\n",
    "\n",
    "    test_dataset, target_test = get_hyperspectral_dataset(data, test_gt, shuffle=False, **params)\n",
    "\n",
    "    datasets = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_steps': math.ceil(len(train_labels) / batch_size),\n",
    "        'val_dataset': val_dataset,\n",
    "        'val_steps': math.ceil(len(val_labels) / batch_size),\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_steps': math.ceil(len(target_test) / batch_size),\n",
    "        'target_test': target_test,\n",
    "    }\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om2S3912lW_a"
   },
   "source": [
    "# 3) Set up GRSS DFC 2018 University of Houston Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRb5YllXlW_a"
   },
   "source": [
    "## 3.1 GRSS DFC 2018 UH constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BVEr-XPa59ZO"
   },
   "outputs": [],
   "source": [
    "#@title Initialize GRSS DFC 2018 UH constants\n",
    "\n",
    "# Setup global variables for grss_dfc_2018 datset\n",
    "# Path to directory containing all GRSS 2018 Data Fusion Contest\n",
    "# University of Houston image data\n",
    "UH_2018_DATASET_DIRECTORY_PATH = '../datasets/grss_dfc_2018/'\n",
    "\n",
    "# Following paths are assumed to be from the root UH 2018 dataset path\n",
    "UH_2018_TRAINING_GT_IMAGE_PATH = 'TrainingGT/2018_IEEE_GRSS_DFC_GT_TR.tif'\n",
    "UH_2018_TESTING_GT_IMAGE_PATH = 'TestingGT/Test_Labels.tif'\n",
    "UH_2018_HS_IMAGE_PATH = 'FullHSIDataset/20170218_UH_CASI_S4_NAD83.pix'\n",
    "UH_2018_LIDAR_DSM_PATH = 'Lidar GeoTiff Rasters/DSM_C12/UH17c_GEF051.tif'\n",
    "UH_2018_LIDAR_DEM_3MSR_PATH = 'Lidar GeoTiff Rasters/DEM_C123_3msr/UH17_GEG051.tif'\n",
    "UH_2018_LIDAR_DEM_TLI_PATH = 'Lidar GeoTiff Rasters/DEM_C123_TLI/UH17_GEG05.tif'\n",
    "UH_2018_LIDAR_DEM_B_PATH = 'Lidar GeoTiff Rasters/DEM+B_C123/UH17_GEM051.tif'\n",
    "UH_2018_LIDAR_INTENSITY_1550NM_PATH = 'Lidar GeoTiff Rasters/Intensity_C1/UH17_GI1F051.tif'\n",
    "UH_2018_LIDAR_INTENSITY_1064NM_PATH = 'Lidar GeoTiff Rasters/Intensity_C2/UH17_GI2F051.tif'\n",
    "UH_2018_LIDAR_INTENSITY_532NM_PATH = 'Lidar GeoTiff Rasters/Intensity_C3/UH17_GI3F051.tif'\n",
    "\n",
    "# Paths, in order of tile, for the very high resolution RGB image\n",
    "UH_2018_VHR_IMAGE_PATHS = [\n",
    "    [\n",
    "        'Final RGB HR Imagery/UH_NAD83_271460_3290290.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_272056_3290290.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_272652_3290290.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_273248_3290290.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_273844_3290290.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_274440_3290290.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_275036_3290290.tif',\n",
    "    ],\n",
    "    [\n",
    "        'Final RGB HR Imagery/UH_NAD83_271460_3289689.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_272056_3289689.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_272652_3289689.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_273248_3289689.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_273844_3289689.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_274440_3289689.tif',\n",
    "        'Final RGB HR Imagery/UH_NAD83_275036_3289689.tif',\n",
    "    ],\n",
    "]\n",
    "\n",
    "# Number of rows and columns in the overall dataset image such that\n",
    "# parts of the dataset can be matched with the ground truth\n",
    "UH_2018_NUM_TILE_COLUMNS = 7\n",
    "UH_2018_NUM_TILE_ROWS = 2\n",
    "\n",
    "# List of tuples corresponding to the image tiles that match the ground\n",
    "# truth for the training set\n",
    "UH_2018_TRAINING_GT_TILES = ((1,1), (1,2), (1, 3), (1, 4))\n",
    "\n",
    "# List of tuples corresponding to the image tiles that match the ground\n",
    "# truth for the testing set\n",
    "UH_2018_TESTING_GT_TILES = ((0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6),\n",
    "                            (1,0),                             (1,5), (1,6))\n",
    "\n",
    "# Mapping of overall dataset training ground truth tile indices to the\n",
    "# tile indices of the actual ground truth image\n",
    "UH_2018_TRAINING_GT_TILE_OFFSETS = {\n",
    "    (1,1) : (0,0),\n",
    "    (1,2) : (0,1),\n",
    "    (1,3) : (0,2),\n",
    "    (1,4) : (0,3)\n",
    "}\n",
    "\n",
    "# GSD = Ground Sampling Distance\n",
    "UH_2018_VHR_GSD = 0.05  # Very high resolution image GSD in meters\n",
    "UH_2018_HS_GSD = 1.0    # Hyperspectral image GSD in meters\n",
    "UH_2018_LIDAR_GSD = 0.5 # LiDAR raster image GSD in meters\n",
    "UH_2018_GT_GSD = 0.5    # Ground truth images GSD in meters\n",
    "\n",
    "# Number of hyperspectral band channels\n",
    "UH_2018_NUM_HS_BANDS = 48\n",
    "\n",
    "# Threshold value for hyperspectral band intensities\n",
    "UH_2018_HS_BAND_THRES = 100_000\n",
    "\n",
    "# Number of multispectral LiDAR band channels\n",
    "UH_2018_NUM_LIDAR_BANDS = 3\n",
    "\n",
    "# Threshold value for LiDAR multispectral intensities\n",
    "UH_2018_MS_INTENSITY_THRESHOLD = 1e4\n",
    "\n",
    "# Threshold value for LiDAR DSM\n",
    "UH_2018_DSM_THRESHOLD = 1e10\n",
    "\n",
    "# A list of the wavelength values for each of the hyperspectal band\n",
    "# channels\n",
    "UH_2018_HS_BAND_WAVELENGTHS = [\n",
    "    '374.395nm +/- 7.170nm',\n",
    "    '388.733nm +/- 7.168nm',\n",
    "    '403.068nm +/- 7.167nm',\n",
    "    '417.401nm +/- 7.166nm',\n",
    "    '431.732nm +/- 7.165nm',\n",
    "    '446.061nm +/- 7.164nm',\n",
    "    '460.388nm +/- 7.163nm',\n",
    "    '474.712nm +/- 7.162nm',\n",
    "    '489.035nm +/- 7.161nm',\n",
    "    '503.356nm +/- 7.160nm',\n",
    "    '517.675nm +/- 7.159nm',\n",
    "    '531.992nm +/- 7.158nm',\n",
    "    '546.308nm +/- 7.158nm',\n",
    "    '560.622nm +/- 7.157nm',\n",
    "    '574.936nm +/- 7.156nm',\n",
    "    '589.247nm +/- 7.156nm',\n",
    "    '603.558nm +/- 7.155nm',\n",
    "    '617.868nm +/- 7.155nm',\n",
    "    '632.176nm +/- 7.154nm',\n",
    "    '646.484nm +/- 7.154nm',\n",
    "    '660.791nm +/- 7.153nm',\n",
    "    '675.097nm +/- 7.153nm',\n",
    "    '689.402nm +/- 7.153nm',\n",
    "    '703.707nm +/- 7.152nm',\n",
    "    '718.012nm +/- 7.152nm',\n",
    "    '732.316nm +/- 7.152nm',\n",
    "    '746.620nm +/- 7.152nm',\n",
    "    '760.924nm +/- 7.152nm',\n",
    "    '775.228nm +/- 7.152nm',\n",
    "    '789.532nm +/- 7.152nm',\n",
    "    '803.835nm +/- 7.152nm',\n",
    "    '818.140nm +/- 7.152nm',\n",
    "    '832.444nm +/- 7.152nm',\n",
    "    '846.749nm +/- 7.153nm',\n",
    "    '861.054nm +/- 7.153nm',\n",
    "    '875.360nm +/- 7.153nm',\n",
    "    '889.666nm +/- 7.153nm',\n",
    "    '903.974nm +/- 7.154nm',\n",
    "    '918.282nm +/- 7.154nm',\n",
    "    '932.591nm +/- 7.155nm',\n",
    "    '946.901nm +/- 7.155nm',\n",
    "    '961.212nm +/- 7.156nm',\n",
    "    '975.525nm +/- 7.157nm',\n",
    "    '989.839nm +/- 7.157nm',\n",
    "    '1004.154nm +/- 7.158nm',\n",
    "    '1018.471nm +/- 7.159nm',\n",
    "    '1032.789nm +/- 7.160nm',\n",
    "    '1047.109nm +/- 7.160nm',\n",
    "]\n",
    "\n",
    "# A list of hexidecimal color values corresponding to the wavelength of\n",
    "# the hyperspectral bands\n",
    "UH_2018_HS_BAND_RGB = [\n",
    "    '#610061',  #374nm\n",
    "    '#780088',  #389nm\n",
    "    '#8300c0',  #403nm\n",
    "    '#7100f4',  #417nm\n",
    "    '#3300ff',  #432nm\n",
    "    '#002fff',  #446nm\n",
    "    '#007bff',  #460nm\n",
    "    '#00c0ff',  #475nm\n",
    "    '#00fbff',  #489nm\n",
    "    '#00ff6e',  #503nm\n",
    "    '#2dff00',  #518nm\n",
    "    '#65ff00',  #532nm\n",
    "    '#96ff00',  #546nm\n",
    "    '#c6ff00',  #561nm\n",
    "    '#f0ff00',  #575nm\n",
    "    '#ffe200',  #589nm\n",
    "    '#ffb000',  #604nm\n",
    "    '#ff7e00',  #618nm\n",
    "    '#ff4600',  #632nm\n",
    "    '#ff0000',  #646nm\n",
    "    '#fd0000',  #661nm\n",
    "    '#fb0000',  #675nm\n",
    "    '#fa0000',  #689nm\n",
    "    '#f80000',  #704nm\n",
    "    '#de0000',  #718nm\n",
    "    '#c40000',  #732nm\n",
    "    '#a70000',  #747nm\n",
    "    '#8a0000',  #761nm\n",
    "    '#6d0000',  #775nm\n",
    "    '#610000',  #790nm (representation)\n",
    "    '#5e0000',  #804nm (representation)\n",
    "    '#5c0000',  #818nm (representation)\n",
    "    '#590000',  #843nm (representation)\n",
    "    '#570000',  #847nm (representation)\n",
    "    '#540000',  #862nm (representation)\n",
    "    '#510000',  #875nm (representation)\n",
    "    '#4f0000',  #890nm (representation)\n",
    "    '#4c0000',  #904nm (representation)\n",
    "    '#4a0000',  #918nm (representation)\n",
    "    '#470000',  #933nm (representation)\n",
    "    '#440000',  #947nm (representation)\n",
    "    '#420000',  #961nm (representation)\n",
    "    '#3f0000',  #976nm (representation)\n",
    "    '#3d0000',  #990nm (representation)\n",
    "    '#3a0000',  #1004nm (representation)\n",
    "    '#370000',  #1018nm (representation)\n",
    "    '#350000',  #1033nm (representation)\n",
    "    '#320000',  #1047nm (representation)\n",
    "]\n",
    "\n",
    "UH_2018_LIDAR_MS_BAND_WAVELENGTHS = [\n",
    "    '1550nm',\n",
    "    '1064nm',\n",
    "    '532nm',\n",
    "]\n",
    "\n",
    "UH_2018_LIDAR_MS_BAND_RGB = [\n",
    "    '#150000',  #1550nm (representation)\n",
    "    '#2f0000',  #1064nm (representation)\n",
    "    '#65ff00',  #532nm\n",
    "]\n",
    "\n",
    "UH_2018_VHR_CHANNELS = [\n",
    "    'Red',\n",
    "    'Green',\n",
    "    'Blue',\n",
    "]\n",
    "\n",
    "UH_2018_VHR_RGB = [\n",
    "    '#ff0000',  #Red\n",
    "    '#00ff00',  #Green\n",
    "    '#0000ff',  #Blue\n",
    "]\n",
    "\n",
    "UH_2018_IGNORED_CLASSES = [0]\n",
    "\n",
    "# List of classes where the index is the value of the pixel in the\n",
    "# ground truth image\n",
    "UH_2018_CLASS_LIST = [\n",
    "    'Undefined',\n",
    "    'Healthy grass',\n",
    "    'Stressed grass',\n",
    "    'Artificial turf',\n",
    "    'Evergreen trees',\n",
    "    'Deciduous trees',\n",
    "    'Bare earth',\n",
    "    'Water',\n",
    "    'Residential buildings',\n",
    "    'Non-residential buildings',\n",
    "    'Roads',\n",
    "    'Sidewalks',\n",
    "    'Crosswalks',\n",
    "    'Major thoroughfares',\n",
    "    'Highways',\n",
    "    'Railways',\n",
    "    'Paved parking lots',\n",
    "    'Unpaved parking lots',\n",
    "    'Cars',\n",
    "    'Trains',\n",
    "    'Stadium seats',\n",
    "]\n",
    "\n",
    "# Map of classes where the key is the value of the pixel in the\n",
    "# ground truth image\n",
    "UH_2018_CLASS_MAP = {index: label for index, label in enumerate(UH_2018_CLASS_LIST)}\n",
    "\n",
    "\n",
    "# Number of class labels for the University of Houston 2018 dataset\n",
    "# (one is subtracted to exclude the 'undefined' class)\n",
    "# UH_2018_NUM_CLASSES = len(UH_2018_CLASS_LIST) - len(UH_2018_IGNORED_CLASSES)\n",
    "UH_2018_NUM_CLASSES = len(UH_2018_CLASS_LIST)\n",
    "\n",
    "# The default resampling method to use when no method is indicated or\n",
    "# the method indicated is unsupported\n",
    "DEFAULT_RESAMPLING_METHOD = 'nearest'\n",
    "\n",
    "# Mapping of resampling method strings to their associated value\n",
    "#   - 'max', 'min', 'med', 'q1', 'q3' are only supported in GDAL >= 2.0.0.\n",
    "#   - 'nearest', 'bilinear', 'cubic', 'cubic_spline', 'lanczos', 'average',\n",
    "#      'mode' are always available (GDAL >= 1.10).\n",
    "#   - 'sum' is only supported in GDAL >= 3.1.\n",
    "#   - 'rms' is only supported in GDAL >= 3.3.\n",
    "RESAMPLING_METHODS = {\n",
    "    'nearest': Resampling.nearest,\n",
    "    'bilinear': Resampling.bilinear,\n",
    "    'cubic': Resampling.cubic,\n",
    "    'cubic_spline': Resampling.cubic_spline,\n",
    "    'lanczos': Resampling.lanczos,\n",
    "    'average': Resampling.average,\n",
    "    'mode': Resampling.mode,\n",
    "    'gauss': Resampling.gauss,\n",
    "    'max': Resampling.max,\n",
    "    'min': Resampling.min,\n",
    "    'med': Resampling.med,\n",
    "    'q1': Resampling.q3,\n",
    "    'q3': Resampling.sum,\n",
    "    'rms': Resampling.rms\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtSTIz1RlW_b"
   },
   "source": [
    "## 3.2) Class for loading and manipulating different parts of the GRSS 2018 Data Fusion Contest University of Houston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-Z5OJ8Lc-Dfb"
   },
   "outputs": [],
   "source": [
    "#@title University of Houston 2018 Dataset Class\n",
    "class UH_2018_Dataset:\n",
    "    \"\"\"\n",
    "    Class for loading and manipulating different parts of the GRSS 2018\n",
    "    Data Fusion Contest University of Houston dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path=UH_2018_DATASET_DIRECTORY_PATH):\n",
    "\n",
    "        # Set dataset attributes\n",
    "        self.name = 'GRSS_DFC_2018_UH'\n",
    "\n",
    "        # Set dataset file paths\n",
    "        self.path_to_dataset_directory = dataset_path\n",
    "        self.path_to_training_gt_image = UH_2018_TRAINING_GT_IMAGE_PATH\n",
    "        self.path_to_testing_gt_image = UH_2018_TESTING_GT_IMAGE_PATH\n",
    "        self.path_to_hs_image = UH_2018_HS_IMAGE_PATH\n",
    "        self.path_to_lidar_dsm = UH_2018_LIDAR_DSM_PATH\n",
    "        self.path_to_lidar_dem_3msr = UH_2018_LIDAR_DEM_3MSR_PATH\n",
    "        self.path_to_lidar_dem_tli = UH_2018_LIDAR_DEM_TLI_PATH\n",
    "        self.path_to_lidar_dem_b = UH_2018_LIDAR_DEM_B_PATH\n",
    "        self.path_to_lidar_1550nm_intensity = UH_2018_LIDAR_INTENSITY_1550NM_PATH\n",
    "        self.path_to_lidar_1064nm_intensity = UH_2018_LIDAR_INTENSITY_1064NM_PATH\n",
    "        self.path_to_lidar_532nm_intensity = UH_2018_LIDAR_INTENSITY_532NM_PATH\n",
    "        self.paths_to_vhr_images = UH_2018_VHR_IMAGE_PATHS\n",
    "\n",
    "        # Set dataset ground truth attributes\n",
    "        self.gt_class_label_list = UH_2018_CLASS_LIST\n",
    "        self.gt_class_value_mapping = UH_2018_CLASS_MAP\n",
    "        self.gt_num_classes = UH_2018_NUM_CLASSES\n",
    "        self.gt_ignored_labels = UH_2018_IGNORED_CLASSES\n",
    "\n",
    "        # Set dataset hyperspectral image attributes\n",
    "        self.hs_num_bands = UH_2018_NUM_HS_BANDS\n",
    "        self.hs_band_rgb_list = UH_2018_HS_BAND_RGB\n",
    "        self.hs_band_wavelength_labels = UH_2018_HS_BAND_WAVELENGTHS\n",
    "        self.hs_band_val_thres = UH_2018_HS_BAND_THRES\n",
    "\n",
    "        # Set dataset lidar data attributes\n",
    "        self.lidar_ms_num_bands = UH_2018_NUM_LIDAR_BANDS\n",
    "        self.lidar_ms_band_wavelength_labels = UH_2018_LIDAR_MS_BAND_WAVELENGTHS\n",
    "        self.lidar_ms_band_rgb_list = UH_2018_LIDAR_MS_BAND_RGB\n",
    "        self.lidar_ms_intensity_thres = UH_2018_MS_INTENSITY_THRESHOLD\n",
    "        self.lidar_dsm_thres = UH_2018_DSM_THRESHOLD\n",
    "\n",
    "        # Set dataset VHR RGB image attributes\n",
    "        self.vhr_channel_labels = UH_2018_VHR_CHANNELS\n",
    "        self.vhr_channel_rgb_list = UH_2018_VHR_RGB\n",
    "\n",
    "        # Set miscellaneous dataset attributes\n",
    "        self.gsd_gt = UH_2018_GT_GSD\n",
    "        self.gsd_hs = UH_2018_HS_GSD\n",
    "        self.gsd_lidar = UH_2018_LIDAR_GSD\n",
    "        self.gsd_vhr = UH_2018_VHR_GSD\n",
    "\n",
    "        self.dataset_tiled_subset_rows = UH_2018_NUM_TILE_ROWS\n",
    "        self.dataset_tiled_subset_cols = UH_2018_NUM_TILE_COLUMNS\n",
    "        self.dataset_training_subset = UH_2018_TRAINING_GT_TILES\n",
    "        self.dataset_training_subset_map = UH_2018_TRAINING_GT_TILE_OFFSETS\n",
    "        self.dataset_testing_subset = UH_2018_TESTING_GT_TILES\n",
    "\n",
    "        # Initialize dataset variables\n",
    "        self.gt_image = None\n",
    "        self.gt_image_tiles = None\n",
    "        self.hs_image = None\n",
    "        self.hs_image_tiles = None\n",
    "        self.lidar_ms_image = None\n",
    "        self.lidar_ms_image_tiles = None\n",
    "        self.lidar_dsm_image = None\n",
    "        self.lidar_dsm_image_tiles = None\n",
    "        self.lidar_dem_image = None\n",
    "        self.lidar_dem_image_tiles = None\n",
    "        self.lidar_ndsm_image = None\n",
    "        self.lidar_ndsm_image_tiles = None\n",
    "        self.vhr_image = None\n",
    "        self.vhr_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def clear_all_images(self):\n",
    "        \"\"\"Clears values of all image variables to free memory.\"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.gt_image\n",
    "        del self.gt_image_tiles\n",
    "        del self.hs_image\n",
    "        del self.hs_image_tiles\n",
    "        del self.lidar_ms_image\n",
    "        del self.lidar_ms_image_tiles\n",
    "        del self.lidar_dsm_image\n",
    "        del self.lidar_dsm_image_tiles\n",
    "        del self.lidar_dem_image\n",
    "        del self.lidar_dem_image_tiles\n",
    "        del self.lidar_ndsm_image\n",
    "        del self.lidar_ndsm_image_tiles\n",
    "        del self.vhr_image\n",
    "        del self.vhr_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.gt_image = None\n",
    "        self.gt_image_tiles = None\n",
    "        self.hs_image = None\n",
    "        self.hs_image_tiles = None\n",
    "        self.lidar_ms_image = None\n",
    "        self.lidar_ms_image_tiles = None\n",
    "        self.lidar_dsm_image = None\n",
    "        self.lidar_dsm_image_tiles = None\n",
    "        self.lidar_dem_image = None\n",
    "        self.lidar_dem_image_tiles = None\n",
    "        self.lidar_ndsm_image = None\n",
    "        self.lidar_ndsm_image_tiles = None\n",
    "        self.vhr_image = None\n",
    "        self.vhr_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def clear_gt_images(self):\n",
    "        \"\"\"Clears values of ground truth image variables to free memory.\"\"\"\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.gt_image\n",
    "        del self.gt_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.gt_image = None\n",
    "        self.gt_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def clear_hs_images(self):\n",
    "        \"\"\"Clears values of hyperspectral image variables to free memory.\"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.hs_image\n",
    "        del self.hs_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.hs_image = None\n",
    "        self.hs_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def clear_lidar_ms_images(self):\n",
    "        \"\"\"\n",
    "        Clears values of lidar multispectral image variables to free\n",
    "        memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.lidar_ms_image\n",
    "        del self.lidar_ms_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.lidar_ms_image = None\n",
    "        self.lidar_ms_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def clear_lidar_dsm_images(self):\n",
    "        \"\"\"\n",
    "        Clears values of lidar digital surface model (NDSM) image\n",
    "        variables to free memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.lidar_dsm_image\n",
    "        del self.lidar_dsm_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.lidar_dsm_image = None\n",
    "        self.lidar_dsm_image_tiles = None\n",
    "\n",
    "    def clear_lidar_dem_images(self):\n",
    "        \"\"\"\n",
    "        Clears values of lidar digital surface model (NDSM) image\n",
    "        variables to free memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.lidar_dem_image\n",
    "        del self.lidar_dem_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.lidar_dem_image = None\n",
    "        self.lidar_dem_image_tiles = None\n",
    "\n",
    "    def clear_lidar_ndsm_images(self):\n",
    "        \"\"\"\n",
    "        Clears values of lidar normalized digital surface model (NDSM)\n",
    "        image variables to free memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.lidar_ndsm_image\n",
    "        del self.lidar_ndsm_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.lidar_ndsm_image = None\n",
    "        self.lidar_ndsm_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def clear_vhr_images(self):\n",
    "        \"\"\"\n",
    "        Clears values of very high resolution image variables to free\n",
    "        memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Delete the variables to mark the memory as unused\n",
    "        del self.vhr_image\n",
    "        del self.vhr_image_tiles\n",
    "\n",
    "        # Run garbage collection to release the memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Reinitialize the variables\n",
    "        self.vhr_image = None\n",
    "        self.vhr_image_tiles = None\n",
    "\n",
    "\n",
    "\n",
    "    def merge_tiles(self, tiles, num_rows = None, num_cols = None):\n",
    "        \"\"\"Merges a set of image tiles into a single image.\"\"\"\n",
    "\n",
    "        print('Merging image tiles...')\n",
    "\n",
    "        # If rows or columns are not specified, use defaults\n",
    "        if num_rows is None: num_rows = self.dataset_tiled_subset_rows\n",
    "        if num_cols is None: num_cols = self.dataset_tiled_subset_cols\n",
    "\n",
    "        # Initialize empty list of image row values\n",
    "        image_rows = []\n",
    "\n",
    "        # Loop through each tile and stitch them together into single image\n",
    "        for row in range(0, num_rows):\n",
    "            # Get first tile in row\n",
    "            img_row = np.copy(tiles[row * num_cols])\n",
    "\n",
    "            # Loop through remaining tiles in current row\n",
    "            for col in range(1, num_cols):\n",
    "                # Concatenate each subsequent tile in row to image row array\n",
    "                img_row = np.concatenate((img_row, tiles[row*num_cols + col]), axis=1)\n",
    "\n",
    "            # Append image row to list of image rows\n",
    "            image_rows.append(img_row)\n",
    "\n",
    "        # Concatenate all image rows together to create single image\n",
    "        merged_image = np.concatenate(image_rows, axis=0)\n",
    "\n",
    "        return merged_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_gt_image(self, train_only=False, test_only=False):\n",
    "        \"\"\"\n",
    "        Loads the full-size ground truth image mask for the University\n",
    "        of Houston 2018 dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if train_only:\n",
    "            print('Loading training ground truth image...')\n",
    "        elif test_only:\n",
    "            print('Loading test ground truth image...')\n",
    "        else:\n",
    "            print('Loading full ground truth image...')\n",
    "\n",
    "        # Ground truth can only be loaded as tiles since there's two\n",
    "        # images, so load tiles and then merge them to create full GT\n",
    "        # image\n",
    "        self.gt_image = self.merge_tiles(\n",
    "            self.load_gt_image_tiles(train_only=train_only,\n",
    "                                     test_only=test_only))\n",
    "\n",
    "        return self.gt_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_gt_image_tiles(self, tile_list=None, train_only=False, test_only=False):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's ground truth\n",
    "        images as a set of tiles. If no tile list is given, the whole\n",
    "        image will be loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        if train_only:\n",
    "            print('Loading training ground truth tiles...')\n",
    "        elif test_only:\n",
    "            print('Loading test ground truth tiles...')\n",
    "        else:\n",
    "            print('Loading training and test ground truth tiles...')\n",
    "\n",
    "        self.gt_image_tiles = []\n",
    "\n",
    "        # train_only flag takes priority\n",
    "        if train_only: test_only = False\n",
    "\n",
    "        # Get full path to dataset's ground truth images\n",
    "        train_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                        self.path_to_training_gt_image)\n",
    "        test_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                       self.path_to_testing_gt_image)\n",
    "\n",
    "        # Throw error if file path does not exist\n",
    "        if not os.path.isfile(train_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 training ground truth image is invalid!'\n",
    "            f'Path={train_image_path}')\n",
    "\n",
    "        if not os.path.isfile(test_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 testing ground truth image is invalid!'\n",
    "            f'Path={test_image_path}')\n",
    "\n",
    "        with rasterio.open(train_image_path) as train_src, \\\n",
    "             rasterio.open(test_image_path) as test_src:\n",
    "\n",
    "            # Get the size of the tile windows (use full size test image)\n",
    "            tile_width = test_src.width / self.dataset_tiled_subset_cols\n",
    "            tile_height = test_src.height / self.dataset_tiled_subset_rows\n",
    "\n",
    "                # Read in the image data for each image tile\n",
    "            for tile_row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for tile_column in range(0, self.dataset_tiled_subset_cols):\n",
    "\n",
    "                    # Check to see if current tile is one of the training\n",
    "                    # ground truth tiles\n",
    "                    if not test_only and (tile_row, tile_column) in self.dataset_training_subset:\n",
    "\n",
    "                        offset_row, offset_column = self.dataset_training_subset_map[\n",
    "                                                        (tile_row, tile_column)]\n",
    "\n",
    "                        # Set the tile window to read from the image\n",
    "                        window = Window(tile_width * offset_column ,\n",
    "                                        tile_height * offset_row,\n",
    "                                        tile_width, tile_height)\n",
    "\n",
    "                        # Read the tile window from the image\n",
    "                        tile = train_src.read(1, window = window)\n",
    "\n",
    "                        # Copy the tile to the tiles array\n",
    "                        self.gt_image_tiles.append(np.copy(tile))\n",
    "                    elif not train_only and (tile_row, tile_column) in self.dataset_testing_subset:\n",
    "                        # Set the tile window to read from the image\n",
    "                        window = Window(tile_width * tile_column,\n",
    "                                        tile_height * tile_row,\n",
    "                                        tile_width, tile_height)\n",
    "\n",
    "                        # Read the tile window from the image\n",
    "                        tile = test_src.read(1, window = window)\n",
    "\n",
    "                        # Copy the tile to the tiles array\n",
    "                        self.gt_image_tiles.append(np.copy(tile))\n",
    "                    else:\n",
    "                        self.gt_image_tiles.append(np.zeros(\n",
    "                            (int(tile_height), int(tile_width)),\n",
    "                            dtype=np.uint8))\n",
    "\n",
    "        return self.gt_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_gt_image_array(self, path, file_name='full_gt_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full ground truth image to a file\n",
    "        for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Saving full ground truth image numpy array to file ({file_name})...')\n",
    "\n",
    "        # If the gt image member variable is empty, then load the full\n",
    "        # ground truth image\n",
    "        if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.gt_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_gt_image_array(self, path, file_name='tiled_gt_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled ground truth image to a file\n",
    "        for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Saving full ground truth image numpy array to file ({file_name})...')\n",
    "\n",
    "        # If the gt image tile member variable is empty, then load all\n",
    "        # ground truth image tiles\n",
    "        if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.gt_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_gt_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset ground truth image.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Loading full ground truth image numpy array from file ({file_path})...')\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.gt_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_gt_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset ground truth image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'Loading tiled ground truth image numpy array from file ({file_path})...')\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.gt_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def get_gt_class_statistics(self, tiles=None, print_results=False):\n",
    "        \"\"\"\n",
    "        Outputs statistics per each class per ground truth tile (or, if\n",
    "        no tile or set of tile is specified, the whole ground truth).\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image tile member variable is empty, then load all\n",
    "        # ground truth image tiles\n",
    "        if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "        # Initialize statistics dictionary\n",
    "        statistics = {}\n",
    "\n",
    "        # If no tiles are specified, use all ground truth tiles\n",
    "        if tiles is None:\n",
    "            tiles = []\n",
    "            for row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for col in range(0, self.dataset_tiled_subset_cols):\n",
    "                    tiles.append((row, col))\n",
    "\n",
    "        # Iterate through the tile list to get statistics for each\n",
    "        # individual tile\n",
    "        for tile in tiles:\n",
    "            row, col = tile\n",
    "            index = row * self.dataset_tiled_subset_cols + col\n",
    "\n",
    "            # Create tile statistics mapping\n",
    "            tile_statistics = {x:0 for x in range(0,len(self.gt_class_label_list))}\n",
    "\n",
    "            # Count the class of each pixel in the image tile\n",
    "            for pixel in np.ravel(self.gt_image_tiles[index]):\n",
    "                tile_statistics[pixel] += 1\n",
    "\n",
    "            # Create key/value pair for statistics dictionary\n",
    "            key = f'Tile ({row}, {col})'\n",
    "            value = [tile_statistics[i] for i in range(1,len(self.gt_class_label_list))]\n",
    "\n",
    "            # Add key value pair to dictionary\n",
    "            statistics[key] = value\n",
    "\n",
    "        # Create Pandas DataFrame from statistics dictionary and set the\n",
    "        # index to be the class labels\n",
    "        statistics_df = pd.DataFrame(data=statistics)\n",
    "        statistics_df.index = self.gt_class_label_list[1:]\n",
    "\n",
    "        # Print out statistics\n",
    "        if print_results:\n",
    "            print(statistics_df)\n",
    "            print()\n",
    "            print(statistics_df.T.describe())\n",
    "\n",
    "        return statistics_df\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_hs_image(self, gsd=UH_2018_GT_GSD,\n",
    "                           thres=True, normalize=True,\n",
    "                           resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the full-size hyperspectral image for the University of\n",
    "        Houston 2018 dataset sampled at the specified GSD.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading full hyperspectral image...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_hs / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's hyperspectral image\n",
    "        image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                  self.path_to_hs_image)\n",
    "\n",
    "        # Throw error if file path does not exist\n",
    "        if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 hyperspectral image is invalid! Path={image_path}')\n",
    "\n",
    "        # Create image variable\n",
    "        self.hs_image = None\n",
    "\n",
    "        # Open the training HSI Envi file as src\n",
    "        with rasterio.open(image_path, format='ENVI') as src:\n",
    "\n",
    "            # Read the image, resample it to the appropriate GSD,\n",
    "            # arrange the numpy array to be (rows, cols, bands),\n",
    "            # and remove unused bands\n",
    "            if resample_factor == 1.0:\n",
    "                self.hs_image = np.moveaxis(src.read(), 0, -1)[:,:,:-2]\n",
    "            else:\n",
    "                # Set the shape of the resampled image\n",
    "                out_shape=(src.count,\n",
    "                            int(src.height * resample_factor),\n",
    "                            int(src.width * resample_factor))\n",
    "\n",
    "                self.hs_image = np.moveaxis(src.read(\n",
    "                            out_shape=out_shape,\n",
    "                            resampling=resampling), 0, -1)[:,:,:-2]\n",
    "\n",
    "            # Cast image array as float type for normalization\n",
    "            if normalize:\n",
    "                self.hs_image = self.hs_image.astype(float, copy=False)\n",
    "\n",
    "            # Threshold the image so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.hs_image[self.hs_image > self.hs_band_val_thres] = self.hs_image.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.hs_image -= self.hs_image.min()\n",
    "                self.hs_image /= self.hs_image.max()\n",
    "\n",
    "\n",
    "        return self.hs_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_hs_image_tiles(self, gsd=UH_2018_GT_GSD, tile_list=None,\n",
    "                            thres=True, normalize=True, resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's hyperspectral\n",
    "        images as a set of tiles sampled at a specified GSD. If no tile\n",
    "        list is given, the whole image will be loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading hyperspectral image as tiles...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Check tile_list parameter value\n",
    "        if tile_list and not isinstance(tile_list, tuple): raise ValueError(\n",
    "            \"'tile_list' parameter should be a tuple of tuples!\")\n",
    "\n",
    "        # Initialize list for tiles\n",
    "        self.hs_image_tiles = []\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_hs / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's hyperspectral image\n",
    "        image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                  self.path_to_hs_image)\n",
    "\n",
    "        # Throw error if file path does not exist\n",
    "        if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 hyperspectral image is invalid! Path={image_path}')\n",
    "\n",
    "        # Open the training HSI Envi file as src\n",
    "        with rasterio.open(image_path, format='ENVI') as src:\n",
    "            # Get the size of the tile windows\n",
    "            tile_width = src.width / self.dataset_tiled_subset_cols\n",
    "            tile_height = src.height / self.dataset_tiled_subset_rows\n",
    "\n",
    "            # Set the shape of the resampled tile\n",
    "            out_shape=(src.count,\n",
    "                    int(tile_height * resample_factor),\n",
    "                    int(tile_width * resample_factor))\n",
    "\n",
    "            # Read in the image data for each image tile\n",
    "            for tile_row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for tile_column in range(0, self.dataset_tiled_subset_cols):\n",
    "\n",
    "                    # If specified tiles are desired, then skip any\n",
    "                    # tiles that do not match the tile_list parameter\n",
    "                    if tile_list and (tile_row, tile_column) not in tile_list:\n",
    "                        continue\n",
    "\n",
    "                    # Set the tile window to read from the image\n",
    "                    window = Window(tile_width * tile_column,\n",
    "                                    tile_height * tile_row,\n",
    "                                    tile_width, tile_height)\n",
    "\n",
    "                    # Read the tile window from the image, resample it to\n",
    "                    # the appropriate GSD, arrange the numpy array to be\n",
    "                    # (rows, cols, bands), and remove unused bands\n",
    "                    if resample_factor == 1.0:\n",
    "                        tile = np.moveaxis(src.read(window = window), 0, -1)[:,:,:-2]\n",
    "                    else:\n",
    "                        tile = np.moveaxis(src.read(\n",
    "                            window = window,\n",
    "                            out_shape=out_shape,\n",
    "                            resampling=resampling), 0, -1)[:,:,:-2]\n",
    "\n",
    "                    # Copy the tile to the tiles array\n",
    "                    self.hs_image_tiles.append(np.copy(tile))\n",
    "\n",
    "        # If no tiles were added to the tile list, then set image tiles\n",
    "        # variable to 'None'\n",
    "        if len(self.hs_image_tiles) == 0: self.hs_image_tiles = None\n",
    "        else:\n",
    "            # Turn list of numpy arrays into single numpy array\n",
    "            self.hs_image_tiles = np.stack(self.hs_image_tiles)\n",
    "\n",
    "            # Cast image array as float type for normalization\n",
    "            if normalize:\n",
    "                self.hs_image_tiles = self.hs_image_tiles.astype(float, copy=False)\n",
    "\n",
    "            # Threshold the image tiles so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.hs_image_tiles[self.hs_image_tiles > self.hs_band_val_thres] = self.hs_image_tiles.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.hs_image_tiles -= self.hs_image_tiles.min()\n",
    "                self.hs_image_tiles /= self.hs_image_tiles.max()\n",
    "\n",
    "        return self.hs_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_hs_image_array(self, path, file_name='full_hs_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full hyperspectral image to a file\n",
    "        for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image member variable is empty, then load the full\n",
    "        # hyperspectral image\n",
    "        if self.hs_image is None: self.load_full_hs_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.hs_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_hs_image_array(self, path, file_name='tiled_hs_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled hyperspectral image to a file\n",
    "        for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image tile member variable is empty, then load all\n",
    "        # hyperspectral image tiles\n",
    "        if self.hs_image_tiles is None: self.load_hs_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.hs_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_hs_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset hyperspectral image.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.hs_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_hs_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset hyperspectral image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.hs_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def show_hs_image(self, rgb_channels=None, size=(15,9),\n",
    "                      full_gt_overlay=False,\n",
    "                      train_gt_overlay=False,\n",
    "                      test_gt_overlay=False):\n",
    "        \"\"\"\n",
    "        Displays the hyperspectral image using the specified band\n",
    "        channels as the rgb values.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image member variable is empty, then load the full\n",
    "        # hyperspectral image\n",
    "        if self.hs_image is None: self.load_full_hs_image()\n",
    "\n",
    "        image = self.hs_image[:,:,:]\n",
    "\n",
    "        if full_gt_overlay:\n",
    "            # If the hs image member variable is empty, then load the full\n",
    "            # ground truth image\n",
    "            if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "            classes = self.gt_image\n",
    "            title = 'Hyperspectral image w/ ground truth overlay'\n",
    "        elif test_gt_overlay or train_gt_overlay:\n",
    "            # If the hs image member variable is empty, then load the\n",
    "            # ground truth image tiles\n",
    "            if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "            # create a copy of ground truth image tiles\n",
    "            gt_tiles = self.gt_image_tiles.copy()\n",
    "\n",
    "            # Get tile dimensions\n",
    "            tile_shape = gt_tiles[0].shape\n",
    "\n",
    "            # Choose which set of tiles to set to zero values and\n",
    "            # set proper image title\n",
    "            if train_gt_overlay:\n",
    "                tiles_to_remove = self.dataset_testing_subset\n",
    "                title = 'Hyperspectral image w/ training ground truth overlay'\n",
    "            else:\n",
    "                tiles_to_remove = self.dataset_training_subset\n",
    "                title = 'Hyperspectral image w/ testing ground truth overlay'\n",
    "\n",
    "            # Zero out tiles not in the desired subset\n",
    "            for tile in tiles_to_remove:\n",
    "                row, col = tile\n",
    "                index = row * self.dataset_tiled_subset_cols + col\n",
    "                gt_tiles[index] = np.zeros(tile_shape, dtype=np.uint8)\n",
    "\n",
    "            # Create single ground truth image mask\n",
    "            classes = self.merge_tiles(gt_tiles)\n",
    "\n",
    "        else:\n",
    "            classes = None\n",
    "            title = 'Hyperspectral image'\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        view = spectral.imshow(image,\n",
    "                               source=image,\n",
    "                               bands=rgb_channels,\n",
    "                               classes=classes,\n",
    "                               figsize=size)\n",
    "        if (full_gt_overlay\n",
    "            or test_gt_overlay\n",
    "            or train_gt_overlay): view.set_display_mode('overlay')\n",
    "\n",
    "        view.set_title(title)\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_hs_data_cube(self, size=(1200,900)):\n",
    "        \"\"\"\n",
    "        Creates 3-D visualization of the hyperspectral data cube for\n",
    "        the hyperspectral image.\n",
    "        \"\"\"\n",
    "\n",
    "        import wx\n",
    "\n",
    "        # If the hs image member variable is empty, then load the full\n",
    "        # hyperspectral image\n",
    "        if self.hs_image is None: self.load_full_hs_image()\n",
    "\n",
    "        # Setup WxApp to display 3D spectral cube\n",
    "        app = wx.App(False)\n",
    "\n",
    "        # View 3D hyperspectral cube image\n",
    "        spectral.view_cube(self.hs_image, size=size)\n",
    "\n",
    "        # Prevent app from closing immediately\n",
    "        app.MainLoop()\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_ms_image(self, gsd=UH_2018_GT_GSD,\n",
    "                                 thres=True, normalize=True,\n",
    "                                 resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the full-size lidar multispectral intensisty image for the\n",
    "        University of Houston 2018 dataset sampled at the specified GSD.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading full LiDAR multispectral intensity image...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's hyperspectral image\n",
    "        c1_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                     self.path_to_lidar_1550nm_intensity)\n",
    "        c2_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                     self.path_to_lidar_1064nm_intensity)\n",
    "        c3_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                     self.path_to_lidar_532nm_intensity)\n",
    "\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(c1_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 1550nm LiDAR intensity image is invalid! Path={c1_image_path}')\n",
    "        if not os.path.isfile(c2_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 1064nm LiDAR intensity image is invalid! Path={c2_image_path}')\n",
    "        if not os.path.isfile(c3_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 532nm LiDAR intensity image is invalid! Path={c3_image_path}')\n",
    "\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_ms_image = None\n",
    "\n",
    "        # Open the LiDAR multispectral intensity image files as c1_src\n",
    "        # (1550nm), c2_src (1064nm), and c3_src (532nm)\n",
    "        with rasterio.open(c1_image_path) as c1_src, \\\n",
    "             rasterio.open(c2_image_path) as c2_src, \\\n",
    "             rasterio.open(c3_image_path) as c3_src:\n",
    "\n",
    "            # Read the image, resample it to the appropriate GSD,\n",
    "            # arrange the numpy array to be (rows, cols, bands)\n",
    "            if resample_factor == 1.0:\n",
    "                c1 = np.moveaxis(c1_src.read(), 0, -1)\n",
    "                c2 = np.moveaxis(c2_src.read(), 0, -1)\n",
    "                c3 = np.moveaxis(c3_src.read(), 0, -1)\n",
    "            else:\n",
    "                # Set the shape of the resampled image\n",
    "                c1_out_shape=(c1_src.count,\n",
    "                            int(c1_src.height * resample_factor),\n",
    "                            int(c1_src.width * resample_factor))\n",
    "                c2_out_shape=(c2_src.count,\n",
    "                            int(c2_src.height * resample_factor),\n",
    "                            int(c2_src.width * resample_factor))\n",
    "                c3_out_shape=(c3_src.count,\n",
    "                            int(c3_src.height * resample_factor),\n",
    "                            int(c3_src.width * resample_factor))\n",
    "\n",
    "                # Read the image, resample it to the appropriate GSD,\n",
    "                # arrange the numpy array to be (rows, cols, bands)\n",
    "                c1 = np.moveaxis(c1_src.read(out_shape=c1_out_shape,\n",
    "                                resampling=resampling), 0, -1)\n",
    "                c2 = np.moveaxis(c2_src.read(out_shape=c2_out_shape,\n",
    "                                resampling=resampling), 0, -1)\n",
    "                c3 = np.moveaxis(c3_src.read(out_shape=c3_out_shape,\n",
    "                                resampling=resampling), 0, -1)\n",
    "\n",
    "            # Stack each intensity band into a single cube\n",
    "            self.lidar_ms_image = np.dstack((c1, c2, c3))\n",
    "\n",
    "            # Threshold the image so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.lidar_ms_image[self.lidar_ms_image > self.lidar_ms_intensity_thres] = self.lidar_ms_image.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_ms_image -= self.lidar_ms_image.min()\n",
    "                self.lidar_ms_image /= self.lidar_ms_image.max()\n",
    "\n",
    "        return self.lidar_ms_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_lidar_ms_image_tiles(self, gsd=UH_2018_GT_GSD, tile_list=None,\n",
    "                                  thres=True, normalize=True,\n",
    "                                  resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's lidar\n",
    "        multispectral intensity image as a set of tiles sampled at a\n",
    "        specified GSD. If no tile list is given, the whole image will be\n",
    "        loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading LiDAR multispectral intensity image tiles...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Check tile_list parameter value\n",
    "        if tile_list and not isinstance(tile_list, tuple): raise ValueError(\n",
    "            \"'tile_list' parameter should be a tuple of tuples!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's hyperspectral image\n",
    "        c1_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                     self.path_to_lidar_1550nm_intensity)\n",
    "        c2_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                     self.path_to_lidar_1064nm_intensity)\n",
    "        c3_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                     self.path_to_lidar_532nm_intensity)\n",
    "\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(c1_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 1550nm LiDAR intensity image is invalid! Path={c1_image_path}')\n",
    "        if not os.path.isfile(c2_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 1064nm LiDAR intensity image is invalid! Path={c2_image_path}')\n",
    "        if not os.path.isfile(c3_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 532nm LiDAR intensity image is invalid! Path={c3_image_path}')\n",
    "\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_ms_image_tiles = []\n",
    "\n",
    "        # Open the LiDAR multispectral intensity image files as c1_src\n",
    "        # (1550nm), c2_src (1064nm), and c3_src (532nm)\n",
    "        with rasterio.open(c1_image_path) as c1_src, \\\n",
    "             rasterio.open(c2_image_path) as c2_src, \\\n",
    "             rasterio.open(c3_image_path) as c3_src:\n",
    "\n",
    "            # Get the size of the tile windows\n",
    "            c1_tile_width = c1_src.width / self.dataset_tiled_subset_cols\n",
    "            c1_tile_height = c1_src.height / self.dataset_tiled_subset_rows\n",
    "            c2_tile_width = c1_src.width / self.dataset_tiled_subset_cols\n",
    "            c2_tile_height = c1_src.height / self.dataset_tiled_subset_rows\n",
    "            c3_tile_width = c1_src.width / self.dataset_tiled_subset_cols\n",
    "            c3_tile_height = c1_src.height / self.dataset_tiled_subset_rows\n",
    "\n",
    "            # Set the shape of the resampled tile\n",
    "            c1_out_shape=(c1_src.count,\n",
    "                int(c1_src.height * resample_factor),\n",
    "                int(c1_src.width * resample_factor))\n",
    "            c2_out_shape=(c2_src.count,\n",
    "                        int(c2_src.height * resample_factor),\n",
    "                        int(c2_src.width * resample_factor))\n",
    "            c3_out_shape=(c3_src.count,\n",
    "                        int(c3_src.height * resample_factor),\n",
    "                        int(c3_src.width * resample_factor))\n",
    "\n",
    "            # Read in the image data for each image tile\n",
    "            for tile_row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for tile_column in range(0, self.dataset_tiled_subset_cols):\n",
    "\n",
    "                    # If specified tiles are desired, then skip any\n",
    "                    # tiles that do not match the tile_list parameter\n",
    "                    if tile_list and (tile_row, tile_column) not in tile_list:\n",
    "                        continue\n",
    "\n",
    "                    # Set the tile window to read from the image\n",
    "                    c1_window = Window(c1_tile_width * tile_column,\n",
    "                                    c1_tile_height * tile_row,\n",
    "                                    c1_tile_width, c1_tile_height)\n",
    "                    c2_window = Window(c2_tile_width * tile_column,\n",
    "                                    c2_tile_height * tile_row,\n",
    "                                    c2_tile_width, c2_tile_height)\n",
    "                    c3_window = Window(c3_tile_width * tile_column,\n",
    "                                    c3_tile_height * tile_row,\n",
    "                                    c3_tile_width, c3_tile_height)\n",
    "\n",
    "                    # Read the tile window from the image, resample it to\n",
    "                    # the appropriate GSD, arrange the numpy array to be\n",
    "                    # (rows, cols, bands)\n",
    "                    if resample_factor == 1.0:\n",
    "                        c1_tile = np.moveaxis(c1_src.read(window = c1_window), 0, -1)\n",
    "                        c2_tile = np.moveaxis(c2_src.read(window = c2_window), 0, -1)\n",
    "                        c3_tile = np.moveaxis(c3_src.read(window = c3_window), 0, -1)\n",
    "                    else:\n",
    "                        c1_tile = np.moveaxis(c1_src.read(\n",
    "                                        window = c1_window,\n",
    "                                        out_shape=c1_out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "                        c2_tile = np.moveaxis(c2_src.read(\n",
    "                                        window = c2_window,\n",
    "                                        out_shape=c2_out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "                        c3_tile = np.moveaxis(c3_src.read(\n",
    "                                        window = c3_window,\n",
    "                                        out_shape=c3_out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "\n",
    "                    # Copy the tile to the tiles array\n",
    "                    self.lidar_ms_image_tiles.append(np.dstack((c1_tile,\n",
    "                                                                c2_tile,\n",
    "                                                                c3_tile)))\n",
    "\n",
    "        # If no tiles were added to the tile list, then set image tiles\n",
    "        # variable to 'None'\n",
    "        if len(self.lidar_ms_image_tiles) == 0: self.lidar_ms_image_tiles = None\n",
    "        else:\n",
    "            # Turn list of numpy arrays into single numpy array\n",
    "            self.lidar_ms_image_tiles = np.stack(self.lidar_ms_image_tiles)\n",
    "\n",
    "            # Threshold the image tiles so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.lidar_ms_image_tiles[self.lidar_ms_image_tiles > self.lidar_ms_intensity_thres] = self.lidar_ms_image_tiles.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_ms_image_tiles -= self.lidar_ms_image_tiles.min()\n",
    "                self.lidar_ms_image_tiles /= self.lidar_ms_image_tiles.max()\n",
    "\n",
    "        return self.lidar_ms_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_lidar_ms_image_array(self, path, file_name='full_lidar_multispectral_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full lidar multispectral image to a\n",
    "        file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar multispectral intensity image member variable is\n",
    "        # empty, then load the full lidar multispectral intensity image\n",
    "        if self.lidar_ms_image is None: self.load_full_lidar_ms_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_ms_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_lidar_ms_image_array(self, path, file_name='tiled_lidar_multispectral_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled lidar multispectral image to\n",
    "        a file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar multispectral intensity image tile member\n",
    "        # variable is empty, then load all lidar multispectral intensity\n",
    "        # image tiles\n",
    "        if self.lidar_ms_image_tiles is None: self.load_lidar_ms_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_ms_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_ms_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset lidar multispectral intensity image.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_ms_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_lidar_ms_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset lidar multispectral intensity image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_ms_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def show_lidar_ms_image(self, size=(15,9),\n",
    "                            full_gt_overlay=False,\n",
    "                            train_gt_overlay=False,\n",
    "                            test_gt_overlay=False):\n",
    "        \"\"\"\n",
    "        Displays the multispectral lidar image in RGB with red=1550nm,\n",
    "        green=1064nm, and blue=532nm bands, with optional ground truth\n",
    "        overlay on the image.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the LiDAR multispectral intensity image member variable is\n",
    "        # empty, then load the full lidar multispectral intensity image\n",
    "        if self.lidar_ms_image is None: self.load_full_lidar_ms_image()\n",
    "\n",
    "        image = self.lidar_ms_image[:,:,:]\n",
    "\n",
    "        if full_gt_overlay:\n",
    "            # If the ground truth image member variable is empty, then\n",
    "            # load the full ground truth image\n",
    "            if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "            classes = self.gt_image\n",
    "            title = 'LiDAR multispectral intensity image w/ ground truth overlay'\n",
    "        elif test_gt_overlay or train_gt_overlay:\n",
    "            # If the ground truth image tiles member variable is empty,\n",
    "            # then load the ground truth image tiles\n",
    "            if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "            # create a copy of ground truth image tiles\n",
    "            gt_tiles = self.gt_image_tiles.copy()\n",
    "\n",
    "            # Get tile dimensions\n",
    "            tile_shape = gt_tiles[0].shape\n",
    "\n",
    "            # Choose which set of tiles to set to zero values and\n",
    "            # set proper image title\n",
    "            if train_gt_overlay:\n",
    "                tiles_to_remove = self.dataset_testing_subset\n",
    "                title = 'LiDAR multispectral intensity image w/ training ground truth overlay'\n",
    "            else:\n",
    "                tiles_to_remove = self.dataset_training_subset\n",
    "                title = 'LiDAR multispectral intensity image w/ testing ground truth overlay'\n",
    "\n",
    "            # Zero out tiles not in the desired subset\n",
    "            for tile in tiles_to_remove:\n",
    "                row, col = tile\n",
    "                index = row * self.dataset_tiled_subset_cols + col\n",
    "                gt_tiles[index] = np.zeros(tile_shape, dtype=np.uint8)\n",
    "\n",
    "            # Create single ground truth image mask\n",
    "            classes = self.merge_tiles(gt_tiles)\n",
    "\n",
    "        else:\n",
    "            classes = None\n",
    "            title = 'LiDAR multispectral intensity image'\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        view = spectral.imshow(image,\n",
    "                               source=image,\n",
    "                               classes=classes,\n",
    "                               figsize=size)\n",
    "        if (full_gt_overlay\n",
    "            or test_gt_overlay\n",
    "            or train_gt_overlay): view.set_display_mode('overlay')\n",
    "\n",
    "        view.set_title(title)\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_dsm_image(self, gsd=UH_2018_GT_GSD,\n",
    "                                  thres=True, normalize=True,\n",
    "                                  resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the full-size LiDAR digital surface model (DSM) image for\n",
    "        the University of Houston 2018 dataset sampled at the specified\n",
    "        GSD.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading full LiDAR DSM image...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's LiDAR DSM image\n",
    "        image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                  self.path_to_lidar_dsm)\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DSM image is invalid! Path={image_path}')\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_dsm_image = None\n",
    "\n",
    "        # Open the LiDAR DSM file as src\n",
    "        with rasterio.open(image_path) as src:\n",
    "\n",
    "            # Read the image, resample it to the appropriate GSD,\n",
    "            # arrange the numpy array to be (rows, cols, bands),\n",
    "            # and remove unused bands\n",
    "            if resample_factor == 1.0:\n",
    "                self.lidar_dsm_image = np.moveaxis(src.read(), 0, -1)\n",
    "            else:\n",
    "                # Set the shape of the resampled image\n",
    "                out_shape=(src.count,\n",
    "                        int(src.height * resample_factor),\n",
    "                        int(src.width * resample_factor))\n",
    "\n",
    "                self.lidar_dsm_image = np.moveaxis(src.read(\n",
    "                                                out_shape=out_shape,\n",
    "                                                resampling=resampling), 0, -1)\n",
    "\n",
    "            # Threshold the image so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.lidar_dsm_image[self.lidar_dsm_image > self.lidar_dsm_thres] = self.lidar_dsm_image.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_dsm_image -= self.lidar_dsm_image.min()\n",
    "                self.lidar_dsm_image /= self.lidar_dsm_image.max()\n",
    "\n",
    "        return self.lidar_dsm_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_lidar_dsm_image_tiles(self, gsd=UH_2018_GT_GSD, tile_list=None,\n",
    "                                   thres=True, normalize=True, resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's LiDAR digital\n",
    "        surface model (DSM) image as a set of tiles sampled at a\n",
    "        specified GSD. If no tile list is given, the whole image will be\n",
    "        loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading LiDAR DSM image tiles...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Check tile_list parameter value\n",
    "        if tile_list and not isinstance(tile_list, tuple): raise ValueError(\n",
    "            \"'tile_list' parameter should be a tuple of tuples!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's LiDAR DSM image\n",
    "        image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                  self.path_to_lidar_dsm)\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DSM image is invalid! Path={image_path}')\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_dsm_image_tiles = []\n",
    "\n",
    "        # Open the LiDAR DSM file as src\n",
    "        with rasterio.open(image_path) as src:\n",
    "\n",
    "            # Set the shape of the resampled image\n",
    "            out_shape=(src.count,\n",
    "                       int(src.height * resample_factor),\n",
    "                       int(src.width * resample_factor))\n",
    "\n",
    "            # Get the size of the tile windows\n",
    "            tile_width = src.width / self.dataset_tiled_subset_cols\n",
    "            tile_height = src.height / self.dataset_tiled_subset_rows\n",
    "\n",
    "            # Read in the image data for each image tile\n",
    "            for tile_row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for tile_column in range(0, self.dataset_tiled_subset_cols):\n",
    "\n",
    "                    # If specified tiles are desired, then skip any\n",
    "                    # tiles that do not match the tile_list parameter\n",
    "                    if tile_list and (tile_row, tile_column) not in tile_list:\n",
    "                        continue\n",
    "\n",
    "                    # Set the tile window to read from the image\n",
    "                    window = Window(tile_width * tile_column,\n",
    "                                    tile_height * tile_row,\n",
    "                                    tile_width, tile_height)\n",
    "\n",
    "\n",
    "                    # Read the tile window from the image, resample it to\n",
    "                    # the appropriate GSD, arrange the numpy array to be\n",
    "                    # (rows, cols, bands), and remove unused bands\n",
    "                    if resample_factor == 1.0:\n",
    "                        tile = np.moveaxis(src.read(window = window), 0, -1)\n",
    "                    else:\n",
    "                        tile = np.moveaxis(src.read(\n",
    "                                        window = window,\n",
    "                                        out_shape=out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "\n",
    "                    # Copy the tile to the tiles array\n",
    "                    self.lidar_dsm_image_tiles.append(tile)\n",
    "\n",
    "        # If no tiles were added to the tile list, then set image tiles\n",
    "        # variable to 'None'\n",
    "        if len(self.lidar_dsm_image_tiles) == 0: self.lidar_dsm_image_tiles = None\n",
    "        else:\n",
    "            # Turn list of numpy arrays into single numpy array\n",
    "            self.lidar_dsm_image_tiles = np.stack(self.lidar_dsm_image_tiles)\n",
    "\n",
    "            # Threshold the image tiles so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.lidar_dsm_image_tiles[self.lidar_dsm_image_tiles > self.lidar_dsm_thres] = self.lidar_dsm_image_tiles.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_dsm_image_tiles -= self.lidar_dsm_image_tiles.min()\n",
    "                self.lidar_dsm_image_tiles /= self.lidar_dsm_image_tiles.max()\n",
    "\n",
    "        return self.lidar_dsm_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_lidar_dsm_image_array(self, path, file_name='full_lidar_dsm_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full LiDAR dsm image to a\n",
    "        file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dsm image member variable is empty, then load the full\n",
    "        # lidar image\n",
    "        if self.lidar_dsm_image is None: self.load_full_lidar_dsm_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_dsm_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_lidar_dsm_image_array(self, path, file_name='tiled_lidar_dsm_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled LiDAR dsm image to\n",
    "        a file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dsm image tile member variable is empty, then load all\n",
    "        # lidar dsm image tiles\n",
    "        if self.lidar_dsm_image_tiles is None: self.load_lidar_dsm_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_dsm_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_dsm_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset LiDAR digital surface model image.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_dsm_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_lidar_dsm_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset LiDAR digital surface model image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_dsm_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def show_lidar_dsm_image(self, size=(15,9),\n",
    "                            full_gt_overlay=False,\n",
    "                            train_gt_overlay=False,\n",
    "                            test_gt_overlay=False):\n",
    "        \"\"\"\n",
    "        Displays the LiDAR dsm image with optional ground truth\n",
    "        overlay on the image.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dsm image member variable is empty, then load the\n",
    "        # full lidar dsm image\n",
    "        if self.lidar_dsm_image is None: self.load_full_lidar_dsm_image()\n",
    "\n",
    "        image = self.lidar_dsm_image[:,:,:]\n",
    "\n",
    "        if full_gt_overlay:\n",
    "            # If the ground truth image member variable is empty, then\n",
    "            # load the full ground truth image\n",
    "            if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "            classes = self.gt_image\n",
    "            title = 'LiDAR Digital Surface Model (DSM) image w/ ground truth overlay'\n",
    "        elif test_gt_overlay or train_gt_overlay:\n",
    "            # If the ground truth tiles image member variable is empty,\n",
    "            # then load the ground truth image tiles\n",
    "            if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "            # create a copy of ground truth image tiles\n",
    "            gt_tiles = self.gt_image_tiles.copy()\n",
    "\n",
    "            # Get tile dimensions\n",
    "            tile_shape = gt_tiles[0].shape\n",
    "\n",
    "            # Choose which set of tiles to set to zero values and\n",
    "            # set proper image title\n",
    "            if train_gt_overlay:\n",
    "                tiles_to_remove = self.dataset_testing_subset\n",
    "                title = 'LiDAR Digital Surface Model (DSM) image w/ training ground truth overlay'\n",
    "            else:\n",
    "                tiles_to_remove = self.dataset_training_subset\n",
    "                title = 'LiDAR Digital Surface Model (DSM) image w/ testing ground truth overlay'\n",
    "\n",
    "            # Zero out tiles not in the desired subset\n",
    "            for tile in tiles_to_remove:\n",
    "                row, col = tile\n",
    "                index = row * self.dataset_tiled_subset_cols + col\n",
    "                gt_tiles[index] = np.zeros(tile_shape, dtype=np.uint8)\n",
    "\n",
    "            # Create single ground truth image mask\n",
    "            classes = self.merge_tiles(gt_tiles)\n",
    "\n",
    "        else:\n",
    "            classes = None\n",
    "            title = 'LiDAR Digital Surface Model (DSM) image'\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        view = spectral.imshow(image,\n",
    "                               source=image,\n",
    "                               classes=classes,\n",
    "                               figsize=size)\n",
    "        if (full_gt_overlay\n",
    "            or test_gt_overlay\n",
    "            or train_gt_overlay): view.set_display_mode('overlay')\n",
    "\n",
    "        view.set_title(title)\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_dem_image(self, gsd=UH_2018_GT_GSD,\n",
    "                                  use_void_filling_model = False,\n",
    "                                  use_hybrid_model = True,\n",
    "                                  thres=True, normalize=True,\n",
    "                                  resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the full-size lidar digital elevation model (DEM) image\n",
    "        for the University of Houston 2018 dataset sampled at the\n",
    "        specified GSD.\n",
    "        \"\"\"\n",
    "\n",
    "        if use_hybrid_model:\n",
    "            dem_path = self.path_to_lidar_dem_b\n",
    "            print('Loading full LiDAR hybrid DEM image...')\n",
    "        elif use_void_filling_model:\n",
    "            dem_path = self.path_to_lidar_dem_tli\n",
    "            print('Loading full LiDAR bare-earth elevation w/ void filling DEM image...')\n",
    "        else:\n",
    "            dem_path = self.path_to_lidar_dem_3msr\n",
    "            print('Loading full LiDAR bare-earth elevation DEM image...')\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's LiDAR DEM image\n",
    "        image_path = os.path.join(self.path_to_dataset_directory, dem_path)\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DEM image is invalid! Path={image_path}')\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_dem_image = None\n",
    "\n",
    "        # Open the LiDAR DSM file as src\n",
    "        with rasterio.open(image_path) as src:\n",
    "\n",
    "            # Read the image, resample it to the appropriate GSD,\n",
    "            # arrange the numpy array to be (rows, cols, bands),\n",
    "            # and remove unused bands\n",
    "            if resample_factor == 1.0:\n",
    "                self.lidar_dem_image = np.moveaxis(src.read(), 0, -1)\n",
    "            else:\n",
    "                # Set the shape of the resampled image\n",
    "                out_shape=(src.count,\n",
    "                        int(src.height * resample_factor),\n",
    "                        int(src.width * resample_factor))\n",
    "\n",
    "                self.lidar_dem_image = np.moveaxis(src.read(\n",
    "                                                out_shape=out_shape,\n",
    "                                                resampling=resampling), 0, -1)\n",
    "\n",
    "            # Threshold the image so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.lidar_dem_image[self.lidar_dem_image > self.lidar_dsm_thres] = self.lidar_dem_image.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_dem_image -= self.lidar_dem_image.min()\n",
    "                self.lidar_dem_image /= self.lidar_dem_image.max()\n",
    "\n",
    "        return self.lidar_dem_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_lidar_dem_image_tiles(self, gsd=UH_2018_GT_GSD, tile_list=None,\n",
    "                                   use_void_filling_model = False,\n",
    "                                   use_hybrid_model = True,\n",
    "                                   thres=True, normalize=True,\n",
    "                                   resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's lidar digital\n",
    "        elevation model (DEM) image as a set of tiles sampled at a\n",
    "        specified GSD. If no tile list is given, the whole image will be\n",
    "        loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        if use_hybrid_model:\n",
    "            dem_path = self.path_to_lidar_dem_b\n",
    "            print('Loading full LiDAR hybrid DEM image...')\n",
    "        elif use_void_filling_model:\n",
    "            dem_path = self.path_to_lidar_dem_tli\n",
    "            print('Loading full LiDAR bare-earth elevation w/ void filling DEM image...')\n",
    "        else:\n",
    "            dem_path = self.path_to_lidar_dem_3msr\n",
    "            print('Loading full LiDAR bare-earth elevation DEM image...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Check tile_list parameter value\n",
    "        if tile_list and not isinstance(tile_list, tuple): raise ValueError(\n",
    "            \"'tile_list' parameter should be a tuple of tuples!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's LiDAR DSM image\n",
    "        image_path = os.path.join(self.path_to_dataset_directory, dem_path)\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DEM image is invalid! Path={image_path}')\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_dem_image_tiles = []\n",
    "\n",
    "        # Open the LiDAR DSM file as src\n",
    "        with rasterio.open(image_path) as src:\n",
    "\n",
    "            # Set the shape of the resampled image\n",
    "            out_shape=(src.count,\n",
    "                       int(src.height * resample_factor),\n",
    "                       int(src.width * resample_factor))\n",
    "\n",
    "            # Get the size of the tile windows\n",
    "            tile_width = src.width / self.dataset_tiled_subset_cols\n",
    "            tile_height = src.height / self.dataset_tiled_subset_rows\n",
    "\n",
    "            # Read in the image data for each image tile\n",
    "            for tile_row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for tile_column in range(0, self.dataset_tiled_subset_cols):\n",
    "\n",
    "                    # If specified tiles are desired, then skip any\n",
    "                    # tiles that do not match the tile_list parameter\n",
    "                    if tile_list and (tile_row, tile_column) not in tile_list:\n",
    "                        continue\n",
    "\n",
    "                    # Set the tile window to read from the image\n",
    "                    window = Window(tile_width * tile_column,\n",
    "                                    tile_height * tile_row,\n",
    "                                    tile_width, tile_height)\n",
    "\n",
    "                    # Read the tile window from the image, resample it to\n",
    "                    # the appropriate GSD, arrange the numpy array to be\n",
    "                    # (rows, cols, bands), and remove unused bands\n",
    "                    if resample_factor == 1.0:\n",
    "                        tile = np.moveaxis(src.read(window = window), 0, -1)\n",
    "                    else:\n",
    "                        tile = np.moveaxis(src.read(\n",
    "                                        window = window,\n",
    "                                        out_shape=out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "\n",
    "                    # Copy the tile to the tiles array\n",
    "                    self.lidar_dem_image_tiles.append(tile)\n",
    "\n",
    "        # If no tiles were added to the tile list, then set image tiles\n",
    "        # variable to 'None'\n",
    "        if len(self.lidar_dem_image_tiles) == 0: self.lidar_dem_image_tiles = None\n",
    "        else:\n",
    "            # Turn list of numpy arrays into single numpy array\n",
    "            self.lidar_dem_image_tiles = np.stack(self.lidar_dem_image_tiles)\n",
    "\n",
    "            # Threshold the image tiles so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                self.lidar_dem_image_tiles[self.lidar_dem_image_tiles > self.lidar_dsm_thres] = self.lidar_dem_image_tiles.min()\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_dem_image_tiles -= self.lidar_dem_image_tiles.min()\n",
    "                self.lidar_dem_image_tiles /= self.lidar_dem_image_tiles.max()\n",
    "\n",
    "        return self.lidar_dem_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_lidar_dem_image_array(self, path, file_name='full_lidar_dem_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full lidar dem image to a\n",
    "        file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dsm image member variable is empty, then load the full\n",
    "        # lidar image\n",
    "        if self.lidar_dem_image is None: self.load_full_lidar_dem_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_dsm_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_lidar_dem_image_array(self, path, file_name='tiled_lidar_dem_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled lidar dem image to\n",
    "        a file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dsm image tile member variable is empty, then load all\n",
    "        # lidar dsm image tiles\n",
    "        if self.lidar_dem_image_tiles is None: self.load_lidar_dem_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_dsm_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_dem_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset lidar digital elevation model image.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_dem_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_lidar_dem_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset lidar digital elevation model image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_dsm_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def show_lidar_dem_image(self, size=(15,9),\n",
    "                            full_gt_overlay=False,\n",
    "                            train_gt_overlay=False,\n",
    "                            test_gt_overlay=False):\n",
    "        \"\"\"\n",
    "        Displays the lidar dem image with optional ground truth\n",
    "        overlay on the image.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dem image member variable is empty, then load the\n",
    "        # full lidar dem image\n",
    "        if self.lidar_dem_image is None: self.load_full_lidar_dem_image()\n",
    "\n",
    "        image = self.lidar_dem_image[:,:,:]\n",
    "\n",
    "        if full_gt_overlay:\n",
    "            # If the ground truth image member variable is empty, then\n",
    "            # load the full ground truth image\n",
    "            if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "            classes = self.gt_image\n",
    "            title = 'LiDAR Digital Elevation Model (DEM) image w/ ground truth overlay'\n",
    "        elif test_gt_overlay or train_gt_overlay:\n",
    "            # If the ground truth tiles image member variable is empty,\n",
    "            # then load the ground truth image tiles\n",
    "            if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "            # create a copy of ground truth image tiles\n",
    "            gt_tiles = self.gt_image_tiles.copy()\n",
    "\n",
    "            # Get tile dimensions\n",
    "            tile_shape = gt_tiles[0].shape\n",
    "\n",
    "            # Choose which set of tiles to set to zero values and\n",
    "            # set proper image title\n",
    "            if train_gt_overlay:\n",
    "                tiles_to_remove = self.dataset_testing_subset\n",
    "                title = 'LiDAR Digital Elevation Model (DEM) image w/ training ground truth overlay'\n",
    "            else:\n",
    "                tiles_to_remove = self.dataset_training_subset\n",
    "                title = 'LiDAR Digital Elevation Model (DEM) image w/ testing ground truth overlay'\n",
    "\n",
    "            # Zero out tiles not in the desired subset\n",
    "            for tile in tiles_to_remove:\n",
    "                row, col = tile\n",
    "                index = row * self.dataset_tiled_subset_cols + col\n",
    "                gt_tiles[index] = np.zeros(tile_shape, dtype=np.uint8)\n",
    "\n",
    "            # Create single ground truth image mask\n",
    "            classes = self.merge_tiles(gt_tiles)\n",
    "\n",
    "        else:\n",
    "            classes = None\n",
    "            title = 'LiDAR Digital Elevation Model (DEM) image'\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        view = spectral.imshow(image,\n",
    "                               source=image,\n",
    "                               classes=classes,\n",
    "                               figsize=size)\n",
    "        if (full_gt_overlay\n",
    "            or test_gt_overlay\n",
    "            or train_gt_overlay): view.set_display_mode('overlay')\n",
    "\n",
    "        view.set_title(title)\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_ndsm_image(self, gsd=UH_2018_GT_GSD,\n",
    "                                   use_void_filling_model = False,\n",
    "                                   use_hybrid_model = True,\n",
    "                                   thres=True, normalize=True,\n",
    "                                   resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the full-size LiDAR normalized digital surface model\n",
    "        (NDSM) image for the University of Houston 2018 dataset sampled\n",
    "        at the specified GSD.\n",
    "        \"\"\"\n",
    "\n",
    "        if use_hybrid_model:\n",
    "            dem_path = self.path_to_lidar_dem_b\n",
    "            print('Loading full LiDAR NDSM image using hybrid DEM...')\n",
    "        elif use_void_filling_model:\n",
    "            dem_path = self.path_to_lidar_dem_tli\n",
    "            print('Loading full LiDAR NDSM image using bare-earth elevation w/ void filling DEM...')\n",
    "        else:\n",
    "            dem_path = self.path_to_lidar_dem_3msr\n",
    "            print('Loading full LiDAR NDSM image using bare-earth elevation DEM...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's LiDAR DSM image\n",
    "        dsm_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                  self.path_to_lidar_dsm)\n",
    "        dem_image_path = os.path.join(self.path_to_dataset_directory, dem_path)\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(dsm_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DSM image is invalid! Path={dsm_image_path}')\n",
    "\n",
    "        if not os.path.isfile(dem_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DEM image is invalid! Path={dem_image_path}')\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_ndsm_image = None\n",
    "\n",
    "        # Open the LiDAR DSM and DEM files as dsm_src and dem_src\n",
    "        with rasterio.open(dsm_image_path) as dsm_src, \\\n",
    "             rasterio.open(dem_image_path) as dem_src:\n",
    "\n",
    "            # Read the images, resample it to the appropriate GSD,\n",
    "            # arrange the numpy array to be (rows, cols, bands),\n",
    "            # and remove unused bands\n",
    "            if resample_factor == 1.0:\n",
    "                dsm_image = np.moveaxis(dsm_src.read(), 0, -1)\n",
    "                dem_image = np.moveaxis(dem_src.read(), 0, -1)\n",
    "            else:\n",
    "                # Set the shape of the resampled image\n",
    "                dsm_out_shape=(dsm_src.count,\n",
    "                            int(dsm_src.height * resample_factor),\n",
    "                            int(dsm_src.width * resample_factor))\n",
    "                dem_out_shape=(dem_src.count,\n",
    "                            int(dem_src.height * resample_factor),\n",
    "                            int(dem_src.width * resample_factor))\n",
    "\n",
    "                dsm_image = np.moveaxis(dsm_src.read(\n",
    "                                        out_shape=dsm_out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "                dem_image = np.moveaxis(dem_src.read(\n",
    "                                        out_shape=dem_out_shape,\n",
    "                                        resampling=resampling), 0, -1)\n",
    "\n",
    "            # Threshold the images so that any value over the threshold\n",
    "            # is set to the image minimum\n",
    "            if thres:\n",
    "                dsm_image[dsm_image > self.lidar_dsm_thres] = dsm_image.min()\n",
    "                dem_image[dem_image > self.lidar_dsm_thres] = dem_image.min()\n",
    "\n",
    "            # NDSM is the difference between the DSM and the DEM\n",
    "            self.lidar_ndsm_image = dsm_image - dem_image\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_ndsm_image -= self.lidar_ndsm_image.min()\n",
    "                self.lidar_ndsm_image /= self.lidar_ndsm_image.max()\n",
    "\n",
    "        return self.lidar_ndsm_image\n",
    "\n",
    "\n",
    "\n",
    "    def load_lidar_ndsm_image_tiles(self, gsd=UH_2018_GT_GSD, tile_list=None,\n",
    "                                    use_void_filling_model = False,\n",
    "                                    use_hybrid_model = True,\n",
    "                                    thres=True, normalize=True,\n",
    "                                    resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's LiDAR normalized\n",
    "        digital surface model (NDSM) image as a set of tiles sampled at\n",
    "        a specified GSD. If no tile list is given, the whole image will\n",
    "        be loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        if use_hybrid_model:\n",
    "            dem_path = self.path_to_lidar_dem_b\n",
    "            print('Loading full LiDAR NDSM image using hybrid DEM...')\n",
    "        elif use_void_filling_model:\n",
    "            dem_path = self.path_to_lidar_dem_tli\n",
    "            print('Loading full LiDAR NDSM image using bare-earth elevation w/ void filling DEM...')\n",
    "        else:\n",
    "            dem_path = self.path_to_lidar_dem_3msr\n",
    "            print('Loading full LiDAR NDSM image using bare-earth elevation DEM...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Check tile_list parameter value\n",
    "        if tile_list and not isinstance(tile_list, tuple): raise ValueError(\n",
    "            \"'tile_list' parameter should be a tuple of tuples!\")\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "\n",
    "        # Set the factor of GSD resampling\n",
    "        resample_factor = self.gsd_lidar / float(gsd)\n",
    "\n",
    "        # Get full path to dataset's LiDAR DSM image\n",
    "        dsm_image_path = os.path.join(self.path_to_dataset_directory,\n",
    "                                  self.path_to_lidar_dsm)\n",
    "        dem_image_path = os.path.join(self.path_to_dataset_directory, dem_path)\n",
    "\n",
    "        # Throw error if file paths do not exist\n",
    "        if not os.path.isfile(dsm_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DSM image is invalid! Path={dsm_image_path}')\n",
    "\n",
    "        if not os.path.isfile(dem_image_path): raise FileNotFoundError(\n",
    "            f'Path to UH2018 LiDAR DEM image is invalid! Path={dem_image_path}')\n",
    "\n",
    "\n",
    "        # Create image variable\n",
    "        self.lidar_ndsm_image_tiles = []\n",
    "\n",
    "        # Open the LiDAR DSM and DEM files as dsm_src and dem_src\n",
    "        with rasterio.open(dsm_image_path) as dsm_src, \\\n",
    "             rasterio.open(dem_image_path) as dem_src:\n",
    "\n",
    "            # Set the shape of the resampled image\n",
    "            dsm_out_shape=(dsm_src.count,\n",
    "                           int(dsm_src.height * resample_factor),\n",
    "                           int(dsm_src.width * resample_factor))\n",
    "            dem_out_shape=(dem_src.count,\n",
    "                           int(dem_src.height * resample_factor),\n",
    "                           int(dem_src.width * resample_factor))\n",
    "\n",
    "            # Get the size of the tile windows\n",
    "            dsm_tile_width = dsm_src.width / self.dataset_tiled_subset_cols\n",
    "            dsm_tile_height = dsm_src.height / self.dataset_tiled_subset_rows\n",
    "            dem_tile_width = dem_src.width / self.dataset_tiled_subset_cols\n",
    "            dem_tile_height = dem_src.height / self.dataset_tiled_subset_rows\n",
    "\n",
    "            # Read in the image data for each image tile\n",
    "            for tile_row in range(0, self.dataset_tiled_subset_rows):\n",
    "                for tile_column in range(0, self.dataset_tiled_subset_cols):\n",
    "\n",
    "                    # If specified tiles are desired, then skip any\n",
    "                    # tiles that do not match the tile_list parameter\n",
    "                    if tile_list and (tile_row, tile_column) not in tile_list:\n",
    "                        continue\n",
    "\n",
    "                    # Set the tile windows to read from the image\n",
    "                    dsm_window = Window(dsm_tile_width * tile_column,\n",
    "                                       dsm_tile_height * tile_row,\n",
    "                                       dsm_tile_width, dsm_tile_height)\n",
    "                    dem_window = Window(dem_tile_width * tile_column,\n",
    "                                       dem_tile_height * tile_row,\n",
    "                                       dem_tile_width, dem_tile_height)\n",
    "\n",
    "                    # Read the tile window from the image, resample it to\n",
    "                    # the appropriate GSD, arrange the numpy array to be\n",
    "                    # (rows, cols, bands), and remove unused bands\n",
    "                    if resample_factor == 1.0:\n",
    "                        dsm_tile = np.moveaxis(dsm_src.read(\n",
    "                                            window = dsm_window), 0, -1)\n",
    "\n",
    "                        dem_tile = np.moveaxis(dem_src.read(\n",
    "                                            window = dem_window), 0, -1)\n",
    "                    else:\n",
    "                        dsm_tile = np.moveaxis(dsm_src.read(\n",
    "                                            window = dsm_window,\n",
    "                                            out_shape=dsm_out_shape,\n",
    "                                            resampling=resampling), 0, -1)\n",
    "\n",
    "                        dem_tile = np.moveaxis(dem_src.read(\n",
    "                                            window = dem_window,\n",
    "                                            out_shape=dem_out_shape,\n",
    "                                            resampling=resampling), 0, -1)\n",
    "\n",
    "\n",
    "                    # Threshold the image tiles so that any value over\n",
    "                    # the threshold is set to the image minimum\n",
    "                    if thres:\n",
    "                        dsm_tile[dsm_tile > self.lidar_dsm_thres] = dsm_tile.min()\n",
    "                        dem_tile[dem_tile > self.lidar_dsm_thres] = dem_tile.min()\n",
    "\n",
    "                    # NDSM is the difference between the DSM and the DEM\n",
    "                    ndsm_tile = dsm_tile - dem_tile\n",
    "\n",
    "                    # Copy the tile to the tiles array\n",
    "                    self.lidar_ndsm_image_tiles.append(ndsm_tile)\n",
    "\n",
    "        # If no tiles were added to the tile list, then set image tiles\n",
    "        # variable to 'None'\n",
    "        if len(self.lidar_ndsm_image_tiles) == 0: self.lidar_ndsm_image_tiles = None\n",
    "        else:\n",
    "            # Turn list of numpy arrays into single numpy array\n",
    "            self.lidar_ndsm_image_tiles = np.stack(self.lidar_ndsm_image_tiles)\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.lidar_ndsm_image_tiles -= self.lidar_ndsm_image_tiles.min()\n",
    "                self.lidar_ndsm_image_tiles /= self.lidar_ndsm_image_tiles.max()\n",
    "\n",
    "        return self.lidar_ndsm_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_lidar_ndsm_image_array(self, path, file_name='full_lidar_ndsm_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full LiDAR ndsm image to a\n",
    "        file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar ndsm image member variable is empty, then load the full\n",
    "        # lidar image\n",
    "        if self.lidar_ndsm_image is None: self.load_full_lidar_ndsm_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_ndsm_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_lidar_ndsm_image_array(self, path, file_name='tiled_lidar_ndsm_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled LiDAR ndsm image to\n",
    "        a file for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar dsm image tile member variable is empty, then load all\n",
    "        # lidar ndsm image tiles\n",
    "        if self.lidar_ndsm_image_tiles is None: self.load_lidar_ndsm_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.lidar_ndsm_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_lidar_ndsm_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset LiDAR normalized digital surface model image.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_ndsm_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_lidar_ndsm_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset LiDAR normalized digital surface model image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.lidar_ndsm_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def show_lidar_ndsm_image(self, size=(15,9),\n",
    "                            full_gt_overlay=False,\n",
    "                            train_gt_overlay=False,\n",
    "                            test_gt_overlay=False):\n",
    "        \"\"\"\n",
    "        Displays the LiDAR ndsm image with optional ground truth\n",
    "        overlay on the image.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the lidar ndsm image member variable is empty, then load the\n",
    "        # full lidar ndsm image\n",
    "        if self.lidar_ndsm_image is None: self.load_full_lidar_ndsm_image()\n",
    "\n",
    "        image = self.lidar_ndsm_image[:,:,:]\n",
    "\n",
    "        if full_gt_overlay:\n",
    "            # If the ground truth image member variable is empty, then\n",
    "            # load the full ground truth image\n",
    "            if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "            classes = self.gt_image\n",
    "            title = 'LiDAR Normalized Digital Surface Model (NDSM) image w/ ground truth overlay'\n",
    "        elif test_gt_overlay or train_gt_overlay:\n",
    "            # If the ground truth tiles image member variable is empty,\n",
    "            # then load the ground truth image tiles\n",
    "            if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "            # create a copy of ground truth image tiles\n",
    "            gt_tiles = self.gt_image_tiles.copy()\n",
    "\n",
    "            # Get tile dimensions\n",
    "            tile_shape = gt_tiles[0].shape\n",
    "\n",
    "            # Choose which set of tiles to set to zero values and\n",
    "            # set proper image title\n",
    "            if train_gt_overlay:\n",
    "                tiles_to_remove = self.dataset_testing_subset\n",
    "                title = 'LiDAR Normalized Digital Surface Model (NDSM) image w/ training ground truth overlay'\n",
    "            else:\n",
    "                tiles_to_remove = self.dataset_training_subset\n",
    "                title = 'LiDAR Normalized Digital Surface Model (NDSM) image w/ testing ground truth overlay'\n",
    "\n",
    "            # Zero out tiles not in the desired subset\n",
    "            for tile in tiles_to_remove:\n",
    "                row, col = tile\n",
    "                index = row * self.dataset_tiled_subset_cols + col\n",
    "                gt_tiles[index] = np.zeros(tile_shape, dtype=np.uint8)\n",
    "\n",
    "            # Create single ground truth image mask\n",
    "            classes = self.merge_tiles(gt_tiles)\n",
    "\n",
    "        else:\n",
    "            classes = None\n",
    "            title = 'LiDAR Normalized Digital Surface Model (NDSM) image'\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        view = spectral.imshow(image,\n",
    "                               source=image,\n",
    "                               classes=classes,\n",
    "                               figsize=size)\n",
    "        if (full_gt_overlay\n",
    "            or test_gt_overlay\n",
    "            or train_gt_overlay): view.set_display_mode('overlay')\n",
    "\n",
    "        view.set_title(title)\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def load_full_vhr_image(self, gsd=UH_2018_GT_GSD,\n",
    "                           thres=True, normalize=True, resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the full-size VHR RGB image for the University of\n",
    "        Houston 2018 dataset sampled at the specified GSD.\n",
    "        \"\"\"\n",
    "        print('Loading full VHR RGB image...')\n",
    "\n",
    "        # VHR image can only be loaded as tiles since there's 14\n",
    "        # images, so load tiles and then merge them to create full VHR\n",
    "        # image\n",
    "        self.vhr_image = self.merge_tiles(\n",
    "            self.load_vhr_image_tiles(gsd=gsd, thres=thres,\n",
    "                                      normalize=normalize,\n",
    "                                      resampling=resampling))\n",
    "\n",
    "        return self.vhr_image\n",
    "\n",
    "    def load_vhr_image_tiles(self, gsd=UH_2018_GT_GSD, tile_list=None,\n",
    "                             thres=True, normalize=True, resampling=None):\n",
    "        \"\"\"\n",
    "        Loads the University of Houston 2018 dataset's hyperspectral\n",
    "        images as a set of tiles sampled at a specified GSD. If no tile\n",
    "        list is given, the whole image will be loaded as tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading VHR RGB dataset tile images...')\n",
    "\n",
    "        # Check GSD parameter value\n",
    "        if gsd <= 0: raise ValueError(\"'gsd' parameter must be greater than 0!\")\n",
    "\n",
    "        # Initialize list for tiles\n",
    "        self.vhr_image_tiles = []\n",
    "\n",
    "        if resampling is None:\n",
    "            print('No resampling method chosen, defaulting to '\n",
    "                    f'{DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "        elif resampling not in RESAMPLING_METHODS:\n",
    "            print(f'Incompatible resampling method {resampling}, '\n",
    "                    f'defaulting to {DEFAULT_RESAMPLING_METHOD}')\n",
    "            resampling = DEFAULT_RESAMPLING_METHOD\n",
    "\n",
    "        resampling = RESAMPLING_METHODS[resampling]\n",
    "\n",
    "\n",
    "        # Set the factor of GSD resampling 1/factor for 1m to 0.5m\n",
    "        resample_factor = self.gsd_vhr / float(gsd)\n",
    "\n",
    "        for tile_row, tile_paths in enumerate(self.paths_to_vhr_images):\n",
    "            for tile_column, tile_path in enumerate(tile_paths):\n",
    "\n",
    "                # If specified tiles are desired, then skip any\n",
    "                # tiles that do not match the tile_list parameter\n",
    "                if tile_list and (tile_row, tile_column) not in tile_list:\n",
    "                    continue\n",
    "\n",
    "                image_path = os.path.join(self.path_to_dataset_directory, tile_path)\n",
    "\n",
    "                # Throw error if file path does not exist\n",
    "                if not os.path.isfile(image_path): raise FileNotFoundError(\n",
    "                    f'Path to UH2018 VHR RGB tile ({tile_row}, {tile_column})'\n",
    "                    f' image is invalid! Path={image_path}')\n",
    "\n",
    "\n",
    "                with rasterio.open(image_path) as src:\n",
    "\n",
    "                    # Read the tile image and resample it to\n",
    "                    # the appropriate GSD, arrange the numpy array to be\n",
    "                    # (rows, cols, bands)\n",
    "                    if resample_factor == 1.0:\n",
    "                            tile = np.moveaxis(src.read(), 0, -1)\n",
    "                    else:\n",
    "                        # Set the shape of the resampled tile\n",
    "                        out_shape=(src.count,\n",
    "                                int(src.height * resample_factor),\n",
    "                                int(src.width * resample_factor))\n",
    "\n",
    "                        tile = np.moveaxis(src.read(\n",
    "                            out_shape=out_shape,\n",
    "                            resampling=resampling), 0, -1)\n",
    "\n",
    "                    # Copy the tile to the tiles array\n",
    "                    self.vhr_image_tiles.append(np.copy(tile))\n",
    "\n",
    "        # If no tiles were added to the tile list, then set image tiles\n",
    "        # variable to 'None'\n",
    "        if len(self.vhr_image_tiles) == 0: self.vhr_image_tiles = None\n",
    "        else:\n",
    "            # Turn list of numpy arrays into single numpy array\n",
    "            self.vhr_image_tiles = np.stack(self.vhr_image_tiles)\n",
    "\n",
    "            # Normalize each intensity band between 0.0 and 1.0\n",
    "            if normalize:\n",
    "                self.vhr_image_tiles = self.vhr_image_tiles.astype(float, copy=False)\n",
    "                self.vhr_image_tiles -= self.vhr_image_tiles.min()\n",
    "                self.vhr_image_tiles /= self.vhr_image_tiles.max()\n",
    "\n",
    "        return self.vhr_image_tiles\n",
    "\n",
    "\n",
    "\n",
    "    def save_full_vhr_image_array(self, path, file_name='full_vhr_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the full vhr image to a file\n",
    "        for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image member variable is empty, then load the full\n",
    "        # vhr image\n",
    "        if self.vhr_image is None: self.load_full_vhr_image()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.hs_image)\n",
    "\n",
    "\n",
    "\n",
    "    def save_tiled_vhr_image_array(self, path, file_name='tiled_vhr_image.npy'):\n",
    "        \"\"\"\n",
    "        Saves the numpy array of the tiled vhr image to a file\n",
    "        for faster loading in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image tile member variable is empty, then load all\n",
    "        # vhr image tiles\n",
    "        if self.vhr_image_tiles is None: self.load_vhr_image_tiles()\n",
    "\n",
    "        with open(os.path.join(path, file_name), 'wb') as outfile:\n",
    "            np.save(outfile, self.hs_image_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_full_vhr_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset VHR image.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.vhr_image = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def load_tiled_vhr_image_array(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads a saved numpy array for the University of Houston 2018\n",
    "        dataset VHR image tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(file_path, 'rb') as infile:\n",
    "            self.vhr_image_tiles = np.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "    def show_vhr_image(self, size=(15,9),\n",
    "                      full_gt_overlay=False,\n",
    "                      train_gt_overlay=False,\n",
    "                      test_gt_overlay=False):\n",
    "        \"\"\"\n",
    "        Displays the vhr rgb image.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image member variable is empty, then load the full\n",
    "        # vhr rgb image\n",
    "        if self.vhr_image is None: self.load_full_vhr_image()\n",
    "\n",
    "        image = self.vhr_image[:,:,:]\n",
    "\n",
    "        if full_gt_overlay:\n",
    "            # If the gt image member variable is empty, then load the full\n",
    "            # ground truth image\n",
    "            if self.gt_image is None: self.load_full_gt_image()\n",
    "\n",
    "            classes = self.gt_image\n",
    "            title = 'VHR RGB image w/ ground truth overlay'\n",
    "        elif test_gt_overlay or train_gt_overlay:\n",
    "            # If the hs image member variable is empty, then load the\n",
    "            # ground truth image tiles\n",
    "            if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "            # create a copy of ground truth image tiles\n",
    "            gt_tiles = self.gt_image_tiles.copy()\n",
    "\n",
    "            # Get tile dimensions\n",
    "            tile_shape = gt_tiles[0].shape\n",
    "\n",
    "            # Choose which set of tiles to set to zero values and\n",
    "            # set proper image title\n",
    "            if train_gt_overlay:\n",
    "                tiles_to_remove = self.dataset_testing_subset\n",
    "                title = 'VHR RGB image w/ training ground truth overlay'\n",
    "            else:\n",
    "                tiles_to_remove = self.dataset_training_subset\n",
    "                title = 'VHR RGB image w/ testing ground truth overlay'\n",
    "\n",
    "            # Zero out tiles not in the desired subset\n",
    "            for tile in tiles_to_remove:\n",
    "                row, col = tile\n",
    "                index = row * self.dataset_tiled_subset_cols + col\n",
    "                gt_tiles[index] = np.zeros(tile_shape, dtype=np.uint8)\n",
    "\n",
    "            # Create single ground truth image mask\n",
    "            classes = self.merge_tiles(gt_tiles)\n",
    "\n",
    "        else:\n",
    "            classes = None\n",
    "            title = 'VHR RGB image'\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        view = spectral.imshow(image,\n",
    "                               source=image,\n",
    "                               classes=classes,\n",
    "                               figsize=size)\n",
    "        if (full_gt_overlay\n",
    "            or test_gt_overlay\n",
    "            or train_gt_overlay):\n",
    "            view.set_display_mode('overlay')\n",
    "            view.class_alpha = 0.5\n",
    "\n",
    "        view.set_title(title)\n",
    "\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def get_tile_indices(self, tile, row_offset=0, col_offset=0):\n",
    "        \"\"\"\n",
    "        Returns the indices where there is a ground truth defined for a\n",
    "        specific tile. Tile row and column offsets for the x and y\n",
    "        values can also be defined.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image member variable is empty, then load the\n",
    "        # ground truth image tiles\n",
    "        if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "        tile_height, tile_width = self.gt_image_tiles[0].shape\n",
    "\n",
    "        col_offset = col_offset * tile_width\n",
    "        row_offset = row_offset * tile_height\n",
    "\n",
    "        indices = []\n",
    "\n",
    "        row, col = tile\n",
    "        index = row * self.dataset_tiled_subset_cols + col\n",
    "        img = self.gt_image_tiles[index]\n",
    "\n",
    "        for r in range(tile_height):\n",
    "            for c in range(tile_width):\n",
    "                if img[r][c] > 0:\n",
    "                    indices.append((r+row_offset,c+col_offset))\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def get_train_test_split(self, flatten=False):\n",
    "        \"\"\"\n",
    "        Returns the training and testing indicies of the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # If the hs image member variable is empty, then load the\n",
    "        # ground truth image tiles\n",
    "        if self.gt_image_tiles is None: self.load_gt_image_tiles()\n",
    "\n",
    "        tile_height, tile_width = self.gt_image_tiles[0].shape\n",
    "\n",
    "        image_width = tile_width * self.dataset_tiled_subset_cols\n",
    "\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "\n",
    "        for row in range(0, self.dataset_tiled_subset_rows):\n",
    "            for col in range(0, self.dataset_tiled_subset_cols):\n",
    "                tile = self.gt_image_tiles[row * self.dataset_tiled_subset_cols + col]\n",
    "                for tr in range(tile_height):\n",
    "                    for tc in range(tile_width):\n",
    "                        r = tr + row*tile_height\n",
    "                        c = tc + col*tile_width\n",
    "                        if tile[tr][tc] > 0:\n",
    "                            if flatten:\n",
    "                                index = r*image_width + c\n",
    "                            else:\n",
    "                                index = (r,c)\n",
    "                            if (row,col) in self.dataset_training_subset:\n",
    "                                train_indices.append(index)\n",
    "                            else:\n",
    "                                test_indices.append(index)\n",
    "\n",
    "        return train_indices, test_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtdrWu4_lW_e"
   },
   "source": [
    "## 3.3) Dataset loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4ghDeAndlW_f"
   },
   "outputs": [],
   "source": [
    "#@title Set up GRSS DFC 2018 UH loading function\n",
    "\n",
    "def load_grss_dfc_2018_uh_dataset(**hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    path_to_dataset = hyperparams['path_to_dataset']\n",
    "\n",
    "    skip_data_preprocessing = hyperparams['skip_data_preprocessing']\n",
    "\n",
    "    hs_resampling = hyperparams['hs_resampling']\n",
    "    lidar_ms_resampling = hyperparams['lidar_ms_resampling']\n",
    "    lidar_ndsm_resampling = hyperparams['lidar_ndsm_resampling']\n",
    "    vhr_resampling = hyperparams['vhr_resampling']\n",
    "\n",
    "    normalize_hs_data = hyperparams['normalize_hs_data']\n",
    "    normalize_lidar_ms_data = hyperparams['normalize_lidar_ms_data']\n",
    "    normalize_lidar_ndsm_data = hyperparams['normalize_lidar_ndsm_data']\n",
    "    normalize_vhr_data = hyperparams['normalize_vhr_data']\n",
    "\n",
    "    hs_histogram_equalization = hyperparams['hs_histogram_equalization']\n",
    "    lidar_ms_histogram_equalization = hyperparams['lidar_ms_histogram_equalization']\n",
    "    lidar_dsm_histogram_equalization = hyperparams['lidar_dsm_histogram_equalization']\n",
    "    lidar_dem_histogram_equalization = hyperparams['lidar_dem_histogram_equalization']\n",
    "    lidar_ndsm_histogram_equalization = hyperparams['lidar_ndsm_histogram_equalization']\n",
    "    vhr_histogram_equalization = hyperparams['vhr_histogram_equalization']\n",
    "\n",
    "    hs_data_filter = hyperparams['hs_data_filter']\n",
    "    lidar_ms_data_filter = hyperparams['lidar_ms_data_filter']\n",
    "    lidar_dsm_data_filter = hyperparams['lidar_dsm_data_filter']\n",
    "    lidar_dem_data_filter = hyperparams['lidar_dem_data_filter']\n",
    "    vhr_data_filter = hyperparams['vhr_data_filter']\n",
    "\n",
    "\n",
    "    use_all_data = hyperparams['use_all_data']\n",
    "    if use_all_data:\n",
    "        use_hs_data = True\n",
    "        use_lidar_ms_data = True\n",
    "        use_lidar_ndsm_data = True\n",
    "        use_vhr_data = True\n",
    "    else:\n",
    "        use_hs_data = hyperparams['use_hs_data']\n",
    "        use_lidar_ms_data = hyperparams['use_lidar_ms_data']\n",
    "        use_lidar_ndsm_data = hyperparams['use_lidar_ndsm_data']\n",
    "        use_vhr_data = hyperparams['use_vhr_data']\n",
    "\n",
    "    hs_channels = []\n",
    "    lidar_ms_channels = []\n",
    "    lidar_ndsm_channels = []\n",
    "    vhr_rgb_channels = []\n",
    "\n",
    "\n",
    "    if path_to_dataset is not None:\n",
    "        dataset = UH_2018_Dataset(dataset_path=path_to_dataset)\n",
    "    else:\n",
    "        dataset = UH_2018_Dataset()\n",
    "    train_gt = dataset.load_full_gt_image(train_only=True)\n",
    "    test_gt = dataset.load_full_gt_image(test_only=True)\n",
    "\n",
    "    data = None\n",
    "    channel_labels = None\n",
    "\n",
    "    # Check to see if hyperspectral data is being used\n",
    "    if use_hs_data:\n",
    "        # Load hyperspectral data\n",
    "        if dataset.hs_image is None:\n",
    "            hs_data = dataset.load_full_hs_image(resampling=hs_resampling)\n",
    "        else:\n",
    "            hs_data = dataset.hs_image\n",
    "        print(f'{dataset.name} hs_data shape: {hs_data.shape}')\n",
    "\n",
    "        # Check for data equalization, filtering and normalization\n",
    "        if hs_histogram_equalization and not skip_data_preprocessing:\n",
    "            hs_data = histogram_equalization(hs_data)\n",
    "        if hs_data_filter is not None and not skip_data_preprocessing:\n",
    "            print(f'Filtering hyperspectral data with {hs_data_filter} filter...')\n",
    "            hs_data = filter_image(hs_data, hs_data_filter)\n",
    "        if normalize_hs_data:\n",
    "            print('Normalizing hyperspectral data...')\n",
    "            hs_data = normalize_image(hs_data)\n",
    "\n",
    "        # Add hyperspectral data to data cube and save channel indices\n",
    "        # for hyperspectral data\n",
    "        if data is None:\n",
    "            hs_channels = range(hs_data.shape[-1])\n",
    "            data = np.copy(hs_data)\n",
    "            channel_labels = dataset.hs_band_wavelength_labels\n",
    "        else:\n",
    "            hs_channels = [x + data.shape[-1] for x in range(hs_data.shape[-1])]\n",
    "            data = np.dstack((data, hs_data))\n",
    "            channel_labels = channel_labels + dataset.hs_band_wavelength_labels\n",
    "\n",
    "    # Check to see if lidar multispectral intensity data is being used\n",
    "    if use_lidar_ms_data:\n",
    "        # Load LiDAR multispectral data\n",
    "        if dataset.lidar_ms_image is None:\n",
    "            lidar_ms_data = dataset.load_full_lidar_ms_image(normalize=normalize_lidar_ms_data,\n",
    "                                                             resampling=lidar_ms_resampling)\n",
    "        else:\n",
    "            lidar_ms_data = dataset.lidar_ms_image\n",
    "        print(f'{dataset.name} lidar_ms_data shape: {lidar_ms_data.shape}')\n",
    "\n",
    "        # Check for data equalization, filtering and normalization\n",
    "        if lidar_ms_histogram_equalization and not skip_data_preprocessing:\n",
    "            lidar_ms_data = histogram_equalization(lidar_ms_data)\n",
    "        if lidar_ms_data_filter is not None and not skip_data_preprocessing:\n",
    "            print(f'Filtering LiDAR multispectral data with {lidar_ms_data_filter} filter...')\n",
    "            lidar_ms_data = filter_image(lidar_ms_data, lidar_ms_data_filter)\n",
    "        if normalize_lidar_ms_data:\n",
    "            print('Normalizing LiDAR multispectral data...')\n",
    "            lidar_ms_data = normalize_image(lidar_ms_data)\n",
    "\n",
    "        # Add lidar multispectral data to data cube and save channel\n",
    "        # indices for lidar multispectral data\n",
    "        if data is None:\n",
    "            lidar_ms_channels = range(lidar_ms_data.shape[-1])\n",
    "            data = np.copy(lidar_ms_data)\n",
    "            channel_labels = dataset.lidar_ms_band_wavelength_labels\n",
    "        else:\n",
    "            lidar_ms_channels = [x + data.shape[-1] for x in range(lidar_ms_data.shape[-1])]\n",
    "            data = np.dstack((data, lidar_ms_data))\n",
    "            channel_labels = channel_labels + dataset.lidar_ms_band_wavelength_labels\n",
    "\n",
    "    # Check to see if lidar normalized digital surface model data is\n",
    "    # being used\n",
    "    if use_lidar_ndsm_data:\n",
    "        if dataset.lidar_dsm_image is None or dataset.lidar_dem_image is None:\n",
    "            lidar_dsm_data = dataset.load_full_lidar_dsm_image(resampling=lidar_ndsm_resampling)\n",
    "            lidar_dem_data = dataset.load_full_lidar_dem_image(resampling=lidar_ndsm_resampling)\n",
    "        else:\n",
    "            lidar_dsm_data = dataset.lidar_dsm_image\n",
    "            lidar_dem_data = dataset.lidar_dem_image\n",
    "        print(f'{dataset.name} lidar_dsm_data shape: {lidar_dsm_data.shape}')\n",
    "        print(f'{dataset.name} lidar_dem_data shape: {lidar_dem_data.shape}')\n",
    "\n",
    "        # Check for data equalization, filtering and normalization\n",
    "        if lidar_dsm_histogram_equalization and not skip_data_preprocessing:\n",
    "            lidar_dem_data = histogram_equalization(lidar_dsm_data)\n",
    "        # Check for data equalization, filtering and normalization\n",
    "        if lidar_dem_histogram_equalization and not skip_data_preprocessing:\n",
    "            lidar_dem_data = histogram_equalization(lidar_dem_data)\n",
    "\n",
    "        # Check for data filtering\n",
    "        if lidar_dsm_data_filter is not None and not skip_data_preprocessing:\n",
    "            print(f'Filtering LiDAR DSM data with {lidar_dsm_data_filter} filter...')\n",
    "            lidar_dsm_data = filter_image(lidar_dsm_data, lidar_dsm_data_filter)\n",
    "        if lidar_dem_data_filter is not None:\n",
    "            print(f'Filtering LiDAR DEM data with {lidar_dem_data_filter} filter...')\n",
    "            lidar_dem_data = filter_image(lidar_dem_data, lidar_dem_data_filter)\n",
    "\n",
    "        # Create NDSM image\n",
    "        print('Creating NDSM image from DSM and DEM (NDSM = DSM - DEM)...')\n",
    "        lidar_ndsm_data = lidar_dsm_data - lidar_dem_data\n",
    "\n",
    "        # Check for data equalization, filtering and normalization\n",
    "        if lidar_ndsm_histogram_equalization and not skip_data_preprocessing:\n",
    "            lidar_ndsm_data = histogram_equalization(lidar_ndsm_data)\n",
    "\n",
    "        # Check for data normalization\n",
    "        if normalize_lidar_ndsm_data:\n",
    "            print('Normalizing LiDAR NDSM data...')\n",
    "            lidar_ndsm_data = normalize_image(lidar_ndsm_data)\n",
    "\n",
    "        # Add lidar NDSM data to data cube and save channel\n",
    "        # index for lidar NDSM data\n",
    "        if data is None:\n",
    "            lidar_ndsm_channels = [0]\n",
    "            data = np.copy(lidar_ndsm_data)\n",
    "            channel_labels = ['NDSM']\n",
    "        else:\n",
    "            lidar_ndsm_channels = [data.shape[-1]]\n",
    "            data = np.dstack((data, lidar_ndsm_data))\n",
    "            channel_labels = channel_labels + ['NDSM']\n",
    "\n",
    "    # Check to see if very high resolution RGB image data is being used\n",
    "    if use_vhr_data:\n",
    "        # Load Very High Resolution RGB image\n",
    "        if dataset.vhr_image is None:\n",
    "            vhr_data = dataset.load_full_vhr_image(normalize=normalize_vhr_data,\n",
    "                                                   resampling=vhr_resampling)\n",
    "        else:\n",
    "            vhr_data = dataset.vhr_image\n",
    "        print(f'{dataset.name} vhr_data shape: {vhr_data.shape}')\n",
    "\n",
    "        # Check for data equalization, filtering and normalization\n",
    "        if vhr_histogram_equalization and not skip_data_preprocessing:\n",
    "            vhr_data = histogram_equalization(vhr_data)\n",
    "        if vhr_data_filter is not None and not skip_data_preprocessing:\n",
    "            print(f'Filtering VHR RGB data with {vhr_data_filter} filter...')\n",
    "            vhr_data = filter_image(vhr_data, vhr_data_filter)\n",
    "        if normalize_vhr_data:\n",
    "            print('Normalizing VHR RGB data...')\n",
    "            vhr_data = normalize_image(vhr_data)\n",
    "\n",
    "        # Add VHR data to data cube and save channel indices for VHR\n",
    "        # RGB data\n",
    "        if data is None:\n",
    "            vhr_rgb_channels = range(vhr_data.shape[-1])\n",
    "            data = np.copy(vhr_data)\n",
    "            channel_labels = ['vhr_red', 'vhr_green', 'vhr_blue']\n",
    "        else:\n",
    "            vhr_rgb_channels = [x + data.shape[-1] for x in range(vhr_data.shape[-1])]\n",
    "            data = np.dstack((data, vhr_data))\n",
    "            channel_labels = channel_labels + ['vhr_red', 'vhr_green', 'vhr_blue']\n",
    "\n",
    "    # Verify that some data was loaded\n",
    "    if data is not None:\n",
    "        print(f'{dataset.name} full dataset shape: {data.shape}')\n",
    "    else:\n",
    "        print('No data was loaded! Training cancelled...')\n",
    "        return\n",
    "\n",
    "\n",
    "    print(f'{dataset.name} train_gt shape: {train_gt.shape}')\n",
    "    print(f'{dataset.name} test_gt shape: {test_gt.shape}')\n",
    "\n",
    "    dataset_info = {\n",
    "        'name': dataset.name,\n",
    "        'num_classes': dataset.gt_num_classes,\n",
    "        'ignored_labels': dataset.gt_ignored_labels,\n",
    "        'class_labels': dataset.gt_class_label_list,\n",
    "        'label_mapping': dataset.gt_class_value_mapping,\n",
    "        'hs_channels': list(hs_channels),\n",
    "        'lidar_ms_channels': list(lidar_ms_channels),\n",
    "        'lidar_ndsm_channels': list(lidar_ndsm_channels),\n",
    "        'vhr_rgb_channels': list(vhr_rgb_channels),\n",
    "        'channel_labels': channel_labels,\n",
    "    }\n",
    "\n",
    "    return data, train_gt, test_gt, dataset_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_TeBG0GlW_f"
   },
   "source": [
    "# 4) Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1xNeIV7mlW_f"
   },
   "outputs": [],
   "source": [
    "#@title Preprocess Data\n",
    "def preprocess_data(data, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP1NPLxNlW_f"
   },
   "source": [
    "# 5) Data Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-hK6FCTAlW_f"
   },
   "outputs": [],
   "source": [
    "#@title Postprocess Data\n",
    "def postprocess_data(pred_test, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFQOYSe1lW_g"
   },
   "source": [
    "# 6) Utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7Ga3sawAlW_g"
   },
   "outputs": [],
   "source": [
    "#@title Setup 'Get GPU Device' function\n",
    "def get_device(ordinal):\n",
    "    \"\"\"\n",
    "    Takes a GPU device identifier and, if available, returns the device,\n",
    "    and if not returns the CPU device.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ordinal : int\n",
    "        The Tensorflow device ordinal ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    device\n",
    "        A context manager for the specified device to use for newly created ops\n",
    "    \"\"\"\n",
    "    if ordinal < 0:\n",
    "        print(\"Computation on CPU\")\n",
    "        device = '/CPU:0'\n",
    "    elif len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "        print(f'Computation on CUDA GPU device {ordinal}')\n",
    "        device = f'/GPU:{ordinal}'\n",
    "    else:\n",
    "        print(\"<!> CUDA was requested but is not available! Computation will go on CPU. <!>\")\n",
    "        device = '/CPU:0'\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "43d933oFlW_g"
   },
   "outputs": [],
   "source": [
    "#@title Setup 'Prime Generator' function\n",
    "def prime_generator():\n",
    "    \"\"\"\n",
    "    Generate an infinite sequence of prime numbers.\n",
    "\n",
    "    Sieve of Eratosthenes\n",
    "    Code by David Eppstein, UC Irvine, 28 Feb 2002\n",
    "    http://code.activestate.com/recipes/117119/\n",
    "    \"\"\"\n",
    "    # Maps composites to primes witnessing their compositeness.\n",
    "    # This is memory efficient, as the sieve is not \"run forward\"\n",
    "    # indefinitely, but only as long as required by the current\n",
    "    # number being tested.\n",
    "    #\n",
    "    D = {}\n",
    "\n",
    "    # The running integer that's checked for primeness\n",
    "    q = 2\n",
    "\n",
    "    while True:\n",
    "        if q not in D:\n",
    "            # q is a new prime.\n",
    "            # Yield it and mark its first multiple that isn't\n",
    "            # already marked in previous iterations\n",
    "            #\n",
    "            yield q\n",
    "            D[q * q] = [q]\n",
    "        else:\n",
    "            # q is composite. D[q] is the list of primes that\n",
    "            # divide it. Since we've reached q, we no longer\n",
    "            # need it in the map, but we'll mark the next\n",
    "            # multiples of its witnesses to prepare for larger\n",
    "            # numbers\n",
    "            #\n",
    "            for p in D[q]:\n",
    "                D.setdefault(p + q, []).append(p)\n",
    "            del D[q]\n",
    "\n",
    "        q += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hwDCnUpUlW_g"
   },
   "outputs": [],
   "source": [
    "#@title Setup 'Filter Prediciton Results' function\n",
    "def filter_pred_results(test_gt, pred_test, ignored_labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Reshape pred_test to be the same size as train_gt\n",
    "    pred_test = np.reshape(pred_test, test_gt.shape)\n",
    "    indices = get_valid_gt_indices(test_gt, ignored_labels=ignored_labels)\n",
    "    target_test = np.array([test_gt[x, y] for x, y in indices])\n",
    "    pred_test = np.array([pred_test[x, y] for x, y in indices])\n",
    "\n",
    "    return target_test, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "voQt3e94A_ck"
   },
   "outputs": [],
   "source": [
    "#@title Setup 'Average Accuracy' and 'Each Class Accuracy' function\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GVgtzLThB8RO"
   },
   "outputs": [],
   "source": [
    "#@title Setup zero padding functions\n",
    "\n",
    "def zeroPadding_1D(old_matrix, pad_length, pad_depth = 0):\n",
    "    new_matrix = np.lib.pad(old_matrix, ((0, pad_length)), 'constant', constant_values=0)\n",
    "    return new_matrix\n",
    "\n",
    "def zeroPadding_2D(old_matrix, pad_length):\n",
    "    new_matrix = np.lib.pad(old_matrix, ((pad_length, pad_length),(pad_length, pad_length)), 'constant', constant_values=0)\n",
    "    return new_matrix\n",
    "\n",
    "def zeroPadding_3D(old_matrix, pad_length, pad_depth = 0):\n",
    "    new_matrix = np.lib.pad(old_matrix, ((pad_length, pad_length), (pad_length, pad_length), (pad_depth, pad_depth)), 'constant', constant_values=0)\n",
    "    return new_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tTCQ-cO6Bob2"
   },
   "outputs": [],
   "source": [
    "#@title Setup model statistics recording functions\n",
    "\n",
    "# KAPPA_3D_DenseNet, OA_3D_DenseNet, AA_3D_DenseNet, ELEMENT_ACC_3D_DenseNet,TRAINING_TIME_3D_DenseNet,\n",
    "# TESTING_TIME_3D_DenseNet, history_3d_densenet, loss_and_metrics, CATEGORY,\n",
    "def outputStats(KAPPA_AE, OA_AE, AA_AE, ELEMENT_ACC_AE, TRAINING_TIME_AE, TESTING_TIME_AE, history, loss_and_metrics, CATEGORY, path1, path2):\n",
    "\n",
    "\n",
    "    f = open(path1, 'a')\n",
    "\n",
    "    sentence0 = 'KAPPAs, mean_KAPPA  std_KAPPA for each iteration are:' + str(KAPPA_AE) + str(np.mean(KAPPA_AE)) + '  ' + str(np.std(KAPPA_AE)) + '\\n'\n",
    "    f.write(sentence0)\n",
    "    sentence1 = 'OAs, mean_OA  std_OA for each iteration are:' + str(OA_AE) + str(np.mean(OA_AE)) + '  ' + str(np.std(OA_AE)) + '\\n'\n",
    "    f.write(sentence1)\n",
    "    sentence2 = 'AAs, mean_AA  std_AA for each iteration are:' + str(AA_AE) + str(np.mean(AA_AE)) + '  ' + str(np.std(AA_AE)) + '\\n'\n",
    "    f.write(sentence2)\n",
    "    sentence3 = 'Total average Training time is :' + str(np.sum(TRAINING_TIME_AE)) + '\\n'\n",
    "    f.write(sentence3)\n",
    "    sentence4 = 'Total average Testing time is:' + str(np.sum(TESTING_TIME_AE)) + '\\n'\n",
    "    f.write(sentence4)\n",
    "\n",
    "    element_mean = np.mean(ELEMENT_ACC_AE, axis=0)\n",
    "    element_std = np.std(ELEMENT_ACC_AE, axis=0)\n",
    "    sentence5 = \"Mean of all elements in confusion matrix:\" + str(np.mean(ELEMENT_ACC_AE, axis=0)) + '\\n'\n",
    "    f.write(sentence5)\n",
    "    sentence6 = \"Standard deviation of all elements in confusion matrix\" + str(np.std(ELEMENT_ACC_AE, axis=0)) + '\\n'\n",
    "    f.write(sentence6)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print_matrix = np.zeros((CATEGORY), dtype=object)\n",
    "    for i in range(CATEGORY):\n",
    "        print_matrix[i] = str(element_mean[i]) + \"  \" + str(element_std[i])\n",
    "\n",
    "    np.savetxt(path2, print_matrix.astype(str), fmt='%s', delimiter=\"\\t\",\n",
    "               newline='\\n')\n",
    "\n",
    "    print('Test score:', loss_and_metrics[0])\n",
    "    print('Test accuracy:', loss_and_metrics[1])\n",
    "    print(history.history.keys())\n",
    "\n",
    "\n",
    "def outputStats_assess(KAPPA_AE, OA_AE, AA_AE, ELEMENT_ACC_AE, CATEGORY, path1, path2):\n",
    "\n",
    "\n",
    "    f = open(path1, 'a')\n",
    "\n",
    "    sentence0 = 'KAPPAs, mean_KAPPA  std_KAPPA for each iteration are:' + str(KAPPA_AE) + str(np.mean(KAPPA_AE)) + '  ' + str(np.std(KAPPA_AE)) + '\\n'\n",
    "    f.write(sentence0)\n",
    "    sentence1 = 'OAs, mean_OA  std_OA for each iteration are:' + str(OA_AE) + str(np.mean(OA_AE)) + '  ' + str(np.std(OA_AE)) + '\\n'\n",
    "    f.write(sentence1)\n",
    "    sentence2 = 'AAs, mean_AA  std_AA for each iteration are:' + str(AA_AE) + str(np.mean(AA_AE)) + '  ' + str(np.std(AA_AE)) + '\\n'\n",
    "    f.write(sentence2)\n",
    "\n",
    "    element_mean = np.mean(ELEMENT_ACC_AE, axis=0)\n",
    "    element_std = np.std(ELEMENT_ACC_AE, axis=0)\n",
    "    sentence5 = \"Mean of all elements in confusion matrix:\" + str(np.mean(ELEMENT_ACC_AE, axis=0)) + '\\n'\n",
    "    f.write(sentence5)\n",
    "    sentence6 = \"Standard deviation of all elements in confusion matrix\" + str(np.std(ELEMENT_ACC_AE, axis=0)) + '\\n'\n",
    "    f.write(sentence6)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print_matrix = np.zeros((CATEGORY), dtype=object)\n",
    "    for i in range(CATEGORY):\n",
    "        print_matrix[i] = str(element_mean[i]) + \"  \" + str(element_std[i])\n",
    "\n",
    "    np.savetxt(path2, print_matrix.astype(str), fmt='%s', delimiter=\"\\t\",\n",
    "               newline='\\n')\n",
    "\n",
    "\n",
    "def outputStats_SVM(KAPPA_AE, OA_AE, AA_AE, ELEMENT_ACC_AE, TRAINING_TIME_AE, TESTING_TIME_AE, CATEGORY, path1, path2):\n",
    "\n",
    "\n",
    "    f = open(path1, 'a')\n",
    "\n",
    "    sentence0 = 'KAPPAs, mean_KAPPA  std_KAPPA for each iteration are:' + str(KAPPA_AE) + str(np.mean(KAPPA_AE)) + '  ' + str(np.std(KAPPA_AE)) + '\\n'\n",
    "    f.write(sentence0)\n",
    "    sentence1 = 'OAs, mean_OA  std_OA for each iteration are:' + str(OA_AE) + str(np.mean(OA_AE)) + '  ' + str(np.std(OA_AE)) + '\\n'\n",
    "    f.write(sentence1)\n",
    "    sentence2 = 'AAs, mean_AA  std_AA for each iteration are:' + str(AA_AE) + str(np.mean(AA_AE)) + '  ' + str(np.std(AA_AE)) + '\\n'\n",
    "    f.write(sentence2)\n",
    "    sentence3 = 'Total average Training time is :' + str(np.sum(TRAINING_TIME_AE)) + '\\n'\n",
    "    f.write(sentence3)\n",
    "    sentence4 = 'Total average Testing time is:' + str(np.sum(TESTING_TIME_AE)) + '\\n'\n",
    "    f.write(sentence4)\n",
    "\n",
    "    element_mean = np.mean(ELEMENT_ACC_AE, axis=0)\n",
    "    element_std = np.std(ELEMENT_ACC_AE, axis=0)\n",
    "    sentence5 = \"Mean of all elements in confusion matrix:\" + str(np.mean(ELEMENT_ACC_AE, axis=0)) + '\\n'\n",
    "    f.write(sentence5)\n",
    "    sentence6 = \"Standard deviation of all elements in confusion matrix\" + str(np.std(ELEMENT_ACC_AE, axis=0)) + '\\n'\n",
    "    f.write(sentence6)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print_matrix = np.zeros((CATEGORY), dtype=object)\n",
    "    for i in range(CATEGORY):\n",
    "        print_matrix[i] = str(element_mean[i]) + \"  \" + str(element_std[i])\n",
    "\n",
    "    np.savetxt(path2, print_matrix.astype(str), fmt='%s', delimiter=\"\\t\",\n",
    "               newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "f7JQJNwRF0LB"
   },
   "outputs": [],
   "source": [
    "#@title Setup 1-D index to row/column assignment function\n",
    "\n",
    "def indexToAssignment(indices, Row, Col, pad_length):\n",
    "    \"\"\"\n",
    "    Takes a list of indices to samples in the dataset and creates a new\n",
    "    list of row-column index pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : list of int\n",
    "        A list of indices to the sample points on the dataset.\n",
    "    Row : int\n",
    "        The number of rows in the dataset.\n",
    "    Col : int\n",
    "        The number of columns in the dataset.\n",
    "    pad_length : int\n",
    "        The number of neighbors of the sample in each spatial direction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_assign : dictionary of lists of int\n",
    "        A new list of row-column sample indicies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize assignment dictionary\n",
    "    new_assign = {}\n",
    "\n",
    "    # Loop through the enumeration of the indices\n",
    "    for counter, value in enumerate(indices):\n",
    "        assign_0 = value // Col + pad_length    # Row assignment\n",
    "        assign_1 = value % Col + pad_length     # Column assignment\n",
    "        new_assign[counter] = [assign_0, assign_1] # Assign row-col pair\n",
    "\n",
    "    return new_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bvxB1kEDGDsp"
   },
   "outputs": [],
   "source": [
    "#@title Setup neighboring pixel patch selection function\n",
    "\n",
    "def selectNeighboringPatch(matrix, pos_row, pos_col, ex_len):\n",
    "    \"\"\"\n",
    "    Selects the patch of neighbors for a particular sample point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : zero padded nparray\n",
    "        The dataset from which to select the neighborhood patch.\n",
    "    pos_row : int\n",
    "        Row index of sample to find neighborhood of.\n",
    "    pos_col : int\n",
    "        Column index of sample to find neighborhood of.\n",
    "    ex_len : int\n",
    "        The number of neighbors in each spatial direction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    selected_patch : nparray\n",
    "        The (ex_len*2+1) by (ex_len*2+1) matrix of samples in the\n",
    "        (pos_row, pos_col) sample neighborhood.\n",
    "    \"\"\"\n",
    "    # Narrow down the data matrix to the rows that are in the sample's\n",
    "    # neighborhood\n",
    "    selected_rows = matrix[range(pos_row - ex_len, pos_row + ex_len + 1), :]\n",
    "\n",
    "    # Of the set of rows that are in the neighborhood, select the set\n",
    "    # of columns in the neighborhood\n",
    "    selected_patch = selected_rows[:, range(pos_col - ex_len, pos_col + ex_len + 1)]\n",
    "\n",
    "    return selected_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eHl01le9GPKq"
   },
   "outputs": [],
   "source": [
    "#@title Setup test/train/validation sampling function\n",
    "\n",
    "def sampling(proportionVal, groundTruth):\n",
    "    \"\"\"\n",
    "    Divides the dataset into training and testing datasets by randomly\n",
    "    sampling each class and separating the samples by validation split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    proportionVal : float\n",
    "        The 0.0 < 'proportionVal' < 1.0 proportion of the entire dataset\n",
    "        that will be used for validation/test set.\n",
    "    groundTruth : nparray of int\n",
    "        The dataset of ground truth classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_indices : list of int\n",
    "        A list of whole dataset indices that will be used for the\n",
    "        training dataset.\n",
    "    test_indices : list of int\n",
    "        A list of whole dataset indices that will be used for the\n",
    "        testing/validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize label - sample dictionaries\n",
    "    labels_loc = {}\n",
    "    train = {}\n",
    "    test = {}\n",
    "\n",
    "    # Get the number of classes in the ground truth\n",
    "    m = max(groundTruth)\n",
    "    print(m)\n",
    "\n",
    "    # Get a random sampling of each class for the training and testing\n",
    "    # sets\n",
    "    for i in range(m):\n",
    "        # Get indicies of samples that belong to class i\n",
    "        indices = [j for j, x in enumerate(groundTruth.ravel().tolist()) if x == i + 1]\n",
    "\n",
    "        # Shuffle the indicies 'randomly' (repeatable due to random seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Save the locations of all the matching samples for current\n",
    "        # label\n",
    "        labels_loc[i] = indices\n",
    "\n",
    "        # Get the number of samples dedicated to the training set vs.\n",
    "        # the testing set\n",
    "        nb_val = int(proportionVal * len(indices))\n",
    "\n",
    "        # Set (1-proportionVal) fraction of samples for this label to\n",
    "        # the training set\n",
    "        train[i] = indices[:-nb_val]\n",
    "\n",
    "        # Set proportionVal fraction of samples for this label to\n",
    "        # the testing/validation set\n",
    "        test[i] = indices[-nb_val:]\n",
    "\n",
    "    # Initialize lists for training and testing point indicies\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Copy training and testing sample indicies to their respective list\n",
    "    for i in range(m):\n",
    "        train_indices += train[i]\n",
    "        test_indices += test[i]\n",
    "\n",
    "    # Shuffle the order of the sample indicies in the indices lists\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    # Print number of testing and training samples\n",
    "    print(len(test_indices))\n",
    "    print(len(train_indices))\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trK_uc8MlW_m"
   },
   "source": [
    "# 7) Band Selection Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQGGpaDolW_m"
   },
   "source": [
    "## 7.1) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "skPHIMcolW_m"
   },
   "outputs": [],
   "source": [
    "#@title Rolling Window Function\n",
    "def rolling_window(array, window=(0,), asteps=None, wsteps=None, axes=None, toend=True):\n",
    "    \"\"\"Create a view of `array` which for every point gives the n-dimensional\n",
    "    neighbourhood of size window. New dimensions are added at the end of\n",
    "    `array` or after the corresponding original dimension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array to which the rolling window is applied.\n",
    "    window : int or tuple\n",
    "        Either a single integer to create a window of only the last axis or a\n",
    "        tuple to create it for the last len(window) axes. 0 can be used as a\n",
    "        to ignore a dimension in the window.\n",
    "    asteps : tuple\n",
    "        Aligned at the last axis, new steps for the original array, ie. for\n",
    "        creation of non-overlapping windows. (Equivalent to slicing result)\n",
    "    wsteps : int or tuple (same size as window)\n",
    "        steps for the added window dimensions. These can be 0 to repeat values\n",
    "        along the axis.\n",
    "    axes: int or tuple\n",
    "        If given, must have the same size as window. In this case window is\n",
    "        interpreted as the size in the dimension given by axes. IE. a window\n",
    "        of (2, 1) is equivalent to window=2 and axis=-2.\n",
    "    toend : bool\n",
    "        If False, the new dimensions are right after the corresponding original\n",
    "        dimension, instead of at the end of the array. Adding the new axes at the\n",
    "        end makes it easier to get the neighborhood, however toend=False will give\n",
    "        a more intuitive result if you view the whole array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A view on `array` which is smaller to fit the windows and has windows added\n",
    "    dimensions (0s not counting), ie. every point of `array` is an array of size\n",
    "    window.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> a = np.arange(9).reshape(3,3)\n",
    "    >>> rolling_window(a, (2,2))\n",
    "    array([[[[0, 1],\n",
    "             [3, 4]],\n",
    "\n",
    "            [[1, 2],\n",
    "             [4, 5]]],\n",
    "\n",
    "\n",
    "           [[[3, 4],\n",
    "             [6, 7]],\n",
    "\n",
    "            [[4, 5],\n",
    "             [7, 8]]]])\n",
    "\n",
    "    Or to create non-overlapping windows, but only along the first dimension:\n",
    "    >>> rolling_window(a, (2,0), asteps=(2,1))\n",
    "    array([[[0, 3],\n",
    "            [1, 4],\n",
    "            [2, 5]]])\n",
    "\n",
    "    Note that the 0 is discared, so that the output dimension is 3:\n",
    "    >>> rolling_window(a, (2,0), asteps=(2,1)).shape\n",
    "    (1, 3, 2)\n",
    "\n",
    "    This is useful for example to calculate the maximum in all (overlapping)\n",
    "    2x2 submatrixes:\n",
    "    >>> rolling_window(a, (2,2)).max((2,3))\n",
    "    array([[4, 5],\n",
    "           [7, 8]])\n",
    "\n",
    "    Or delay embedding (3D embedding with delay 2):\n",
    "    >>> x = np.arange(10)\n",
    "    >>> rolling_window(x, 3, wsteps=2)\n",
    "    array([[0, 2, 4],\n",
    "           [1, 3, 5],\n",
    "           [2, 4, 6],\n",
    "           [3, 5, 7],\n",
    "           [4, 6, 8],\n",
    "           [5, 7, 9]])\n",
    "    \"\"\"\n",
    "    array = np.asarray(array)\n",
    "    orig_shape = np.asarray(array.shape)\n",
    "    window = np.atleast_1d(window).astype(int)  # maybe crude to cast to int...\n",
    "\n",
    "    if axes is not None:\n",
    "        axes = np.atleast_1d(axes)\n",
    "        w = np.zeros(array.ndim, dtype=int)\n",
    "        for axis, size in zip(axes, window):\n",
    "            w[axis] = size\n",
    "        window = w\n",
    "\n",
    "    # Check if window is legal:\n",
    "    if window.ndim > 1:\n",
    "        raise ValueError(\"`window` must be one-dimensional.\")\n",
    "    if np.any(window < 0):\n",
    "        raise ValueError(\"All elements of `window` must be larger then 1.\")\n",
    "    if len(array.shape) < len(window):\n",
    "        raise ValueError(\"`window` length must be less or equal `array` dimension.\")\n",
    "\n",
    "    _asteps = np.ones_like(orig_shape)\n",
    "    if asteps is not None:\n",
    "        asteps = np.atleast_1d(asteps)\n",
    "        if asteps.ndim != 1:\n",
    "            raise ValueError(\"`asteps` must be either a scalar or one dimensional.\")\n",
    "        if len(asteps) > array.ndim:\n",
    "            raise ValueError(\"`asteps` cannot be longer then the `array` dimension.\")\n",
    "        # does not enforce alignment, so that steps can be same as window too.\n",
    "        _asteps[-len(asteps):] = asteps\n",
    "\n",
    "        if np.any(asteps < 1):\n",
    "            raise ValueError(\"All elements of `asteps` must be larger then 1.\")\n",
    "    asteps = _asteps\n",
    "\n",
    "    _wsteps = np.ones_like(window)\n",
    "    if wsteps is not None:\n",
    "        wsteps = np.atleast_1d(wsteps)\n",
    "        if wsteps.shape != window.shape:\n",
    "            raise ValueError(\"`wsteps` must have the same shape as `window`.\")\n",
    "        if np.any(wsteps < 0):\n",
    "            raise ValueError(\"All elements of `wsteps` must be larger then 0.\")\n",
    "\n",
    "        _wsteps[:] = wsteps\n",
    "        _wsteps[window == 0] = 1  # make sure that steps are 1 for non-existing dims.\n",
    "    wsteps = _wsteps\n",
    "\n",
    "    # Check that the window would not be larger then the original:\n",
    "    if np.any(orig_shape[-len(window):] < window * wsteps):\n",
    "        raise ValueError(\"`window` * `wsteps` larger then `array` in at least one dimension.\")\n",
    "\n",
    "    new_shape = orig_shape  # just renaming...\n",
    "\n",
    "    # For calculating the new shape 0s must act like 1s:\n",
    "    _window = window.copy()\n",
    "    _window[_window == 0] = 1\n",
    "\n",
    "    new_shape[-len(window):] += wsteps - _window * wsteps\n",
    "    new_shape = (new_shape + asteps - 1) // asteps\n",
    "    # make sure the new_shape is at least 1 in any \"old\" dimension (ie. steps\n",
    "    # is (too) large, but we do not care.\n",
    "    new_shape[new_shape < 1] = 1\n",
    "    shape = new_shape\n",
    "\n",
    "    strides = np.asarray(array.strides)\n",
    "    strides *= asteps\n",
    "    new_strides = array.strides[-len(window):] * wsteps\n",
    "\n",
    "    # The full new shape and strides:\n",
    "    if toend:\n",
    "        new_shape = np.concatenate((shape, window))\n",
    "        new_strides = np.concatenate((strides, new_strides))\n",
    "    else:\n",
    "        _ = np.zeros_like(shape)\n",
    "        _[-len(window):] = window\n",
    "        _window = _.copy()\n",
    "        _[-len(window):] = new_strides\n",
    "        _new_strides = _\n",
    "\n",
    "        new_shape = np.zeros(len(shape) * 2, dtype=int)\n",
    "        new_strides = np.zeros(len(shape) * 2, dtype=int)\n",
    "\n",
    "        new_shape[::2] = shape\n",
    "        new_strides[::2] = strides\n",
    "        new_shape[1::2] = _window\n",
    "        new_strides[1::2] = _new_strides\n",
    "\n",
    "    new_strides = new_strides[new_shape != 0]\n",
    "    new_shape = new_shape[new_shape != 0]\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(array, shape=new_shape, strides=new_strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "V100lwislW_n"
   },
   "outputs": [],
   "source": [
    "#@title Processor Class\n",
    "import spectral as spy\n",
    "\n",
    "class Processor:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def prepare_data(self, img_path, gt_path):\n",
    "        if img_path[-3:] == 'mat':\n",
    "            import scipy.io as sio\n",
    "            img_mat = sio.loadmat(img_path)\n",
    "            gt_mat = sio.loadmat(gt_path)\n",
    "            img_keys = img_mat.keys()\n",
    "            gt_keys = gt_mat.keys()\n",
    "            img_key = [k for k in img_keys if k != '__version__' and k != '__header__' and k != '__globals__']\n",
    "            gt_key = [k for k in gt_keys if k != '__version__' and k != '__header__' and k != '__globals__']\n",
    "            return img_mat.get(img_key[0]).astype('float64'), gt_mat.get(gt_key[0]).astype('int8')\n",
    "        else:\n",
    "            import spectral as spy\n",
    "            img = spy.open_image(img_path).load()\n",
    "            gt = spy.open_image(gt_path)\n",
    "            a = spy.principal_components()\n",
    "            a.transform()\n",
    "            return img, gt.read_band(0)\n",
    "\n",
    "    def get_correct(self, img, gt):\n",
    "        \"\"\"\n",
    "        :param img: 3D arr\n",
    "        :param gt: 2D arr\n",
    "        :return: covert arr  [n_samples,n_bands]\n",
    "        \"\"\"\n",
    "        gt_1D = gt.reshape(-1)\n",
    "        index = gt_1D.nonzero()\n",
    "        gt_correct = gt_1D[index]\n",
    "        img_2D = img.reshape(img.shape[0] * img.shape[1], img.shape[2])\n",
    "        img_correct = img_2D[index]\n",
    "        return img_correct, gt_correct\n",
    "\n",
    "    def get_tr_tx_index(self, y, test_size=0.9):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train_index, X_test_index, y_train_, y_test_ = \\\n",
    "            train_test_split(np.arange(0, y.shape[0]), y, test_size=test_size)\n",
    "        return X_train_index, X_test_index\n",
    "\n",
    "    def divide_img_blocks(self, img, gt, block_size=(5, 5)):\n",
    "        \"\"\"\n",
    "        split image into a*b blocks, the edge filled with its mirror\n",
    "        :param img:\n",
    "        :param gt:\n",
    "        :param block_size; tuple of size, it must be odd and >=3\n",
    "        :return: correct image blocks\n",
    "        \"\"\"\n",
    "        # TODO: padding edge with mirror\n",
    "        w_1, w_2 = int((block_size[0] - 1) / 2), int((block_size[1] - 1) / 2)\n",
    "        img_padding = np.pad(img, ((w_1, w_2),\n",
    "                                   (w_1, w_2), (0, 0)), 'symmetric')\n",
    "        gt_padding = np.pad(gt, ((w_1, w_2),\n",
    "                                 (w_1, w_2)), 'symmetric')\n",
    "        img_blocks = rolling_window(img_padding, block_size, axes=(1, 0))  # divide data into 5x5 blocks\n",
    "        gt_blocks = rolling_window(gt_padding, block_size, axes=(1, 0))\n",
    "        i_1, i_2 = int((block_size[0] - 1) / 2), int((block_size[0] - 1) / 2)\n",
    "        nonzero_index = gt_blocks[:, :, i_1, i_2].nonzero()\n",
    "        img_blocks_nonzero = img_blocks[nonzero_index]\n",
    "        gt_blocks_nonzero = (gt_blocks[:, :, i_1, i_2])[nonzero_index]\n",
    "        return img_blocks_nonzero, gt_blocks_nonzero\n",
    "\n",
    "    def split_tr_tx(self, X, y, test_size=0.4):\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param test_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    def split_each_class(self, X, y, each_train_size=10):\n",
    "        X_tr, y_tr, X_ts, y_ts = [], [], [], []\n",
    "        for c in np.unique(y):\n",
    "            y_index = np.nonzero(y == c)[0]\n",
    "            np.random.shuffle(y_index)\n",
    "            cho, non_cho = np.split(y_index, [each_train_size, ])\n",
    "            X_tr.append(X[cho])\n",
    "            y_tr.append(y[cho])\n",
    "            X_ts.append(X[non_cho])\n",
    "            y_ts.append(y[non_cho])\n",
    "        X_tr, X_ts, y_tr, y_ts = np.asarray(X_tr), np.asarray(X_ts), np.asarray(y_tr), np.asarray(y_ts)\n",
    "        return X_tr.reshape(X_tr.shape[0] * X_tr.shape[1], X.shape[1]),\\\n",
    "               X_ts.reshape(X_ts.shape[0] * X_ts.shape[1], X.shape[1]), \\\n",
    "               y_tr.flatten(), y_ts.flatten()\n",
    "\n",
    "    def save_experiment(self, y_pre, y_test, file_neme=None, parameters=None):\n",
    "        \"\"\"\n",
    "        save classification results and experiment parameters into files for k-folds cross validation.\n",
    "        :param y_pre:\n",
    "        :param y_test:\n",
    "        :param parameters:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        import os\n",
    "        home = os.getcwd() + '/experiments'\n",
    "        if not os.path.exists(home):\n",
    "            os.makedirs(home)\n",
    "        if parameters == None:\n",
    "            parameters = [None]\n",
    "        if file_neme == None:\n",
    "            file_neme = home + '/scores.npz'\n",
    "        else:\n",
    "            file_neme = home + '/' + file_neme + '.npz'\n",
    "\n",
    "        '''save results and scores into a numpy file'''\n",
    "        ca, oa, aa, kappa = [], [], [], []\n",
    "        if np.array(y_pre).shape.__len__() > 1:  # that means test data tested k times\n",
    "            for y in y_pre:\n",
    "                ca_, oa_, aa_, kappa_ = self.score(y_test, y)\n",
    "                ca.append(ca_), oa.append(oa_), aa.append(aa_), kappa.append(kappa_)\n",
    "        else:\n",
    "            ca, oa, aa, kappa = self.score(y_test, y_pre)\n",
    "        np.savez(file_neme, y_test=y_test, y_pre=y_pre, CA=np.array(ca), OA=np.array(oa), AA=aa, Kappa=kappa,\n",
    "                 param=parameters)\n",
    "        print('the experiments have been saved in experiments/scores.npz')\n",
    "\n",
    "    # def get_train_test_indexes(self, train_size, gt):\n",
    "    #     \"\"\"\n",
    "    #\n",
    "    #     :param train_size:\n",
    "    #     :param gt:\n",
    "    #     :return:\n",
    "    #     \"\"\"\n",
    "    #     gt_1D = gt.reshape(-1)\n",
    "    #     samples_correct = gt_1D[gt_1D.nonzero()]\n",
    "    #     n_samples = samples_correct.shape[0]  # the num of available samples\n",
    "    #     classes = {}\n",
    "    #     for i in np.unique(samples_correct):\n",
    "    #         classes[i] = len(np.nonzero(samples_correct == i)[0])\n",
    "    #     if train_size >= min(classes.values()):\n",
    "    #             train_size = min(classes.values())\n",
    "    #     train_indexes = np.empty((0))\n",
    "    #     test_indexes = np.empty((0))\n",
    "    #     for key in classes:\n",
    "    #         size_ci = classes[key]\n",
    "    #         index_ci = np.nonzero(gt_1D == key)[0]  # 1 dim: (row,col=None)\n",
    "    #         index_train__ = np.empty(0)\n",
    "    #         if train_size > 0 and train_size < 1.:\n",
    "    #             # slip data as percentage for each class\n",
    "    #             index_train__ = np.random.choice(index_ci, int(size_ci * train_size), replace=False)\n",
    "    #         else:\n",
    "    #             # slip data as form of fixed numbers\n",
    "    #             index_train__ = np.random.choice(index_ci, int(train_size), replace=False)\n",
    "    #         index_test__ = np.setdiff1d(index_ci,index_train__)\n",
    "    #         train_indexes = np.append(train_indexes,index_train__)\n",
    "    #         test_indexes = np.append(test_indexes,index_test__)\n",
    "    #     return train_indexes.astype(np.int64),test_indexes.astype(np.int64)\n",
    "\n",
    "    def majority_filter(self, classes_map, selems):\n",
    "        \"\"\"\n",
    "        :param classes_map: 2 dim image\n",
    "        :param selems: elements: [disk(1),square(2)...]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        from skimage.filters.rank import modal\n",
    "        # from skimage.morphology import disk,square\n",
    "        classes_map__ = classes_map.astype(np.uint16)  # convert dtype to uint16\n",
    "        out = classes_map__\n",
    "        for selem in selems:\n",
    "            out = modal(classes_map__, selem)\n",
    "            classes_map__ = out\n",
    "        return out.astype(np.int8)\n",
    "\n",
    "    def score(self, y_test, y_predicted):\n",
    "        \"\"\"\n",
    "        calculate the accuracy and other criterion according to predicted results\n",
    "        :param y_test:\n",
    "        :param y_predicted:\n",
    "        :return: ca, oa, aa, kappa\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        '''overall accuracy'''\n",
    "        oa = accuracy_score(y_test, y_predicted)\n",
    "        '''average accuracy for each class'''\n",
    "        n_classes = max([np.unique(y_test).__len__(), np.unique(y_predicted).__len__()])\n",
    "        ca = []\n",
    "        for c in np.unique(y_test):\n",
    "            y_c = y_test[np.nonzero(y_test == c)]  # find indices of each class\n",
    "            y_c_p = y_predicted[np.nonzero(y_test == c)]\n",
    "            acurracy = accuracy_score(y_c, y_c_p)\n",
    "            ca.append(acurracy)\n",
    "        aa = (np.array(ca)).mean()\n",
    "\n",
    "        '''kappa'''\n",
    "        kappa = self.kappa(y_test, y_predicted)\n",
    "        return ca, oa, aa, kappa\n",
    "\n",
    "    def result2gt(self, y_predicted, test_indexes, gt):\n",
    "        \"\"\"\n",
    "\n",
    "        :param y_predicted:\n",
    "        :param test_indexes: indexes got from ground truth\n",
    "        :param gt: 2-dim img\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n_row, n_col = gt.shape\n",
    "        gt_1D = gt.reshape((n_row * n_col))\n",
    "        gt_1D[test_indexes] = y_predicted\n",
    "        return gt_1D.reshape(n_row, n_col)\n",
    "\n",
    "    def extended_morphological_profile(self, components, disk_radius):\n",
    "        \"\"\"\n",
    "\n",
    "        :param components:\n",
    "        :param disk_radius:\n",
    "        :return:2-dim emp\n",
    "        \"\"\"\n",
    "        rows, cols, bands = components.shape\n",
    "        n = disk_radius.__len__()\n",
    "        import numpy as np\n",
    "        emp = np.zeros((rows * cols, bands * (2 * n + 1)))\n",
    "        from skimage.morphology import opening, closing, disk\n",
    "        for band in range(bands):\n",
    "            position = band * (n * 2 + 1) + n\n",
    "            emp_ = np.zeros((rows, cols, 2 * n + 1))\n",
    "            emp_[:, :, n] = components[:, :, band]\n",
    "            i = 1\n",
    "            for r in disk_radius:\n",
    "                closed = closing(components[:, :, band], selem=disk(r))\n",
    "                opened = opening(components[:, :, band], selem=disk(r))\n",
    "                emp_[:, :, n - i] = closed\n",
    "                emp_[:, :, n + i] = opened\n",
    "                i += 1\n",
    "            emp[:, position - n:position + n + 1] = emp_.reshape((rows * cols, 2 * n + 1))\n",
    "        return emp.reshape(rows, cols, bands * (2 * n + 1))\n",
    "\n",
    "    def texture_feature(self, components, theta_arr=None, frequency_arr=None):\n",
    "        \"\"\"\n",
    "        extract the texture features\n",
    "        :param components:\n",
    "        :param theta_arr:\n",
    "        :param frequency_arr:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if theta_arr == None:\n",
    "            theta_arr = np.arange(0, 8) * np.pi / 4  # 8 orientations\n",
    "        if frequency_arr == None:\n",
    "            frequency_arr = np.pi / (2 ** np.arange(1, 5))  # 4 frequency\n",
    "\n",
    "        from skimage.filters import gabor\n",
    "        results = []\n",
    "        for img in components.transpose():\n",
    "            for theta in theta_arr:\n",
    "                for fre in frequency_arr:\n",
    "                    filt_real, filt_imag = gabor(img, frequency=fre, theta=theta)\n",
    "                    results.append(filt_real)\n",
    "        return np.array(results).transpose()\n",
    "\n",
    "    def pca_transform(self, n_components, samples):\n",
    "        \"\"\"\n",
    "\n",
    "        :param n_components:\n",
    "        :param samples: [nb_samples, bands]/or [n_row, n_column, n_bands]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        HSI_or_not = samples.shape.__len__() == 3  # denotes HSI data\n",
    "        n_row, n_column, n_bands = 0, 0, 0\n",
    "        if HSI_or_not:\n",
    "            n_row, n_column, n_bands = samples.shape\n",
    "            samples = samples.reshape((n_row * n_column, n_bands))\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        trans_samples = pca.fit_transform(samples)\n",
    "        if HSI_or_not:\n",
    "            return trans_samples.reshape((n_row, n_column, n_components))\n",
    "        return trans_samples\n",
    "\n",
    "    def normlize_HSI(self, img):\n",
    "        from sklearn.preprocessing import normalize\n",
    "        n_row, n_column, n_bands = img.shape\n",
    "        norm_img = normalize(img.reshape(n_row * n_column, n_bands))\n",
    "        return norm_img.reshape(n_row, n_column, n_bands)\n",
    "\n",
    "    def each_class_OA(self, y_test, y_predicted):\n",
    "        \"\"\"\n",
    "        get each OA for all class respectively\n",
    "        :param y_test:\n",
    "        :param y_predicted:\n",
    "        :return:{}\n",
    "        \"\"\"\n",
    "        classes = np.unique(y_test)\n",
    "        results = []\n",
    "        for c in classes:\n",
    "            y_c = y_test[np.nonzero(y_test == c)]  # find indices of each class\n",
    "            y_c_p = y_predicted[np.nonzero(y_test == c)]\n",
    "            acurracy = self.score(y_c, y_c_p)\n",
    "            results.append(acurracy)\n",
    "        return np.array(results)\n",
    "\n",
    "    def kappa(self, y_test, y_predicted):\n",
    "        from sklearn.metrics import cohen_kappa_score\n",
    "        return round(cohen_kappa_score(y_test, y_predicted), 3)\n",
    "\n",
    "    def color_legend(self, color_map, label):\n",
    "        \"\"\"\n",
    "\n",
    "        :param color_map: 1-n color map in range 0-255\n",
    "        :param label: label list\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        import matplotlib.patches as mpatches\n",
    "        import matplotlib.pyplot as plt\n",
    "        size = len(label)\n",
    "        patchs = []\n",
    "        m = 255.  # float(color_map.max())\n",
    "        color_map_ = (color_map / m)[1:]\n",
    "        for i in range(0, size):\n",
    "            patchs.append(mpatches.Patch(color=color_map_[i], label=label[i]))\n",
    "        # plt.legend(handles=patchs)\n",
    "        return patchs\n",
    "\n",
    "    def get_tr_ts_index_num(self, y, n_labeled=10):\n",
    "        import random\n",
    "        classes = np.unique(y)\n",
    "        X_train_index, X_test_index = np.empty(0, dtype='int8'), np.empty(0, dtype='int8')\n",
    "        for c in classes:\n",
    "            index_c = np.nonzero(y == c)[0]\n",
    "            random.shuffle(index_c)\n",
    "            X_train_index = np.append(X_train_index, index_c[:n_labeled])\n",
    "            X_test_index = np.append(X_test_index, index_c[n_labeled:])\n",
    "        return X_train_index, X_test_index\n",
    "\n",
    "    def save_res_4kfolds_cv(self, y_pres, y_tests, file_name=None, verbose=False):\n",
    "        \"\"\"\n",
    "        save experiment results for k-folds cross validation\n",
    "        :param y_pres: predicted labels, k*Ntest\n",
    "        :param y_tests: true labels, k*Ntest\n",
    "        :param file_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ca, oa, aa, kappa = [], [], [], []\n",
    "        for y_p, y_t in zip(y_pres, y_tests):\n",
    "            ca_, oa_, aa_, kappa_ = self.score(y_t, y_p)\n",
    "            ca.append(np.asarray(ca_)), oa.append(np.asarray(oa_)), aa.append(np.asarray(aa_)),\n",
    "            kappa.append(np.asarray(kappa_))\n",
    "        ca = np.asarray(ca) * 100\n",
    "        oa = np.asarray(oa) * 100\n",
    "        aa = np.asarray(aa) * 100\n",
    "        kappa = np.asarray(kappa)\n",
    "        ca_mean, ca_std = np.round(ca.mean(axis=0), 2), np.round(ca.std(axis=0), 2)\n",
    "        oa_mean, oa_std = np.round(oa.mean(), 2), np.round(oa.std(), 2)\n",
    "        aa_mean, aa_std = np.round(aa.mean(), 2), np.round(aa.std(), 2)\n",
    "        kappa_mean, kappa_std = np.round(kappa.mean(), 3), np.round(kappa.std(), 3)\n",
    "        if file_name is not None:\n",
    "            file_name = 'scores.npz'\n",
    "            np.savez(file_name, y_test=y_tests, y_pre=y_pres,\n",
    "                     ca_mean=ca_mean, ca_std=ca_std,\n",
    "                     oa_mean=oa_mean, oa_std=oa_std,\n",
    "                     aa_mean=aa_mean, aa_std=aa_std,\n",
    "                     kappa_mean=kappa_mean, kappa_std=kappa_std)\n",
    "            print('the experiments have been saved in ', file_name)\n",
    "\n",
    "        if verbose is True:\n",
    "            print('---------------------------------------------')\n",
    "            print('ca\\t\\t', '\\taa\\t\\t', '\\toa\\t\\t', '\\tkappa\\t\\t')\n",
    "            print(ca_mean, '+-', ca_std)\n",
    "            print(aa_mean, '+-', aa_std)\n",
    "            print(oa_mean, '+-', oa_std)\n",
    "            print(kappa_mean, '+-', kappa_std)\n",
    "        return ca, oa, aa, kappa\n",
    "\n",
    "    # def view_clz_map(self, gt, y_index, y_predicted, save_path=None, show_error=False):\n",
    "    #     \"\"\"\n",
    "    #     view HSI classification results\n",
    "    #     :param gt:\n",
    "    #     :param y_index: index of excluding 0th class\n",
    "    #     :param y_predicted:\n",
    "    #     :param show_error:\n",
    "    #     :return:\n",
    "    #     \"\"\"\n",
    "    #     n_row, n_column = gt.shape\n",
    "    #     gt_1d = gt.reshape(-1).copy()\n",
    "    #     nonzero_index = gt_1d.nonzero()\n",
    "    #     gt_corrected = gt_1d[nonzero_index]\n",
    "    #     if show_error:\n",
    "    #         t = y_predicted.copy()\n",
    "    #         correct_index = np.nonzero(y_predicted == gt_corrected[y_index])\n",
    "    #         t[correct_index] = 0  # leave error\n",
    "    #         gt_corrected[:] = 0\n",
    "    #         gt_corrected[y_index] = t\n",
    "    #         gt_1d[nonzero_index] = t\n",
    "    #     else:\n",
    "    #         gt_corrected[y_index] = y_predicted\n",
    "    #         gt_1d[nonzero_index] = gt_corrected\n",
    "    #     gt_map = gt_1d.reshape((n_row, n_column)).astype('uint8')\n",
    "    #     spy.imshow(classes=gt_map)\n",
    "    #     if save_path != None:\n",
    "    #         spy.save_rgb(save_path, gt_map, colors=spy.spy_colors)\n",
    "    #         print('the figure is saved in ', save_path)\n",
    "\n",
    "    def split_source_target(self, X, y, split_attribute_index, split_threshold, save_name=None):\n",
    "        \"\"\"\n",
    "        split source/target domain data for transfer learning according to attribute\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param split_attribute_index:\n",
    "        :param split_threshold: split condition. e.g if 1.2 those x[:,index] >= 1.2 are split into source\n",
    "        :param save_name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        source_index = np.nonzero(X[:, split_attribute_index] >= split_threshold)\n",
    "        target_index = np.nonzero(X[:, split_attribute_index] < split_threshold)\n",
    "        X_source = X[source_index]\n",
    "        X_target = X[target_index]\n",
    "        y_source = y[source_index].astype('int')\n",
    "        y_target = y[target_index].astype('int')\n",
    "        if save_name is not None:\n",
    "            np.savez(save_name, X_source=X_source, X_target=X_target, y_source=y_source, y_target=y_target)\n",
    "        return X_source, X_target, y_source, y_target\n",
    "\n",
    "    def results_to_cvs(self, res_file_name, save_name):\n",
    "        import csv\n",
    "        dt = np.load(res_file_name)\n",
    "        ca_mean = np.round(dt['CA'].mean(axis=0) * 100, 2)\n",
    "        ca_std = np.round(dt['CA'].std(axis=0), 2)\n",
    "        oa_mean = np.round(dt['OA'].mean() * 100, 2)\n",
    "        oa_std = np.round(dt['OA'].std(axis=0), 2)\n",
    "        aa_mean = np.round(dt['AA'].mean() * 100, 2)\n",
    "        aa_std = np.round(dt['AA'].std(axis=0), 2)\n",
    "        kappa_mean = np.round(dt['Kappa'].mean(), 3)\n",
    "        kappa_std = np.round(dt['Kappa'].std(axis=0), 2)\n",
    "        with open(save_name, 'wb') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for i in zip(ca_mean, ca_std):\n",
    "                writer.writerow(i)\n",
    "            writer.writerow([oa_mean, oa_std])\n",
    "            writer.writerow([aa_mean, aa_std])\n",
    "            writer.writerow([kappa_mean, kappa_std])\n",
    "\n",
    "    def view_clz_map_spyversion4single_img(self, gt, y_index, y_predicted, save_path=None, show_error=False,\n",
    "                                           show_axis=False):\n",
    "        \"\"\"\n",
    "        view HSI classification results\n",
    "        :param gt:\n",
    "        :param y_index: test index of excluding 0th class\n",
    "        :param y_predicted:\n",
    "        :param show_error:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n_row, n_column = gt.shape\n",
    "        gt_1d = gt.reshape(-1).copy()\n",
    "        nonzero_index = gt_1d.nonzero()\n",
    "        gt_corrected = gt_1d[nonzero_index]\n",
    "        if show_error:\n",
    "            t = y_predicted.copy()\n",
    "            correct_index = np.nonzero(y_predicted == gt_corrected[y_index])\n",
    "            t[correct_index] = 0  # leave error\n",
    "            gt_corrected[:] = 0\n",
    "            gt_corrected[y_index] = t\n",
    "            gt_1d[nonzero_index] = t\n",
    "        else:\n",
    "            gt_corrected[y_index] = y_predicted\n",
    "            gt_1d[nonzero_index] = gt_corrected\n",
    "        gt_map = gt_1d.reshape((n_row, n_column)).astype('uint8')\n",
    "        spy.imshow(classes=gt_map)\n",
    "        if save_path != None:\n",
    "            import matplotlib.pyplot as plt\n",
    "            spy.save_rgb('temp.png', gt_map, colors=spy.spy_colors)\n",
    "            if show_axis:\n",
    "                plt.savefig(save_path, format='eps')\n",
    "            else:\n",
    "                plt.axis('off')\n",
    "                plt.savefig(save_path, format='eps')\n",
    "            print('the figure is saved in ', save_path)\n",
    "\n",
    "    def view_clz_map_mlpversion(self, test_index, results, sub_indexes, labels, save_name=None):\n",
    "        \"\"\" visualize image with 2 rows and 3 columns with the color legend for knn classification\n",
    "            --------\n",
    "            Usage:\n",
    "                res = [gt, y_pre_spectral, y_pre_shape, y_pre_texture, y_pre_stack, y_pre_kernel]\n",
    "                sub_index = [331, 332, 333, 334, 335, 336, 313]\n",
    "                labels = ['(a) groundtruth', r'(b) $kNN_{spectral}$', r'(c) $kNN_{shape}$', r'(d) $kNN_{texture}$',\n",
    "                r'(e) $kNN_{stack}$', r'(f) $kNN_{multi}$']\n",
    "                view_clz_map_mlpversion(tx_index, res, sub_index, labels, save_name='./experiments/paviaU_class_map.eps')\n",
    "        \"\"\"\n",
    "        import matplotlib.patches as mpatches\n",
    "        import matplotlib.pyplot as plt\n",
    "        import copy\n",
    "        n_res = results.__len__()\n",
    "        gt = copy.deepcopy(results[0])\n",
    "        n_row, n_column = gt.shape\n",
    "        gt_1d = gt.reshape(-1).copy()\n",
    "        nonzero_index = gt_1d.nonzero()\n",
    "        for i in range(n_res):\n",
    "            if i == 0:\n",
    "                gt_map = gt\n",
    "            else:\n",
    "                gt_corrected = copy.deepcopy(gt_1d[nonzero_index])\n",
    "                gt_corrected[test_index] = results[i]\n",
    "                gt_1d_temp = copy.deepcopy(gt.reshape(-1))\n",
    "                gt_1d_temp[nonzero_index] = gt_corrected\n",
    "                gt_map = gt_1d_temp.reshape((n_row, n_column)).astype('uint8')\n",
    "            axe = plt.subplot(sub_indexes[i])\n",
    "            im = axe.imshow(gt_map, cmap='jet')\n",
    "            axe.set_title(labels[i], fontdict={'fontsize': 10})\n",
    "            axe.axis('off')\n",
    "        values = np.unique(gt.ravel())\n",
    "        # get the colors of the values, according to the\n",
    "        # colormap used by imshow\n",
    "        colors = [im.cmap(im.norm(value)) for value in values]\n",
    "        # create a patch (proxy artist) for every color\n",
    "        patches = [mpatches.Patch(color=colors[i], label=\"{l}\".format(l=values[i])) for i in range(len(values))]\n",
    "        # put those patched as legend-handles into the legend\n",
    "        axe_legend = plt.subplot(sub_indexes[-1])\n",
    "        axe_legend.legend(handles=patches, loc=10, ncol=6)\n",
    "        axe_legend.axis('off')\n",
    "\n",
    "        # save image\n",
    "        plt.savefig(save_name, format='eps', dpi=1000)\n",
    "        print('the figure is saved in ', save_name)\n",
    "\n",
    "    def standardize_label(self, y):\n",
    "        \"\"\"\n",
    "        standardize the class label into 0-k\n",
    "        :param y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        import copy\n",
    "        classes = np.unique(y)\n",
    "        standardize_y = copy.deepcopy(y)\n",
    "        for i in range(classes.shape[0]):\n",
    "            standardize_y[np.nonzero(y == classes[i])] = i\n",
    "        return standardize_y\n",
    "\n",
    "    def one2array(self, y):\n",
    "        n_classes = np.unique(y).__len__()\n",
    "        y_expected = np.zeros((y.shape[0], n_classes))\n",
    "        for i in range(y.shape[0]):\n",
    "            y_expected[i][y[i]] = 1\n",
    "        return y_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23DZLfpIlW_n"
   },
   "source": [
    "## 7.2) Band Selection Utility Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KHXFcZuglW_o"
   },
   "outputs": [],
   "source": [
    "#@title Evaluate Band\n",
    "def eval_band(new_img, gt, train_inx, test_idx):\n",
    "    \"\"\"\n",
    "\n",
    "    :param new_img:\n",
    "    :param gt:\n",
    "    :param train_inx:\n",
    "    :param test_idx:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    p = Processor()\n",
    "    # img_, gt_ = p.get_correct(new_img, gt)\n",
    "    gt_ = gt\n",
    "    img_ = maxabs_scale(new_img)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(img_, gt_, test_size=0.4, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = img_[train_inx], img_[test_idx], gt_[train_inx], gt_[test_idx]\n",
    "    knn_classifier = KNN(n_neighbors=5)\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    # score = cross_val_score(knn_classifier, img_, y=gt_, cv=3)\n",
    "    y_pre = knn_classifier.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pre)\n",
    "    # score = np.mean(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jm16eMwjlW_o"
   },
   "outputs": [],
   "source": [
    "#@title Evaluate Band with Cross Validation\n",
    "def eval_band_cv(X, y, times=10):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param times: n times k-fold cv\n",
    "    :return:  knn/svm/elm=>(OA+std, Kappa+std)\n",
    "    \"\"\"\n",
    "    p = Processor()\n",
    "    img_ = maxabs_scale(X)\n",
    "    estimator = [KNN(n_neighbors=5), SVC(C=1e4, kernel='rbf', gamma=1.), ELM_Classifier(200)]\n",
    "    estimator_pre, y_test_all = [[], [], []], []\n",
    "    for i in range(times):  # repeat N times K-fold CV\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "        for train_index, test_index in skf.split(img_, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            y_test_all.append(y_test)\n",
    "            for c in range(3):\n",
    "                estimator[c].fit(X_train, y_train)\n",
    "                estimator_pre[c].append(estimator[c].predict(X_test))\n",
    "    clf = ['knn', 'svm', 'elm']\n",
    "    score = []\n",
    "    for z in range(3):\n",
    "        ca, oa, aa, kappa = p.save_res_4kfolds_cv(estimator_pre[z], y_test_all, file_name=clf[z] + 'score.npz', verbose=True)\n",
    "        score.append([oa, kappa])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cVFAuYXGlW_o"
   },
   "outputs": [],
   "source": [
    "#@title Extreme Learning Classifier Class\n",
    "class ELM_Classifier(BaseEstimator, ClassifierMixin):\n",
    "    upper_bound = .5\n",
    "    lower_bound = -.5\n",
    "\n",
    "    def __init__(self, n_hidden, dropout_prob=None):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # check label has form of 2-dim array\n",
    "        X, y, = copy.deepcopy(X), copy.deepcopy(y)\n",
    "        self.sample_weight = None\n",
    "        if y.shape.__len__() != 2:\n",
    "            self.classes_ = np.unique(y)\n",
    "            self.n_classes_ = self.classes_.__len__()\n",
    "            y = self.one2array(y, self.n_classes_)\n",
    "        else:\n",
    "            self.classes_ = np.arange(y.shape[1])\n",
    "            self.n_classes_ = self.classes_.__len__()\n",
    "        self.W = np.random.uniform(self.lower_bound, self.upper_bound, size=(X.shape[1], self.n_hidden))\n",
    "        if self.dropout_prob is not None:\n",
    "            self.W = self.dropout(self.W, prob=self.dropout_prob)\n",
    "            # X = self.dropout(X, prob=self.dropout_prob)\n",
    "        self.b = np.random.uniform(self.lower_bound, self.upper_bound, size=self.n_hidden)\n",
    "        H = expit(np.dot(X, self.W) + self.b)\n",
    "        # H = self.dropout(H, prob=0.1)\n",
    "        if sample_weight is not None:\n",
    "            self.sample_weight = sample_weight / sample_weight.sum()\n",
    "            extend_sample_weight = np.diag(self.sample_weight)\n",
    "            inv_ = linalg.pinv(np.dot(\n",
    "                np.dot(H.transpose(), extend_sample_weight), H))\n",
    "            self.B = np.dot(np.dot(np.dot(inv_, H.transpose()), extend_sample_weight), y)\n",
    "        else:\n",
    "            self.B = np.dot(linalg.pinv(H), y)\n",
    "        return self\n",
    "\n",
    "    def one2array(self, y, n_dim):\n",
    "        y_expected = np.zeros((y.shape[0], n_dim))\n",
    "        for i in range(y.shape[0]):\n",
    "            y_expected[i][y[i]] = 1\n",
    "        return y_expected\n",
    "\n",
    "    def predict(self, X, prob=False):\n",
    "        X = copy.deepcopy(X)\n",
    "        H = expit(np.dot(X, self.W) + self.b)\n",
    "        output = np.dot(H, self.B)\n",
    "        if prob:\n",
    "            return output\n",
    "        return output.argmax(axis=1)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = {'n_hidden': self.n_hidden, 'dropout_prob': self.dropout_prob}\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        return self\n",
    "\n",
    "    def dropout(self, x, prob=0.2):\n",
    "        if prob < 0. or prob >= 1:\n",
    "            raise Exception('Dropout level must be in interval [0, 1]')\n",
    "        retain_prob = 1. - prob\n",
    "        sample = np.random.binomial(n=1, p=retain_prob, size=x.shape)\n",
    "        x *= sample\n",
    "        # x /= retain_prob\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm3X_DnSlW_o"
   },
   "source": [
    "## 7.3) CAE SSC Band Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C2tn6n4glW_o"
   },
   "outputs": [],
   "source": [
    "#@title Setup 'CAE SSC Band Selection' Class\n",
    "class CAE_BS(object):\n",
    "    \"\"\"\n",
    "    :argument:\n",
    "        Implementation of L2 norm based sparse self-expressive clustering model\n",
    "        with affinity measurement basing on angular similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, n_band=10, coef_=1):\n",
    "        self.n_band = n_band\n",
    "        self.coef_ = coef_\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_cae_fea, X_origin):\n",
    "        \"\"\"\n",
    "        :param X_cae_fea: shape [n_CAE_fea, n_band]\n",
    "        :param X_origin: original HSI data with a 2-D shape of (n_row*n_clm, n_band)\n",
    "        :return: selected band subset\n",
    "        \"\"\"\n",
    "        cluster_res = self.__get_cluster_close(X_cae_fea)\n",
    "        selected_band = self.__get_band(cluster_res, X_origin)\n",
    "        return selected_band\n",
    "\n",
    "    def __get_band(self, cluster_result, X):\n",
    "        \"\"\"\n",
    "        select band according to the center of each cluster\n",
    "        :param cluster_result:\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        selected_band = []\n",
    "        n_cluster = np.unique(cluster_result).__len__()\n",
    "        # img_ = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "        for c in np.unique(cluster_result):\n",
    "            idx = np.nonzero(cluster_result == c)\n",
    "            center = np.mean(X[:, idx[0]], axis=1).reshape((-1, 1))\n",
    "            distance = np.linalg.norm(X[:, idx[0]] - center, axis=0)\n",
    "            band_ = X[:, idx[0]][:, distance.argmin()]\n",
    "            print(f'idx[0] for c={c} :  {idx[0]}')\n",
    "            print(f'distance.argmin() for c={c} :  {distance.argmin()}')\n",
    "            print(f'band_ : {band_}')\n",
    "            print()\n",
    "            selected_band.append(band_)\n",
    "        bands = np.asarray(selected_band).transpose()\n",
    "        # bands = bands.reshape(n_cluster, n_row, n_column)\n",
    "        # bands = np.transpose(bands, axes=(1, 2, 0))\n",
    "        return bands\n",
    "\n",
    "    def __get_cluster_close(self, X):\n",
    "        \"\"\"\n",
    "        using close-form solution\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n_sample = X.transpose().shape[0]\n",
    "        H = X.transpose()    # NRP_ELM(self.n_hidden, sparse=False).fit(X).predict(X)\n",
    "        C = np.zeros((n_sample, n_sample))\n",
    "        for i in range(n_sample):\n",
    "            y_i = H[i]\n",
    "            H_i = np.delete(H, i, axis=0).transpose()\n",
    "            term_1 = np.linalg.inv(np.dot(H_i.transpose(), H_i) + self.coef_ * np.eye(n_sample - 1))\n",
    "            w = np.dot(np.dot(term_1, H_i.transpose()), y_i.reshape((y_i.shape[0], 1)))\n",
    "            w = w.flatten()\n",
    "            #  Normalize the columns of C: ci = ci / ||ci||_ss.\n",
    "            coef = w / np.max(np.abs(w))\n",
    "            C[:i, i] = coef[:i]\n",
    "            C[i + 1:, i] = coef[i:]\n",
    "        # compute affinity matrix\n",
    "        L = 0.5 * (np.abs(C) + np.abs(C.T))  # affinity graph\n",
    "        self.affinity_matrix = L\n",
    "        # spectral clustering\n",
    "        sc = SpectralClustering(n_clusters=self.n_band, affinity='precomputed')\n",
    "        sc.fit(self.affinity_matrix)\n",
    "        return sc.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3jT4AHqlW_p"
   },
   "source": [
    "## 7.4) DSC NET Band Selection\n",
    "\n",
    "Code Authors: Pan Ji,     University of Adelaide,         pan.ji@adelaide.edu.au\n",
    "Tong Zhang, Australian National University, tong.zhang@anu.edu.au\n",
    "Copyright Reserved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nLBdjx2plW_p"
   },
   "outputs": [],
   "source": [
    "#@title DSC Net Class\n",
    "class DSC_NET(object):\n",
    "    def __init__(self, n_input, kernel_size, n_hidden, reg_const1=1.0, reg_const2=1.0, reg=None, batch_size=256,\n",
    "                 max_iter=10, denoise=False, model_path=None, logs_path='./logs'):\n",
    "        # n_hidden is a arrary contains the number of neurals on every layer\n",
    "        tf.reset_default_graph()\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.reg = reg\n",
    "        self.model_path = model_path\n",
    "        self.kernel_size = kernel_size\n",
    "        self.iter = 0\n",
    "        self.batch_size = batch_size\n",
    "        # weights = self._initialize_weights()\n",
    "        # # Variable initialization\n",
    "        weights = dict()\n",
    "        with tf.variable_scope('weight', reuse=tf.AUTO_REUSE):\n",
    "            weights['enc_w0'] = tf.get_variable(\"enc_w0\",\n",
    "                                                    shape=[self.kernel_size[0], self.kernel_size[0], 1,\n",
    "                                                           self.n_hidden[0]],\n",
    "                                                    initializer=layers.xavier_initializer_conv2d(),\n",
    "                                                    regularizer=self.reg)\n",
    "            weights['enc_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype=tf.float32))\n",
    "\n",
    "            weights['dec_w0'] = tf.get_variable(\"dec_w0\",\n",
    "                                                    shape=[self.kernel_size[0], self.kernel_size[0], 1,\n",
    "                                                           self.n_hidden[0]],\n",
    "                                                    initializer=layers.xavier_initializer_conv2d(),\n",
    "                                                    regularizer=self.reg)\n",
    "            weights['dec_b0'] = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "\n",
    "        self.max_iter = max_iter\n",
    "        # model\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_input[0], self.n_input[1], 1])\n",
    "        self.learning_rate = tf.placeholder(tf.float32, [])\n",
    "\n",
    "        if denoise == False:\n",
    "            x_input = self.x\n",
    "            latent, shape = self.encoder(x_input, weights)\n",
    "\n",
    "        else:\n",
    "            x_input = tf.add(self.x, tf.random_normal(shape=tf.shape(self.x),\n",
    "                                                      mean=0,\n",
    "                                                      stddev=0.2,\n",
    "                                                      dtype=tf.float32))\n",
    "\n",
    "            latent, shape = self.encoder(x_input, weights)\n",
    "        self.z_conv = tf.reshape(latent, [batch_size, -1])\n",
    "        self.z_ssc, Coef = self.selfexpressive_moduel(batch_size)\n",
    "        self.Coef = Coef\n",
    "        latent_de_ft = tf.reshape(self.z_ssc, tf.shape(latent))\n",
    "        self.x_r_ft = self.decoder(latent_de_ft, weights, shape)\n",
    "\n",
    "        self.saver = tf.train.Saver([v for v in tf.trainable_variables() if not (v.name.startswith(\"Coef\"))])\n",
    "\n",
    "        self.cost_ssc = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.z_conv, self.z_ssc), 2))\n",
    "        self.recon_ssc = tf.reduce_sum(tf.pow(tf.subtract(self.x_r_ft, self.x), 2.0))\n",
    "        self.reg_ssc = tf.reduce_sum(tf.pow(self.Coef, 2))\n",
    "        tf.summary.scalar(\"ssc_loss\", self.cost_ssc)\n",
    "        tf.summary.scalar(\"reg_lose\", self.reg_ssc)\n",
    "\n",
    "        self.loss_ssc = self.cost_ssc * reg_const2 + reg_const1 * self.reg_ssc + self.recon_ssc\n",
    "\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "        self.optimizer_ssc = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss_ssc)\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(self.init)\n",
    "        self.summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # # this function will raise an exception in higher version Tensorflow\n",
    "    # def _initialize_weights(self):\n",
    "    #     all_weights = dict()\n",
    "    #     with tf.variable_scope('weight', reuse=tf.AUTO_REUSE):\n",
    "    #         all_weights['enc_w0'] = tf.get_variable(\"enc_w0\",\n",
    "    #         shape=[self.kernel_size[0], self.kernel_size[0], 1, self.n_hidden[0]],\n",
    "    #         initializer=layers.xavier_initializer_conv2d(), regularizer=self.reg)\n",
    "    #         all_weights['enc_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype=tf.float32))\n",
    "    #\n",
    "    #         all_weights['dec_w0'] = tf.get_variable(\"dec_w0\",\n",
    "    #         shape=[self.kernel_size[0], self.kernel_size[0], 1, self.n_hidden[0]],\n",
    "    #         initializer=layers.xavier_initializer_conv2d(), regularizer=self.reg)\n",
    "    #         all_weights['dec_b0'] = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "    #     return all_weights\n",
    "\n",
    "    # Building the encoder\n",
    "    def encoder(self, x, weights):\n",
    "        shapes = []\n",
    "        # Encoder Hidden layer with relu activation #1\n",
    "        shapes.append(x.get_shape().as_list())\n",
    "        layer1 = tf.nn.bias_add(tf.nn.conv2d(x, weights['enc_w0'], strides=[1, 2, 2, 1], padding='SAME'),\n",
    "                                weights['enc_b0'])\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "        return layer1, shapes\n",
    "\n",
    "    # Building the decoder\n",
    "    def decoder(self, z, weights, shapes):\n",
    "        # Encoder Hidden layer with relu activation #1\n",
    "        shape_de1 = shapes[0]\n",
    "        layer1 = tf.add(tf.nn.conv2d_transpose(z, weights['dec_w0'], tf.stack(\n",
    "            [tf.shape(self.x)[0], shape_de1[1], shape_de1[2], shape_de1[3]]),\n",
    "                                               strides=[1, 2, 2, 1], padding='SAME'), weights['dec_b0'])\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "        return layer1\n",
    "\n",
    "    def selfexpressive_moduel(self, batch_size):\n",
    "\n",
    "        Coef = tf.Variable(1.0e-8 * tf.ones([self.batch_size, self.batch_size], tf.float32), name='Coef')\n",
    "        z_ssc = tf.matmul(Coef, self.z_conv)\n",
    "        return z_ssc, Coef\n",
    "\n",
    "    def finetune_fit(self, X, lr):\n",
    "        C, l1_cost, l2_cost, total_loss, summary, _ = self.sess.run(\n",
    "            (self.Coef, self.reg_ssc, self.cost_ssc, self.loss_ssc, self.merged_summary_op, self.optimizer_ssc), \\\n",
    "            feed_dict={self.x: X, self.learning_rate: lr})\n",
    "        self.summary_writer.add_summary(summary, self.iter)\n",
    "        self.iter = self.iter + 1\n",
    "        return C, l1_cost, l2_cost, total_loss\n",
    "\n",
    "    def initlization(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.sess.run(self.z_conv, feed_dict={self.x: X})\n",
    "\n",
    "    def save_model(self):\n",
    "        save_path = self.saver.save(self.sess, self.model_path)\n",
    "        print(\"model saved in file: %s\" % save_path)\n",
    "\n",
    "    def restore(self):\n",
    "        self.saver.restore(self.sess, self.model_path)\n",
    "        print(\"model restored\")\n",
    "\n",
    "    def best_map(self, L1, L2):\n",
    "        # L1 should be the labels and L2 should be the clustering number we got\n",
    "        Label1 = np.unique(L1)\n",
    "        nClass1 = len(Label1)\n",
    "        Label2 = np.unique(L2)\n",
    "        nClass2 = len(Label2)\n",
    "        nClass = np.maximum(nClass1, nClass2)\n",
    "        G = np.zeros((nClass, nClass))\n",
    "        for i in range(nClass1):\n",
    "            ind_cla1 = L1 == Label1[i]\n",
    "            ind_cla1 = ind_cla1.astype(float)\n",
    "            for j in range(nClass2):\n",
    "                ind_cla2 = L2 == Label2[j]\n",
    "                ind_cla2 = ind_cla2.astype(float)\n",
    "                G[i, j] = np.sum(ind_cla2 * ind_cla1)\n",
    "        m = Munkres()\n",
    "        index = m.compute(-G.T)\n",
    "        index = np.array(index)\n",
    "        c = index[:, 1]\n",
    "        newL2 = np.zeros(L2.shape)\n",
    "        for i in range(nClass2):\n",
    "            newL2[L2 == Label2[i]] = Label1[c[i]]\n",
    "        return newL2\n",
    "\n",
    "    def thrC(self, C, ro):\n",
    "        if ro < 1:\n",
    "            N = C.shape[1]\n",
    "            Cp = np.zeros((N, N))\n",
    "            S = np.abs(np.sort(-np.abs(C), axis=0))\n",
    "            Ind = np.argsort(-np.abs(C), axis=0)\n",
    "            for i in range(N):\n",
    "                cL1 = np.sum(S[:, i]).astype(float)\n",
    "                stop = False\n",
    "                csum = 0\n",
    "                t = 0\n",
    "                while (stop == False):\n",
    "                    csum = csum + S[t, i]\n",
    "                    if csum > ro * cL1:\n",
    "                        stop = True\n",
    "                        Cp[Ind[0:t + 1, i], i] = C[Ind[0:t + 1, i], i]\n",
    "                    t = t + 1\n",
    "        else:\n",
    "            Cp = C\n",
    "        return Cp\n",
    "\n",
    "    def post_proC(self, C, K, d, alpha):\n",
    "        # C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
    "        n = C.shape[0]\n",
    "        C = 0.5 * (C + C.T)\n",
    "        C = C - np.diag(np.diag(C)) + np.eye(n, n)  # for sparse C, this step will make the algorithm more numerically stable\n",
    "        r = d * K + 1\n",
    "        print('r = %s, C:%s' % (r, np.unique(C).shape))\n",
    "        U, S, _ = svds(C, r)\n",
    "        U = U[:, ::-1]\n",
    "        S = np.sqrt(S[::-1])\n",
    "        S = np.diag(S)\n",
    "        U = U.dot(S)\n",
    "        U = normalize(U, norm='l2', axis=1)\n",
    "        Z = U.dot(U.T)\n",
    "        Z = Z * (Z > 0)\n",
    "        L = np.abs(Z ** alpha)\n",
    "        L = L / L.max()\n",
    "        L = 0.5 * (L + L.T)\n",
    "        spectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',\n",
    "                                              assign_labels='discretize')\n",
    "        spectral.fit(L)\n",
    "        grp = spectral.fit_predict(L)\n",
    "        return grp, L\n",
    "\n",
    "    def cluster(self, X, n_cluster):\n",
    "        n_row, n_column, n_band = X.shape\n",
    "        img_transposed = np.transpose(X, axes=(2, 0, 1))  # Img.transpose()\n",
    "        img_transposed = np.reshape(img_transposed, (n_band, n_row, n_column, 1))\n",
    "        # ft_times = 30\n",
    "        alpha = 0.04\n",
    "        learning_rate = 1e-3\n",
    "        all_loss = []\n",
    "        for i in range(0, 1):\n",
    "            self.initlization()\n",
    "            for iter_ft in range(self.max_iter):\n",
    "                iter_ft = iter_ft + 1\n",
    "                C, l1_cost, l2_cost, total_loss = self.finetune_fit(img_transposed, learning_rate)\n",
    "                print('# epoch %s' % (iter_ft))\n",
    "                all_loss.append(total_loss)\n",
    "            C = self.thrC(C, alpha)\n",
    "            y_x, CKSym_x = self.post_proC(C, n_cluster, 1, 4)\n",
    "            print(all_loss)\n",
    "            return y_x\n",
    "                # all_loss.append(total_loss)\n",
    "                # if iter_ft % display_step == 0:\n",
    "                #     print(\"epoch: %.1d\" % iter_ft,\n",
    "                #           \"L1 cost: %.8f, L2 cost: %.8f, total cost: %.8f\" % (l1_cost, l2_cost, total_loss))\n",
    "                #     C = self.thrC(C, alpha)\n",
    "                #     y_x, CKSym_x = self.post_proC(C, n_cluster, 1, 4)\n",
    "                    # bands = self.select_band(y_x, img)  # n_row * n_clm * n_class\n",
    "                    # score = dsc_bs.eval_band(bands, gt, train_inx, test_idx)\n",
    "                    # all_acc.append(score)\n",
    "                    # print('eval score:', score)\n",
    "            # print(all_loss)\n",
    "            # print(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "h8uGxFGnlW_p"
   },
   "outputs": [],
   "source": [
    "#@title DSC Band Selection Class\n",
    "class DSCBS(object):\n",
    "    \"\"\"\n",
    "    Select band subset using DSC algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, n_band, **kwargs_DSC):\n",
    "        self.n_band = n_band\n",
    "        self.dsc = DSC_NET(**kwargs_DSC)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        :param X: Array-like with size (n_row, n_column, n_band)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        cluster_result = self.dsc.cluster(X, self.n_band)\n",
    "        selected_band = []\n",
    "        n_row, n_column, n_band = X.shape\n",
    "        n_cluster = np.unique(cluster_result).__len__()\n",
    "        img_ = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "        for c in np.unique(cluster_result):\n",
    "            idx = np.nonzero(cluster_result == c)\n",
    "            center = np.mean(img_[:, idx[0]], axis=1).reshape((-1, 1))\n",
    "            distance = np.linalg.norm(img_[:, idx[0]] - center, axis=0)\n",
    "            band_ = img_[:, idx[0]][:, distance.argmin()]\n",
    "            selected_band.append(band_)\n",
    "        bands = np.asarray(selected_band)\n",
    "        bands = bands.reshape(n_cluster, n_row, n_column)\n",
    "        bands = np.transpose(bands, axes=(1, 2, 0))\n",
    "        self.bands = bands\n",
    "        return self.bands\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     from Toolbox.Preprocessing import Processor\n",
    "#     from sklearn.preprocessing import minmax_scale\n",
    "#\n",
    "#     root = 'F:\\\\Python\\\\HSI_Files\\\\'\n",
    "#     # im_, gt_ = 'SalinasA_corrected', 'SalinasA_gt'\n",
    "#     im_, gt_ = 'Indian_pines_corrected', 'Indian_pines_gt'\n",
    "#     # im_, gt_ = 'Pavia', 'Pavia_gt'\n",
    "#     # im_, gt_ = 'Botswana', 'Botswana_gt'\n",
    "#     # im_, gt_ = 'KSC', 'KSC_gt'\n",
    "#\n",
    "#     img_path = root + im_ + '.mat'\n",
    "#     gt_path = root + gt_ + '.mat'\n",
    "#     print(img_path)\n",
    "#\n",
    "#     p = Processor()\n",
    "#     img, gt = p.prepare_data(img_path, gt_path)\n",
    "#     # Img, Label = Img[:256, :, :], Label[:256, :]\n",
    "#     n_row, n_column, n_band = img.shape\n",
    "#     train_inx, test_idx = p.get_tr_tx_index(p.get_correct(img, gt)[1], test_size=0.9)\n",
    "#\n",
    "#     img_train = minmax_scale(img.reshape(n_row * n_column, n_band)).reshape((n_row, n_column, n_band))\n",
    "#     # img_train = np.transpose(img_train, axes=(2, 0, 1))  # Img.transpose()\n",
    "#     # img_train = np.reshape(img_train, (n_band, n_row, n_column, 1))\n",
    "#\n",
    "#     n_input = [n_row, n_column]\n",
    "#     kernel_size = [11]\n",
    "#     n_hidden = [16]\n",
    "#     batch_size = n_band\n",
    "#     model_path = './pretrain-model-COIL20/model.ckpt'\n",
    "#     ft_path = './pretrain-model-COIL20/model.ckpt'\n",
    "#     logs_path = './pretrain-model-COIL20/logs'\n",
    "#\n",
    "#     num_class = 5  # how many class we sample\n",
    "#     batch_size_test = n_band\n",
    "#\n",
    "#     iter_ft = 0\n",
    "#     ft_times = 50\n",
    "#     display_step = 1\n",
    "#     alpha = 0.04\n",
    "#     learning_rate = 1e-3\n",
    "#\n",
    "#     reg1 = 1e-4\n",
    "#     reg2 = 150.0\n",
    "#     kwargs = {'n_input': n_input, 'n_hidden': n_hidden, 'reg_const1': reg1, 'reg_const2': reg2,\n",
    "#               'kernel_size': kernel_size,'batch_size': batch_size_test, 'model_path': model_path, 'logs_path': logs_path}\n",
    "#     dscbs = DSCBS(10, **kwargs)\n",
    "#     dscbs.fit(img_train)\n",
    "#     bands = dscbs.predict(img_train)\n",
    "#     print(bands.shape)\n",
    "#\n",
    "#\n",
    "#\n",
    "#     # CAE = ConvAE(n_input=n_input, n_hidden=n_hidden, reg_const1=reg1, reg_const2=reg2, kernel_size=kernel_size,\n",
    "#     #              batch_size=batch_size_test, model_path=model_path, logs_path=logs_path)\n",
    "#\n",
    "#     # acc_ = []\n",
    "#     # all_loss = []\n",
    "#     # all_acc = []\n",
    "#     # for i in range(0, 1):\n",
    "#     #     # coil20_all_subjs = copy.deepcopy(Img)\n",
    "#     #     # coil20_all_subjs = coil20_all_subjs.astype(float)\n",
    "#     #     # label_all_subjs = copy.deepcopy(Label)\n",
    "#     #     # label_all_subjs = label_all_subjs - label_all_subjs.min() + 1\n",
    "#     #     # label_all_subjs = np.squeeze(label_all_subjs)\n",
    "#     #     CAE.initlization()\n",
    "#     #     # CAE.restore()\n",
    "#     #     for iter_ft in range(ft_times):\n",
    "#     #         iter_ft = iter_ft + 1\n",
    "#     #         C, l1_cost, l2_cost, total_loss = CAE.finetune_fit(img_train, learning_rate)\n",
    "#     #         all_loss.append(total_loss)\n",
    "#     #         if iter_ft % display_step == 0:\n",
    "#     #             print(\"epoch: %.1d\" % iter_ft,\n",
    "#     #                   \"L1 cost: %.8f, L2 cost: %.8f, total cost: %.8f\" % (l1_cost, l2_cost, total_loss))\n",
    "#     #             C = thrC(C, alpha)\n",
    "#     #             y_x, CKSym_x = post_proC(C, num_class, 1, 4)\n",
    "#     #             bands = band_selection(y_x, img)  # n_row * n_clm * n_class\n",
    "#     #             score = eval_band(bands, gt, train_inx, test_idx)\n",
    "#     #             all_acc.append(score)\n",
    "#     #             print('eval score:', score)\n",
    "#     #     print(all_loss)\n",
    "#     #     print(all_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XZGyfWplW_p"
   },
   "source": [
    "## 7.5) ISSC Band Selection\n",
    "Ref:\n",
    "    [1]\tW. Sun, L. Zhang, B. Du, W. Li, and Y. Mark Lai, \"Band Selection Using Improved Sparse Subspace Clustering\n",
    "    for Hyperspectral Imagery Classification,\" IEEE Journal of Selected Topics in Applied Earth Observations and\n",
    "    Remote Sensing, vol. 8, pp. 2784-2797, 2015.\n",
    "\n",
    "Formula:\n",
    "    arg min ||X - XW||_F + lambda||W||_F subject to diag(Z) = 0\n",
    "Solution:\n",
    "    W = (X^T X + lambda*I)^1 (diag((X^T X + lambda*I)1))^1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2aZHnmbAlW_q"
   },
   "outputs": [],
   "source": [
    "#@title ISSC Hyperspectral Image Class\n",
    "class ISSC_HSI(object):\n",
    "    \"\"\"\n",
    "    :argument:\n",
    "        Implementation of L2 norm based sparse self-expressive clustering model\n",
    "        with affinity measurement basing on angular similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, n_band=10, coef_=1):\n",
    "        self.n_band = n_band\n",
    "        self.coef_ = coef_\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: shape [n_row*n_clm, n_band]\n",
    "        :return: selected band subset\n",
    "        \"\"\"\n",
    "        I = np.eye(X.shape[1])\n",
    "        coefficient_mat = -1 * np.dot(np.linalg.inv(np.dot(X.transpose(), X) + self.coef_ * I),\n",
    "                                      np.linalg.inv(np.diag(np.diag(np.dot(X.transpose(), X) + self.coef_ * I))))\n",
    "        temp = np.linalg.norm(coefficient_mat, axis=0).reshape(1, -1)\n",
    "        affinity = (np.dot(coefficient_mat.transpose(), coefficient_mat) /\n",
    "                    np.dot(temp.transpose(), temp))**2\n",
    "\n",
    "        sc = SpectralClustering(n_clusters=self.n_band, affinity='precomputed')\n",
    "        sc.fit(affinity)\n",
    "        selected_band, band_list = self.__get_band(sc.labels_, X)\n",
    "        return selected_band, band_list\n",
    "\n",
    "    def __get_band(self, cluster_result, X):\n",
    "        \"\"\"\n",
    "        select band according to the center of each cluster\n",
    "        :param cluster_result:\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        selected_band = []\n",
    "        band_list = []\n",
    "        n_cluster = np.unique(cluster_result).__len__()\n",
    "        # img_ = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "        for c in np.unique(cluster_result):\n",
    "            idx = np.nonzero(cluster_result == c)\n",
    "            center = np.mean(X[:, idx[0]], axis=1).reshape((-1, 1))\n",
    "            distance = np.linalg.norm(X[:, idx[0]] - center, axis=0)\n",
    "            band_list.append(idx[0][distance.argmin()])\n",
    "            band_ = X[:, idx[0]][:, distance.argmin()]\n",
    "            selected_band.append(band_)\n",
    "        bands = np.asarray(selected_band).transpose()\n",
    "        # bands = bands.reshape(n_cluster, n_row, n_column)\n",
    "        # bands = np.transpose(bands, axes=(1, 2, 0))\n",
    "        return bands, band_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blN8c0KelW_q"
   },
   "source": [
    "## 7.6) Lap Score Band Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2KPJ2gF6lW_q"
   },
   "outputs": [],
   "source": [
    "#@title Lap Score Hyperspectral Image Class\n",
    "class Lap_score_HSI(object):\n",
    "\n",
    "    def __init__(self, n_band=10):\n",
    "        self.n_band = n_band\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: shape [n_row*n_clm, n_band]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # n_row, n_column, __n_band = X.shape\n",
    "        # XX = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "        XX = X\n",
    "\n",
    "        kwargs_W = {\"metric\": \"euclidean\", \"neighbor_mode\": \"knn\", \"weight_mode\": \"heat_kernel\", \"k\": 5, 't': 1}\n",
    "        W = construct_W.construct_W(XX, **kwargs_W)\n",
    "\n",
    "        # obtain the scores of features\n",
    "        score = lap_score.lap_score(X, W=W)\n",
    "\n",
    "        # sort the feature scores in an ascending order according to the feature scores\n",
    "        idx = lap_score.feature_ranking(score)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "        selected_features = X[:, idx[0:self.n_band]]\n",
    "\n",
    "        # selected_features.reshape((self.n_band, n_row, n_column))\n",
    "        # selected_features = np.transpose(selected_features, axes=(1, 2, 0))\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGvyd3IalW_q"
   },
   "source": [
    "## 7.7) NDFS Band Selection\n",
    "Nonnegative Discriminative Feature Selection (NDFS)\n",
    "Reference:\n",
    "    Li, Zechao, et al. \"Unsupervised Feature Selection Using Nonnegative Spectral Analysis.\" AAAI. 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8u8T53SrlW_q"
   },
   "outputs": [],
   "source": [
    "#@title NDFS Hyperspectral Image Class\n",
    "class NDFS_HSI(object):\n",
    "\n",
    "    def __init__(self, n_cluster, n_band=10):\n",
    "        self.n_band = n_band\n",
    "        self.n_cluster = n_cluster\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: shape [n_row*n_clm, n_band]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # construct affinity matrix\n",
    "        kwargs = {\"metric\": \"euclidean\", \"neighborMode\": \"knn\", \"weightMode\": \"heatKernel\", \"k\": 5, 't': 1}\n",
    "        W = construct_W.construct_W(X, **kwargs)\n",
    "\n",
    "        # obtain the feature weight matrix\n",
    "        Weight = NDFS.ndfs(X, W=W, n_clusters=self.n_cluster)\n",
    "\n",
    "        # sort the feature scores in an ascending order according to the feature scores\n",
    "        idx = feature_ranking(Weight)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "        selected_features = X[:, idx[0:self.n_band]]\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCKXAMXblW_q"
   },
   "source": [
    "## 7.8) SNMF Band Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CVeK_9iulW_r"
   },
   "outputs": [],
   "source": [
    "#@title SNMF Band Selection Class\n",
    "class BandSelection_SNMF(object):\n",
    "    def __init__(self, n_band):\n",
    "        self.n_band = n_band\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: with shape (n_pixel, n_band)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # # Note that X has to reshape to (n_fea., n_sample)\n",
    "        # XX = X.transpose()  # (n_band, n_pixel)\n",
    "        # snmf = nimfa.Snmf(X, seed=\"random_c\", rank=self.n_band)  # remain para. default\n",
    "        snmf = nimfa.Snmf(X, rank=self.n_band, max_iter=20, version='r', eta=1.,\n",
    "                          beta=1e-4, i_conv=10, w_min_change=0)\n",
    "        snmf_fit = snmf()\n",
    "        W = snmf.basis()  # shape: n_band * k\n",
    "        H = snmf.coef()  # shape: k * n_pixel\n",
    "\n",
    "        #  get clustering res.\n",
    "        H = np.asarray(H)\n",
    "        indx_sort = np.argsort(H, axis=0)  # ascend order\n",
    "        cluster_res = indx_sort[-1].reshape(-1)\n",
    "\n",
    "        #  select band\n",
    "        selected_band = []\n",
    "        for c in np.unique(cluster_res):\n",
    "            idx = np.nonzero(cluster_res == c)\n",
    "            center = np.mean(X[:, idx[0]], axis=1).reshape((-1, 1))\n",
    "            distance = np.linalg.norm(X[:, idx[0]] - center, axis=0)\n",
    "            band_ = X[:, idx[0]][:, distance.argmin()]\n",
    "            selected_band.append(band_)\n",
    "        while selected_band.__len__() < self.n_band:\n",
    "            selected_band.append(np.zeros(X.shape[0]))\n",
    "        bands = np.asarray(selected_band).transpose()\n",
    "        return bands\n",
    "\n",
    "    # # WH\n",
    "    # def getWH(self, x_input, rank=10):\n",
    "    #     snmf = nimfa.Snmf(x_input, seed=\"random_c\", rank=rank, max_iter=12, version='r', eta=1.,\n",
    "    #                         beta=1e-4, i_conv=10, w_min_change=0)\n",
    "    #     snmf_fit = snmf()\n",
    "    #     W = snmf.basis()\n",
    "    #     H = snmf.coef()\n",
    "    #     return W, H\n",
    "    #\n",
    "    # # H\n",
    "    # def maxh_selection(self, H):\n",
    "    #     selection_h = []\n",
    "    #     n_row, n_column = H.shape\n",
    "    #     for i in range(n_column):\n",
    "    #         max = H[0, i]\n",
    "    #         for j in range(n_row-1):\n",
    "    #             if H[j+1,i]>H[j,i]:\n",
    "    #                 max = H[j+1, i]\n",
    "    #         selection_h.append(max)\n",
    "    #     return selection_h\n",
    "    #\n",
    "    # def fit(self, X):\n",
    "    #     self.X = X\n",
    "    #     return self\n",
    "    #\n",
    "    # # \n",
    "    # def predict(self, X):\n",
    "    #     \"\"\"\n",
    "    #     Select band according to clustering center\n",
    "    #     :param X: array like: shape (n_row, n_column, n_band)\n",
    "    #     :return:\n",
    "    #     \"\"\"\n",
    "    #     n_row, n_column, n_band = X.shape\n",
    "    #     XX = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "    #     self.W, self.H = self.getWH(XX, rank=self.n_band)\n",
    "    #     cluster_result = self.maxh_selection(self.H)\n",
    "    #     selected_band = []\n",
    "    #     n_cluster = np.unique(cluster_result).__len__()\n",
    "    #     for c in np.unique(cluster_result):\n",
    "    #         idx = np.nonzero(cluster_result == c)\n",
    "    #         center = np.mean(XX[:, idx[0]], axis=1).reshape((-1, 1))\n",
    "    #         distance = np.linalg.norm(XX[:, idx[0]] - center, axis=0)\n",
    "    #         band_ = XX[:, idx[0]][:, distance.argmin()]\n",
    "    #         selected_band.append(band_)\n",
    "    #     bands = np.asarray(selected_band)\n",
    "    #     bands = bands.reshape(n_cluster, n_row, n_column)\n",
    "    #     bands = np.transpose(bands, axes=(1, 2, 0))\n",
    "    #     return bands\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNzMcw2flW_r"
   },
   "source": [
    "## 7.9) SpaBS Band Selection\n",
    "@ Author by Zeng Meng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mJLHZxUGlW_r"
   },
   "outputs": [],
   "source": [
    "#@title Approximate KSVD Class\n",
    "class ApproximateKSVD(object):\n",
    "    def __init__(self, n_components, max_iter=10, tol=1e-6,\n",
    "                 transform_n_nonzero_coefs=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_components:\n",
    "            Number of dictionary elements\n",
    "        max_iter:\n",
    "            Maximum number of iterations\n",
    "        tol:\n",
    "            tolerance for error\n",
    "        transform_n_nonzero_coefs:\n",
    "            Number of nonzero coefficients to target\n",
    "        \"\"\"\n",
    "        self.components_ = None\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_components = n_components\n",
    "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n",
    "\n",
    "    def _update_dict(self, X, D, gamma):\n",
    "        for j in range(self.n_components):\n",
    "            I = gamma[:, j] > 0\n",
    "            if np.sum(I) == 0:\n",
    "                continue\n",
    "\n",
    "            D[j, :] = 0\n",
    "            g = gamma[I, j].T\n",
    "            r = X[I, :] - gamma[I, :].dot(D)\n",
    "            d = r.T.dot(g)\n",
    "            d /= np.linalg.norm(d)\n",
    "            g = r.dot(d)\n",
    "            D[j, :] = d\n",
    "            gamma[I, j] = g.T\n",
    "        return D, gamma\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        if min(X.shape) <= self.n_components:\n",
    "            D = np.random.randn(self.n_components, X.shape[1])\n",
    "        else:\n",
    "            u, s, vt = sp.sparse.linalg.svds(X, k=self.n_components)\n",
    "            D = np.dot(np.diag(s), vt)\n",
    "        D /= np.linalg.norm(D, axis=1)[:, np.newaxis]\n",
    "        return D\n",
    "\n",
    "    def _transform(self, D, X):\n",
    "        gram = D.dot(D.T)\n",
    "        Xy = D.dot(X.T)\n",
    "\n",
    "        n_nonzero_coefs = self.transform_n_nonzero_coefs\n",
    "        if n_nonzero_coefs is None:\n",
    "            n_nonzero_coefs = int(0.1 * X.shape[1])\n",
    "\n",
    "        return orthogonal_mp_gram(\n",
    "            gram, Xy, n_nonzero_coefs=n_nonzero_coefs).T\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: shape = [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        D = self._initialize(X)\n",
    "        for i in range(self.max_iter):\n",
    "            gamma = self._transform(D, X)\n",
    "            e = np.linalg.norm(X - gamma.dot(D))\n",
    "            if e < self.tol:\n",
    "                break\n",
    "            D, gamma = self._update_dict(X, D, gamma)\n",
    "\n",
    "        self.components_ = D\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._transform(self.components_, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-vE5XjOclW_r"
   },
   "outputs": [],
   "source": [
    "#@title SpaBS Class\n",
    "class SpaBS(object):\n",
    "\n",
    "    def __init__(self, n_band, sparsity_level=0.5):\n",
    "        self.n_band = n_band\n",
    "        self.sparsity_level = sparsity_level\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Select band according to sparse representation\n",
    "        :param X: array like: shape (n_row*n_column, n_band)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # n_row, n_column, n_band = X.shape\n",
    "        # XX = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "        # SpaBS\n",
    "        # ksvd\n",
    "        # TODO: according to ref., X has to be with shape (n_band, n_sample)\n",
    "        # X = X.transpose()\n",
    "        dico = ApproximateKSVD(n_components=X.shape[1])\n",
    "        dico.fit(X)\n",
    "        gamma_ = dico.transform(X)  # gamma, shape(n_sample, n_atom)\n",
    "        gamma = gamma_.transpose()\n",
    "        sorted_inx = np.argsort(gamma, axis=0)  # ascending order for each column\n",
    "        K = X.shape[0] * self.sparsity_level\n",
    "        largest_k = sorted_inx[-self.n_band:, :]\n",
    "\n",
    "        # # statistic\n",
    "        element, freq = np.unique(largest_k, return_counts=True)\n",
    "        selected_inx = element[np.argsort(freq)][-self.n_band:]\n",
    "        selected_band = X[:, selected_inx]\n",
    "        return selected_band\n",
    "\n",
    "\n",
    "'''\n",
    "---------------------------\n",
    "        Test\n",
    "'''\n",
    "\n",
    "# X ~ gamma.dot(dictionary)\n",
    "# X = np.random.randn(1000, 20)\n",
    "# aksvd = ApproximateKSVD(n_components=20)\n",
    "# dictionary = aksvd.fit(X).components_\n",
    "# gamma = aksvd.transform(X)\n",
    "# print(gamma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oh1noshlW_r"
   },
   "source": [
    "## 7.10) SPEC Band Selection\n",
    "Ref:\n",
    "    Zheng Zhao and Huan Liu. 2007. Spectral feature selection for supervised and unsupervised learning. In Proceedings\n",
    "    of the 24th international conference on Machine learning (ICML '07), Zoubin Ghahramani (Ed.). ACM, New York, NY,\n",
    "    USA, 1151-1157."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zMkre5FplW_s"
   },
   "outputs": [],
   "source": [
    "#@title SPEC Hyperspectral Image Class\n",
    "class SPEC_HSI(object):\n",
    "\n",
    "    def __init__(self, n_band=10):\n",
    "        self.n_band = n_band\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: shape [n_row*n_clm, n_band]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # specify the second ranking function which uses all except the 1st eigenvalue\n",
    "        kwargs = {'style': 0}\n",
    "        # n_row, n_column, __n_band = X.shape\n",
    "        # XX = X.reshape((n_row * n_column, -1))  # n_sample * n_band\n",
    "        XX = X\n",
    "\n",
    "        # obtain the scores of features\n",
    "        score = SPEC.spec(XX, **kwargs)\n",
    "\n",
    "        # sort the feature scores in an descending order according to the feature scores\n",
    "        idx = SPEC.feature_ranking(score, **kwargs)\n",
    "\n",
    "        # obtain the dataset on the selected features\n",
    "        selected_features = XX[:, idx[0:self.n_band]]\n",
    "        # selected_features.reshape((self.n_band, n_row, n_column))\n",
    "        # selected_features = np.transpose(selected_features, axes=(1, 2, 0))\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh03k6IQlW_s"
   },
   "source": [
    "## 7.11) SSR Band Selection\n",
    "Ref:\n",
    "    ,,.[J]. , 2017, 42(4): 441-448.\n",
    "    SUN Weiwei,JIANG Man,LI Weiyue.Band Selection Using Sparse Self-representation for Hyperspectral Imagery[J].\n",
    "    GEOMATICS AND INFORMATION SCIENCE OF WUHAN UNIVERS, 2017, 42(4): 441-448."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "j_K5hcyllW_s"
   },
   "outputs": [],
   "source": [
    "#@title SSC Band Selection Class\n",
    "class SSC_BS(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden, n_clusters, lambda_coef=1):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_clusters = n_clusters\n",
    "        self.lambda_coef = lambda_coef\n",
    "\n",
    "    def fit_predict_omp(self, X, y=None):\n",
    "        n_sample = X.transpose().shape[0]\n",
    "        H = X.transpose()      #NRP_ELM(self.n_hidden, sparse=False).fit(X).predict(X)\n",
    "        C = np.zeros((n_sample, n_sample))\n",
    "        # solve sparse self-expressive representation\n",
    "        for i in range(n_sample):\n",
    "            y_i = H[i]\n",
    "            H_i = np.delete(H, i, axis=0)\n",
    "            # H_T = H_i.transpose()  # M x (N-1)\n",
    "            omp = OrthogonalMatchingPursuit(n_nonzero_coefs=int(n_sample * 0.5), tol=1e20)\n",
    "            omp.fit(H_i.transpose(), y_i)\n",
    "            #  Normalize the columns of C: ci = ci / ||ci||_ss.\n",
    "            coef = omp.coef_ / np.max(np.abs(omp.coef_))\n",
    "            C[:i, i] = coef[:i]\n",
    "            C[i+1:, i] = coef[i:]\n",
    "        # # compute affinity matrix\n",
    "        # L = 0.5 * (np.abs(C) + np.abs(C.T))  # affinity graph\n",
    "        # # L = 0.5 * (C + C.T)\n",
    "        # self.affinity_matrix = L\n",
    "        # # spectral clustering\n",
    "        # sc = SpectralClustering(n_clusters=self.n_clusters, affinity='precomputed')\n",
    "        # sc.fit(self.affinity_matrix)\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, max_iter=500).fit(C)\n",
    "        label = kmeans.labels_\n",
    "        C_ = C\n",
    "        band_index = []\n",
    "        for i in np.unique(label):\n",
    "            index__ = np.nonzero(label == i)\n",
    "            centroids_ = C_[index__]\n",
    "            centroids = centroids_.mean(axis=0)\n",
    "            dis = pairwise_distances(centroids_, centroids.reshape((1, centroids_.shape[1]))).flatten()\n",
    "            index_min = np.argmin(dis)\n",
    "            C_bestrow = centroids_[index_min, :]\n",
    "            index = np.nonzero(np.all(C_ == C_bestrow, axis=1))\n",
    "            band_index.append(index[0][0])\n",
    "        BandData = X[:, band_index]  # BandData = self.X[:, band_index]\n",
    "        print('selected band:', band_index)\n",
    "        return BandData  #sc.labels_\n",
    "\n",
    "    def fit_predict_close(self, X, raw_input_=False):\n",
    "        \"\"\"\n",
    "        using close-form solution\n",
    "        :param X:\n",
    "        :param raw_input_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n_sample = X.transpose().shape[0]\n",
    "        if raw_input_ is True:\n",
    "            H = X.transpose()\n",
    "        else:\n",
    "            H = X.transpose()    #NRP_ELM(self.n_hidden, sparse=False).fit(X).predict(X)\n",
    "        C = np.zeros((n_sample, n_sample))\n",
    "        for i in range(n_sample):\n",
    "            y_i = H[i]\n",
    "            H_i = np.delete(H, i, axis=0).transpose()\n",
    "            term_1 = np.linalg.inv(np.dot(H_i.transpose(), H_i) + self.lambda_coef * np.eye(n_sample - 1))\n",
    "            w = np.dot(np.dot(term_1, H_i.transpose()), y_i.reshape((y_i.shape[0], 1)))\n",
    "            w = w.flatten()\n",
    "            #  Normalize the columns of C: ci = ci / ||ci||_ss.\n",
    "            coef = w / np.max(np.abs(w))\n",
    "            C[:i, i] = coef[:i]\n",
    "            C[i + 1:, i] = coef[i:]\n",
    "        # # compute affinity matrix\n",
    "        # L = 0.5 * (np.abs(C) + np.abs(C.T))  # affinity graph\n",
    "        # self.affinity_matrix = L\n",
    "        # # spectral clustering\n",
    "        # sc = SpectralClustering(n_clusters=self.n_clusters, affinity='precomputed')\n",
    "        # sc.fit(self.affinity_matrix)\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, max_iter=500).fit(C)\n",
    "        label = kmeans.labels_\n",
    "        C_ = C\n",
    "        band_index = []\n",
    "        for i in np.unique(label):\n",
    "            index__ = np.nonzero(label == i)\n",
    "            centroids_ = C_[index__]\n",
    "            centroids = centroids_.mean(axis=0)\n",
    "            dis = pairwise_distances(centroids_, centroids.reshape((1, centroids_.shape[1]))).flatten()\n",
    "            index_min = np.argmin(dis)\n",
    "            C_bestrow = centroids_[index_min, :]\n",
    "            index = np.nonzero(np.all(C_ == C_bestrow, axis=1))\n",
    "            band_index.append(index[0][0])\n",
    "        BandData = X[:, band_index]  # BandData = self.X[:, band_index]\n",
    "        print('selected band:', band_index)\n",
    "        return BandData  # sc.labels_\n",
    "\n",
    "    def fit_predict_cvx(self, X):\n",
    "        n_sample = X.transpose().shape[0]\n",
    "        H = X.transpose()  #NRP_ELM(self.n_hidden, sparse=False).fit(X).predict(X)\n",
    "        C = np.zeros((n_sample, n_sample))\n",
    "        # solve sparse self-expressive representation\n",
    "        for i in range(n_sample):\n",
    "            y_i = H[i]\n",
    "            H_i = np.delete(H, i, axis=0)\n",
    "            # H_T = H_i.transpose()  # M x (N-1)\n",
    "            # omp = OrthogonalMatchingPursuit(n_nonzero_coefs=500)\n",
    "            # omp.fit(H_i.transpose(), y_i)\n",
    "            w = cvx.Variable(n_sample-1)\n",
    "            objective = cvx.Minimize(0.5 * cvx.sum_squares(H_i.transpose() * w - y_i) + 0.5 * self.lambda_coef * cvx.norm(w, 1))\n",
    "            prob = cvx.Problem(objective)\n",
    "            result = prob.solve()\n",
    "            #  Normalize the columns of C: ci = ci / ||ci||_ss.\n",
    "            ww = np.asarray(w.value).flatten()\n",
    "            coef = ww / np.max(np.abs(ww))\n",
    "            C[:i, i] = coef[:i]\n",
    "            C[i + 1:, i] = coef[i:]\n",
    "        # compute affinity matrix\n",
    "        # L = 0.5 * (np.abs(C) + np.abs(C.T))  # affinity graph\n",
    "        # # L = 0.5 * (C + C.T)\n",
    "        # self.affinity_matrix = L\n",
    "        # # spectral clustering\n",
    "        # sc = SpectralClustering(n_clusters=self.n_clusters, affinity='precomputed')\n",
    "        # sc.fit(self.affinity_matrix)\n",
    "        # k-means clustering\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, max_iter=500).fit(C)\n",
    "        label = kmeans.labels_\n",
    "        C_ = C\n",
    "        band_index = []\n",
    "        for i in np.unique(label):\n",
    "            index__ = np.nonzero(label == i)\n",
    "            centroids_ = C_[index__]\n",
    "            centroids = centroids_.mean(axis=0)\n",
    "            dis = pairwise_distances(centroids_, centroids.reshape((1, centroids_.shape[1]))).flatten()\n",
    "            index_min = np.argmin(dis)\n",
    "            C_bestrow = centroids_[index_min, :]\n",
    "            index = np.nonzero(np.all(C_ == C_bestrow, axis=1))\n",
    "            band_index.append(index[0][0])\n",
    "        BandData = X[:, band_index]   #BandData = self.X[:, band_index]\n",
    "        print ('selected band:', band_index)\n",
    "        return BandData  #sc.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5QvARL2lW_s"
   },
   "source": [
    "## 7.12 Band Selection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Mjc87VBUlW_s"
   },
   "outputs": [],
   "source": [
    "#@title Setup 'Band Selection' function\n",
    "def band_selection(data, gt, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    band_reduction_method = hyperparams['band_reduction_method']\n",
    "    n_components = hyperparams['n_components']\n",
    "\n",
    "    orig_rows, orig_cols, orig_channels = data.shape\n",
    "    bands_selected = None\n",
    "\n",
    "    band_selection_start = time.time()\n",
    "\n",
    "    if band_reduction_method == 'pca':\n",
    "        print('Using PCA dimensionality reduction on data...')\n",
    "\n",
    "        # https://towardsdatascience.com/pca-on-hyperspectral-data-99c9c5178385\n",
    "        print('Reshaping the data into two dimensions...')\n",
    "        data = data.reshape(data.shape[0]*data.shape[1], -1)\n",
    "        print(f'Reshaped data shape: {data.shape}')\n",
    "\n",
    "        if n_components is None: n_components = 'mle'\n",
    "        print('Fitting PCA to data...')\n",
    "        pca = PCA(n_components=n_components,\n",
    "                  svd_solver='auto',\n",
    "                  tol=0.0,\n",
    "                  iterated_power='auto',\n",
    "                  random_state=hyperparams['random_seed'])\n",
    "        fit_start = time.time()\n",
    "        pca.fit(data)\n",
    "        fit_end = time.time()\n",
    "        fit_runtime = datetime.timedelta(seconds=(fit_end - fit_start))\n",
    "        print(f'PCA fitting completed! Fit runtime: {fit_runtime}')\n",
    "\n",
    "        print(f'PCA fit data to {pca.n_components_} components!')\n",
    "        data = pca.transform(data)\n",
    "        print(f'New data shape: {data.shape}')\n",
    "        data = np.reshape(data, (orig_rows, orig_cols, data.shape[-1]))\n",
    "        print(f'Reshaped new data shape: {data.shape}')\n",
    "    elif band_reduction_method == 'ica':\n",
    "        print('Using ICA dimensionality reduction on data...')\n",
    "\n",
    "        print('Reshaping the data into two dimensions...')\n",
    "        data = data.reshape(data.shape[0]*data.shape[1], -1)\n",
    "        print(f'Reshaped data shape: {data.shape}')\n",
    "\n",
    "        print('Fitting ICA to data...')\n",
    "        ica = FastICA(n_components=n_components,\n",
    "                  algorithm='parallel',\n",
    "                  fun='logcosh',\n",
    "                  fun_args=None,\n",
    "                  max_iter=200,\n",
    "                  tol=1e-4,\n",
    "                  w_init=None,\n",
    "                  random_state=hyperparams['random_seed'])\n",
    "        fit_start = time.time()\n",
    "        ica.fit(data.astype(np.float32))\n",
    "        fit_end = time.time()\n",
    "        fit_runtime = datetime.timedelta(seconds=(fit_end - fit_start))\n",
    "        print(f'ICA fitting completed! Fit runtime: {fit_runtime}')\n",
    "        print(f'ICA found {ica.n_features_in_} features while fitting!')\n",
    "        data = ica.transform(data)\n",
    "        print(f'New data shape: {data.shape}')\n",
    "        data = np.reshape(data, (orig_rows, orig_cols, data.shape[-1]))\n",
    "        print(f'Reshaped new data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'cae-ssc':\n",
    "        print('Using CAE SSC dimensionality reduction on data...')\n",
    "        cae_ssc = CAE_BS(n_band=n_components)\n",
    "        # cae_ssc = CAE_BS(n_band=data.shape[-1])\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = cae_ssc.predict(np.array([gt.flatten(), data.shape[-1]]),\n",
    "        #                                  data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        # bands_selected = cae_ssc.predict(data.reshape(data.shape[0]*data.shape[1], -1),\n",
    "        #                                  data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data = cae_ssc.predict(data.reshape(data.shape[0]*data.shape[1], -1),\n",
    "                                         data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "        print(f'CAE SSC prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by CAE SSC: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'dsc-net':\n",
    "        print('Using DSC-NET dimensionality reduction on data...')\n",
    "        dscnet = DSCBS(n_band=n_components,\n",
    "                       n_input=(data.shape[0]*data.shape[1], data.shape[2]),\n",
    "                       kernel_size=(3,),\n",
    "                       n_hidden=2)\n",
    "        # dscnet = DSCBS(n_band=data.shape[-1])\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = dscnet.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data = dscnet.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "        print(f'DSC-NET prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by DSC-NET: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'issc':\n",
    "        print('Using ISSC dimensionality reduction on data...')\n",
    "        issc = ISSC_HSI(n_band=n_components)\n",
    "        # issc = ISSC_HSI(n_band=data.shape[-1])\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = issc.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data, bands_selected = issc.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "\n",
    "        # Sort bands\n",
    "        bands_selected = sorted(bands_selected)\n",
    "\n",
    "        print(f'ISSC prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by ISSC: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'ndfs':\n",
    "        print('Using NDFS dimensionality reduction on data...')\n",
    "        ndfs = NDFS_HSI(n_band=data.shape[-1], n_cluster=n_components)\n",
    "        # ndfs = NDFS_HSI(n_band=data.shape[-1])\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = ndfs.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data = ndfs.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "        print(f'NDFS prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by NDFS: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'snmf':\n",
    "        print('Using SNMF dimensionality reduction on data...')\n",
    "        snmf = BandSelection_SNMF(n_band=n_components)\n",
    "        # snmf = BandSelection_SNMF(n_band=data.shape[-1])\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = snmf.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data = snmf.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "        print(f'SNMF prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by SNMF: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'spabs':\n",
    "        print('Using SpaBS dimensionality reduction on data...')\n",
    "        spabs = SpaBS(n_band=n_components, sparsity_level=0.5)\n",
    "        # spabs = SpaBS(n_band=data.shape[-1], sparsity_level=0.5)\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = spabs.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data = spabs.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "        print(f'SpaBS prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by SpaBS: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'spec':\n",
    "        print('Using SPEC dimensionality reduction on data...')\n",
    "        spec = SPEC_HSI(n_band=n_components)\n",
    "        # spec = SPEC_HSI(n_band=data.shape[-1])\n",
    "        predict_start = time.time()\n",
    "        # bands_selected = spec.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        reduced_data = spec.predict(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        predict_end = time.time()\n",
    "        predict_runtime = datetime.timedelta(seconds=(predict_end - predict_start))\n",
    "        print(f'SPEC prediction completed! Prediction runtime: {predict_runtime}')\n",
    "        print(f'Bands selected by SPEC: {bands_selected}')\n",
    "        # data = data[...,bands_selected]\n",
    "        data = np.reshape(reduced_data, (orig_rows, orig_cols, reduced_data.shape[-1]))\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'ssr' or band_reduction_method == 'ssr-close':\n",
    "        print('Using SSR (closed form solution) dimensionality reduction on data...')\n",
    "        ssc = SSC_BS(n_hidden=2, n_clusters=n_components, lambda_coef=1)\n",
    "        fit_start = time.time()\n",
    "        data = ssc.fit_predict_close(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        fit_end = time.time()\n",
    "        fit_runtime = datetime.timedelta(seconds=(fit_end - fit_start))\n",
    "        print(f'SSR fitting completed! Fit runtime: {fit_runtime}')\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'ssr-cvx':\n",
    "        print('Using SSR (self-expressive representation) dimensionality reduction on data...')\n",
    "        ssc = SSC_BS(n_hidden=2, n_clusters=n_components, lambda_coef=1)\n",
    "        fit_start = time.time()\n",
    "        data = ssc.fit_predict_cvx(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        fit_end = time.time()\n",
    "        fit_runtime = datetime.timedelta(seconds=(fit_end - fit_start))\n",
    "        print(f'SSR fitting completed! Fit runtime: {fit_runtime}')\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'ssr-omp':\n",
    "        print('Using SSR (orthogonal matching pursuit) dimensionality reduction on data...')\n",
    "        ssc = SSC_BS(n_hidden=2, n_clusters=n_components, lambda_coef=1)\n",
    "        fit_start = time.time()\n",
    "        data = ssc.fit_predict_omp(data.reshape(data.shape[0]*data.shape[1], -1))\n",
    "        fit_end = time.time()\n",
    "        fit_runtime = datetime.timedelta(seconds=(fit_end - fit_start))\n",
    "        print(f'SSR fitting completed! Fit runtime: {fit_runtime}')\n",
    "        print(f'New data shape: {data.shape}')\n",
    "\n",
    "    elif band_reduction_method == 'manual':\n",
    "        print('Using manually selected bands...')\n",
    "        if hyperparams['selected_bands'] is not None and type (hyperparams['selected_bands']) is list:\n",
    "            # Sort bands\n",
    "            bands_selected = sorted(hyperparams['selected_bands'])\n",
    "            fit_start = time.time()\n",
    "            data = data[...,np.array(bands_selected, dtype=int)]\n",
    "            fit_end = time.time()\n",
    "            fit_runtime = datetime.timedelta(seconds=(fit_end - fit_start))\n",
    "            print(f'Manual fitting completed! Fit runtime: {fit_runtime}')\n",
    "            print(f'Bands selected by manual selection: {bands_selected}')\n",
    "            print(f'New data shape: {data.shape}')\n",
    "        else:\n",
    "            print('Selected bands list for manual selection is invalid! Dimensionality will be unaltered...')\n",
    "    else:\n",
    "        print('No valid band selection method chosen! Dimensionality will be unaltered...')\n",
    "\n",
    "    band_selection_end = time.time()\n",
    "    band_selection_time = datetime.timedelta(seconds=(band_selection_end - band_selection_start))\n",
    "\n",
    "    return data, band_selection_time, bands_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHjv8dDLlW_t"
   },
   "source": [
    "# 8) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FpFhE_KlW_t"
   },
   "source": [
    "## 8.1) Common Model Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_60Bw2tulW_t"
   },
   "outputs": [],
   "source": [
    "#@title Get Optimizer\n",
    "def get_optimizer(**hyperparams):\n",
    "    \"\"\"\n",
    "    Returns appropriately constructed optimizer from hyperparameter\n",
    "    inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    **hyperparams : dict\n",
    "        dictionary of hyperparameter values to use to construct the\n",
    "        optimizer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    optimizer : tensorflow.keras.optimizer.Optimizer\n",
    "        A keras Optimizer object constructed to hyperparam specification\n",
    "    \"\"\"\n",
    "\n",
    "    # Get requisite hyperparameter values\n",
    "    optimizer_name = hyperparams['optimizer']\n",
    "    learning_rate = hyperparams['lr']\n",
    "    momentum = hyperparams['momentum']\n",
    "    epsilon = hyperparams['epsilon']\n",
    "    initial_accumulator_value = hyperparams['initial_accumulator_value']\n",
    "    beta_1 = hyperparams['beta_1']\n",
    "    beta_2 = hyperparams['beta_2']\n",
    "    amsgrad = hyperparams['amsgrad']\n",
    "    rho = hyperparams['rho']\n",
    "    centered = hyperparams['centered']\n",
    "    nesterov = hyperparams['nesterov']\n",
    "    learning_rate_power = hyperparams['learning_rate_power']\n",
    "    l1_regularization_strength = hyperparams['l1_regularization_strength']\n",
    "    l2_regularization_strength = hyperparams['l2_regularization_strength']\n",
    "    l2_shrinkage_regularization_strength = hyperparams['l2_shrinkage_regularization_strength']\n",
    "    beta = hyperparams['beta']\n",
    "\n",
    "    # Set up the optimizers according to the input hyperparameters\n",
    "    if optimizer_name == 'adadelta':\n",
    "        if learning_rate is None: learning_rate = 0.001\n",
    "        if rho is None: rho = 0.95\n",
    "        if epsilon is None: epsilon = 1e-7\n",
    "        optimizer = Adadelta(learning_rate=learning_rate,\n",
    "                             rho=rho,\n",
    "                             epsilon=epsilon)\n",
    "    elif optimizer_name == 'adagrad':\n",
    "        if learning_rate is None: learning_rate = 0.001\n",
    "        if initial_accumulator_value is None: initial_accumulator_value = 0.1\n",
    "        if epsilon is None: epsilon = 1e-7\n",
    "        optimizer = Adagrad(learning_rate=learning_rate,\n",
    "                            initial_accumulator_value=initial_accumulator_value,\n",
    "                            epsilon=epsilon)\n",
    "    elif optimizer_name == 'adam':\n",
    "        if learning_rate is not None: learning_rate = 0.001\n",
    "        if beta_1 is None: beta_1 = 0.9\n",
    "        if beta_2 is None: beta_2 = 0.999\n",
    "        if epsilon is None: epsilon = 1e-7\n",
    "        if amsgrad is None: amsgrad = False\n",
    "        optimizer = Adam(learning_rate=learning_rate,\n",
    "                         beta_1=beta_1,\n",
    "                         beta_2=beta_2,\n",
    "                         epsilon=epsilon,\n",
    "                         amsgrad=amsgrad)\n",
    "    elif optimizer_name == 'adamax':\n",
    "        if learning_rate is not None: learning_rate = 0.001\n",
    "        if beta_1 is None: beta_1 = 0.9\n",
    "        if beta_2 is None: beta_2 = 0.999\n",
    "        if epsilon is None: epsilon = 1e-7\n",
    "        optimizer = Adamax(learning_rate=learning_rate,\n",
    "                           beta_1=beta_1,\n",
    "                           beta_2=beta_2,\n",
    "                           epsilon=epsilon)\n",
    "    elif optimizer_name == 'ftrl':\n",
    "        if learning_rate is not None: learning_rate = 0.001\n",
    "        if learning_rate_power is None: learning_rate_power = -0.5\n",
    "        if initial_accumulator_value is None: initial_accumulator_value = 0.1\n",
    "        if l1_regularization_strength is None: l1_regularization_strength = 0.0\n",
    "        if l2_regularization_strength is None: l2_regularization_strength = 0.0\n",
    "        if l2_shrinkage_regularization_strength is None: l2_shrinkage_regularization_strength = 0.0\n",
    "        if beta is None: beta = 0.0\n",
    "        optimizer = Ftrl(learning_rate=learning_rate,\n",
    "                         learning_rate_power=learning_rate_power,\n",
    "                         initial_accumulator_value=initial_accumulator_value,\n",
    "                         l1_regularization_strength=l1_regularization_strength,\n",
    "                         l2_regularization_strength=l2_regularization_strength,\n",
    "                         l2_shrinkage_regularization_strength=l2_shrinkage_regularization_strength,\n",
    "                         beta=beta)\n",
    "\n",
    "    elif optimizer_name == 'nadam':\n",
    "        if learning_rate is not None: learning_rate = 0.001\n",
    "        if beta_1 is None: beta_1 = 0.9\n",
    "        if beta_2 is None: beta_2 = 0.999\n",
    "        if epsilon is None: epsilon = 1e-7\n",
    "        optimizer = Nadam(learning_rate=learning_rate,\n",
    "                          beta_1=beta_1,\n",
    "                          beta_2=beta_2,\n",
    "                          epsilon=epsilon)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        if learning_rate is not None: learning_rate = 0.001\n",
    "        if rho is None: rho = 0.9\n",
    "        if momentum is None: momentum = 0.0\n",
    "        if epsilon is None: epsilon = 1e-7\n",
    "        if centered is None: centered = False\n",
    "        optimizer = RMSprop(learning_rate=learning_rate,\n",
    "                            rho=rho,\n",
    "                            momentum=momentum,\n",
    "                            epsilon=epsilon,\n",
    "                            centered=centered)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        if learning_rate is not None: learning_rate = 0.001\n",
    "        if momentum is None: momentum = 0.0\n",
    "        if nesterov is None: nesterov = False\n",
    "        optimizer = SGD(learning_rate=learning_rate,\n",
    "                        momentum=momentum,\n",
    "                        nesterov=nesterov)\n",
    "    else:\n",
    "        # This is the default value for the Tensorflow keras compile\n",
    "        # function optimizer argument\n",
    "        optimizer = 'rmsprop'\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mxTYslzilW_t"
   },
   "outputs": [],
   "source": [
    "#@title Handle Dimension Ordering\n",
    "def _handle_dim_ordering():\n",
    "    global CONV_DIM1\n",
    "    global CONV_DIM2\n",
    "    global CONV_DIM3\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        CONV_DIM1 = 1\n",
    "        CONV_DIM2 = 2\n",
    "        CONV_DIM3 = 3\n",
    "        CHANNEL_AXIS = 4\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        CONV_DIM1 = 2\n",
    "        CONV_DIM2 = 3\n",
    "        CONV_DIM3 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GKhhyHylW_t"
   },
   "source": [
    "## 8.2) Model Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qMsnze1qlW_t"
   },
   "outputs": [],
   "source": [
    "#@title 2D Convolutional Block\n",
    "def conv_block_2d(x, growth_rate, name, activation='relu'):\n",
    "    \"\"\"A building block for a dense block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                            name=name + '_0_bn')(x)\n",
    "    x1 = Activation(activation, name=name + f'_0_{activation}')(x1)\n",
    "    x1 = Conv2D(4 * growth_rate, 1, use_bias=False,\n",
    "                name=name + '_1_conv', padding='same')(x1)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                            name=name + '_1_bn')(x1)\n",
    "    x1 = Activation(activation, name=name + f'_1_{activation}')(x1)\n",
    "    x1 = Conv2D(growth_rate, 3, padding='same', use_bias=False,\n",
    "                name=name + '_2_conv')(x1)\n",
    "    x = Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6XAxC-yWlW_u"
   },
   "outputs": [],
   "source": [
    "#@title 3D Convolutional Block\n",
    "def conv_block_3d(x, growth_rate, name, activation='relu'):\n",
    "    \"\"\"A building block for a dense block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 4 if K.image_data_format() == 'channels_last' else 1\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                            name=name + '_0_bn')(x)\n",
    "    x1 = Activation(activation, name=name + f'_0_{activation}')(x1)\n",
    "    x1 = Conv3D(4 * growth_rate, 1, use_bias=False,\n",
    "                name=name + '_1_conv', padding='same')(x1)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                            name=name + '_1_bn')(x1)\n",
    "    x1 = Activation(activation, name=name + f'_1_{activation}')(x1)\n",
    "    x1 = Conv3D(growth_rate, 3, padding='same', use_bias=False,\n",
    "                name=name + '_2_conv')(x1)\n",
    "    x = Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8im9-cUvlW_u"
   },
   "outputs": [],
   "source": [
    "#@title 2D Transition Block\n",
    "def transition_block_2d(x, reduction, name, activation='relu'):\n",
    "    \"\"\"A transition block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        reduction: float, compression rate at transition layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                           name=name + '_bn')(x)\n",
    "    x = Activation(activation, name=name + f'_{activation}')(x)\n",
    "    x = Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1, use_bias=False,\n",
    "               name=name + '_conv', padding='same')(x)\n",
    "    x = AveragePooling2D(1, strides=(2, 2), name=name + '_pool', padding='same')(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nn6wAi9glW_u"
   },
   "outputs": [],
   "source": [
    "#@title 3D Transition Block\n",
    "def transition_block_3d(x, reduction, name, activation='relu'):\n",
    "    \"\"\"A transition block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        reduction: float, compression rate at transition layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 4 if K.image_data_format() == 'channels_last' else 1\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                           name=name + '_bn')(x)\n",
    "    x = Activation(activation, name=name + f'_{activation}')(x)\n",
    "    x = Conv3D(int(K.int_shape(x)[bn_axis] * reduction), 1, use_bias=False,\n",
    "               name=name + '_conv', padding='same')(x)\n",
    "    x = AveragePooling3D(1, strides=(2, 2, 2), name=name + '_pool', padding='same')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RbyTGhVMlW_u"
   },
   "outputs": [],
   "source": [
    "#@title 2D Dense Block\n",
    "def dense_block_2d(x, blocks, name, growth_rate=32, activation='relu'):\n",
    "    \"\"\"A dense block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block_2d(x, growth_rate, activation=activation, name=name + '_block' + str(i + 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8hwIxmMRlW_u"
   },
   "outputs": [],
   "source": [
    "#@title 3D Dense Block\n",
    "def dense_block_3d(x, blocks, name, growth_rate=32, activation='relu'):\n",
    "    \"\"\"A dense block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block_3d(x, growth_rate, activation=activation, name=name + '_block' + str(i + 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XjOgUwCLlW_u"
   },
   "outputs": [],
   "source": [
    "#@title Fusion Fully Convolutional Network (FCN) Convolution Block\n",
    "def fusion_fcn_conv_block(x, branch_num, block_num, growth_rate=64, activation='relu'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if x.shape[0] == 3:\n",
    "        height = x.shape[0]\n",
    "        width = x.shape[1]\n",
    "    else:\n",
    "        height = x.shape[1]\n",
    "        width = x.shape[2]\n",
    "\n",
    "    x = Conv2D(growth_rate, kernel_size=(3,3), strides=(1,1), padding='same',\n",
    "                            name=f'Branch_{branch_num}_Conv2D_{block_num}')(x)\n",
    "    x = Activation(activation, name=f'Branch_{branch_num}_{activation}_{block_num}')(x)\n",
    "    x = AveragePooling2D(pool_size=(2,2), padding='same',\n",
    "                            name=f'Branch_{branch_num}_AveragePool2D_{block_num}')(x)\n",
    "    x = Resizing(height, width, interpolation='nearest',\n",
    "                            name=f'Branch_{branch_num}_Resizing_{block_num}')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MDChEnGLlW_v"
   },
   "outputs": [],
   "source": [
    "#@title Network-in-Network (NiN) Block\n",
    "def nin_block(x, filters, kernel_size, block_num, strides=(1,1), num_mlp_layers=2, activation='relu'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    for layer in range(num_mlp_layers):\n",
    "        x = Conv2D(x.shape[-1], kernel_size=(1,1), strides=(1,1),\n",
    "                   name=f'MLPConv_{block_num}_layer_{layer}',\n",
    "                   activation=activation, padding='valid',\n",
    "                   kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=kernel_size, strides=strides,\n",
    "               name=f'Conv_{block_num}', activation=activation, padding='same',\n",
    "               kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu_-QIf8lW_v"
   },
   "source": [
    "## 8.3) Densenet Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ncl_i68zlW_v"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 3D Builder Class\n",
    "class Densenet3DBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        print('original input shape:', input_shape)\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 4:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, kernel_dim1, kernel_dim2, kernel_dim3)\")\n",
    "\n",
    "        print('original input shape:', input_shape)\n",
    "        # orignal input shape: 1,7,7,200\n",
    "\n",
    "        print(f'Image data format: {K.image_data_format()}')\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[3], input_shape[0])\n",
    "        print('change input shape:', input_shape)\n",
    "\n",
    "        # \n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        # 3D Convolution and pooling\n",
    "        conv1 = Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='SAME', kernel_initializer='he_normal')(\n",
    "            input)\n",
    "        pool1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same')(conv1)\n",
    "\n",
    "        # Dense Block1\n",
    "        # x = dense_block_3d(pool1, 6, name='conv1')\n",
    "        # x = transition_block_3d(x, 0.5, name='pool1')\n",
    "        # x = dense_block_3d(x, 6, name='conv2')\n",
    "        # x = transition_block_3d(x, 0.5, name='pool2')\n",
    "        # x = dense_block_3d(x, 6, name='conv3')\n",
    "        x = dense_block_3d(pool1, 3, name='conv1')\n",
    "        x = transition_block_3d(x, 0.5, name='pool1')\n",
    "        x = dense_block_3d(x, 3, name='conv2')\n",
    "        x = transition_block_3d(x, 0.5, name='pool2')\n",
    "        x = dense_block_3d(x, 3, name='conv3')\n",
    "        print(x.shape)\n",
    "        x = GlobalAveragePooling3D(name='avg_pool')(x)\n",
    "        print(x.shape)\n",
    "        # x = Dense(16, activation='softmax')(x)\n",
    "\n",
    "        # \n",
    "        # Classifier block\n",
    "        dense = Dense(units=num_outputs, activation=\"softmax\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense, name='3D-DenseNet')\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_8(input_shape, num_outputs):\n",
    "        # (1,7,7,200),16\n",
    "        return Densenet3DBuilder.build(input_shape, num_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_dYqOLQIlW_v"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 3D Model Assignment\n",
    "def densenet_3d_model(img_rows, img_cols, img_channels, nb_classes):\n",
    "\n",
    "    model = Densenet3DBuilder.build_resnet_8(\n",
    "        (1, img_rows, img_cols, img_channels), nb_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmLPbxtglW_v"
   },
   "source": [
    "## 8.4) 3D CNN Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ElmhvYgnlW_v"
   },
   "outputs": [],
   "source": [
    "#@title CNN 3D Builder Class\n",
    "class CNN3DBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        print('original input shape:', input_shape)\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 4:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, kernel_dim1, kernel_dim2, kernel_dim3)\")\n",
    "\n",
    "        print('original input shape:', input_shape)\n",
    "        # orignal input shape: 1,7,7,200\n",
    "\n",
    "        print(f'Image data format: {K.image_data_format()}')\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[3], input_shape[0])\n",
    "        print('change input shape:', input_shape)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        conv1 = Conv3D(filters=128, kernel_size=(3, 3, 20), strides=(1, 1, 5), padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(0.01))(input)\n",
    "        act1 = Activation('relu')(conv1)\n",
    "        pool1 = MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same')(act1)\n",
    "\n",
    "        conv2 = Conv3D(filters=192, kernel_size=(2, 2, 3), strides=(1, 1, 2), padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(0.01))(pool1)\n",
    "        act2 = Activation('relu')(conv2)\n",
    "        drop1 = Dropout(0.5)(act2)\n",
    "        pool2 = MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same')(drop1)\n",
    "\n",
    "        conv3 = Conv3D(filters=256, kernel_size=(3, 3, 3), strides=(1, 1, 2), padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(0.01))(pool2)\n",
    "        act3 = Activation('relu')(conv3)\n",
    "        drop2 = Dropout(0.5)(act3)\n",
    "\n",
    "        flatten1 = Flatten()(drop2)\n",
    "        fc1 = Dense(200, kernel_regularizer=regularizers.l2(0.01))(flatten1)\n",
    "        act3 = Activation('relu')(fc1)\n",
    "\n",
    "        # conv1 = Conv3D(filters=32, kernel_size=(3, 3, 20), strides=(1, 1, 5), padding='same',\n",
    "        #                kernel_regularizer=regularizers.l2(0.01))(input)\n",
    "        # act1 = Activation('relu')(conv1)\n",
    "        # pool1 = MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same')(act1)\n",
    "\n",
    "        # conv2 = Conv3D(filters=64, kernel_size=(2, 2, 3), strides=(1, 1, 2), padding='same',\n",
    "        #                kernel_regularizer=regularizers.l2(0.01))(pool1)\n",
    "        # act2 = Activation('relu')(conv2)\n",
    "        # drop1 = Dropout(0.5)(act2)\n",
    "        # pool2 = MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1), padding='same')(drop1)\n",
    "\n",
    "        # conv3 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(1, 1, 2), padding='same',\n",
    "        #                kernel_regularizer=regularizers.l2(0.01))(pool2)\n",
    "        # act3 = Activation('relu')(conv3)\n",
    "        # drop2 = Dropout(0.5)(act3)\n",
    "\n",
    "        # flatten1 = Flatten()(drop2)\n",
    "        # fc1 = Dense(num_outputs*2, kernel_regularizer=regularizers.l2(0.01))(flatten1)\n",
    "        # act3 = Activation('relu')(fc1)\n",
    "\n",
    "\n",
    "        # Classifier block\n",
    "        dense = Dense(units=num_outputs, activation=\"softmax\", kernel_initializer=\"he_normal\")(act3)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense, name='3D-CNN')\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_8(input_shape, num_outputs):\n",
    "        # (1,7,7,200),16\n",
    "        return CNN3DBuilder.build(input_shape, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7KenfapylW_w"
   },
   "outputs": [],
   "source": [
    "#@title CNN 3D Model Assignment\n",
    "def cnn_3d_model(img_rows, img_cols, img_channels, nb_classes):\n",
    "\n",
    "    model = CNN3DBuilder.build_resnet_8(\n",
    "        (1, img_rows, img_cols, img_channels), nb_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg8wrwhElW_w"
   },
   "source": [
    "## 8.5) 2D CNN Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MGNRjJfklW_w"
   },
   "outputs": [],
   "source": [
    "#@title CNN 2D Builder Class\n",
    "class CNN2DBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        print('original input shape:', input_shape)\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, kernel_dim1, kernel_dim2)\")\n",
    "\n",
    "        print('original input shape:', input_shape)\n",
    "\n",
    "        print(f'Image data format: {K.image_data_format()}')\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "        print('change input shape:', input_shape)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        conv1 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(0.01))(input)\n",
    "        act1 = Activation('relu')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(act1)\n",
    "\n",
    "        conv2 = Conv2D(filters=192, kernel_size=(2, 2), strides=(1, 1), padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(0.01))(pool1)\n",
    "        act2 = Activation('relu')(conv2)\n",
    "        drop1 = Dropout(0.5)(act2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(drop1)\n",
    "\n",
    "        conv3 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                       kernel_regularizer=regularizers.l2(0.01))(pool2)\n",
    "        act3 = Activation('relu')(conv3)\n",
    "        drop2 = Dropout(0.5)(act3)\n",
    "\n",
    "        flatten1 = Flatten()(drop2)\n",
    "        fc1 = Dense(200, kernel_regularizer=regularizers.l2(0.01))(flatten1)\n",
    "        act3 = Activation('relu')(fc1)\n",
    "\n",
    "        # Classifier block\n",
    "        dense = Dense(units=num_outputs, activation=\"softmax\", kernel_initializer=\"he_normal\")(act3)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense, name='2D-CNN')\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_8(input_shape, num_outputs):\n",
    "        return CNN2DBuilder.build(input_shape, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FaTkphgflW_w"
   },
   "outputs": [],
   "source": [
    "#@title CNN 2D Model Assignment\n",
    "def cnn_2d_model(img_rows, img_cols, img_channels, nb_classes):\n",
    "\n",
    "    model = CNN2DBuilder.build_resnet_8(\n",
    "        (img_channels, img_rows, img_cols), nb_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJctrIFhlW_w"
   },
   "source": [
    "## 8.6) Baseline CNN Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VKDEkXZ8lW_x"
   },
   "outputs": [],
   "source": [
    "#@title Baseline CNN Model Builder and Assignment\n",
    "def baseline_cnn_model(img_rows, img_cols, img_channels,\n",
    "                       patch_size, nb_filters, nb_classes):\n",
    "    \"\"\"\n",
    "    Generates baseline CNN model for classifying HSI dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_rows : int\n",
    "        Number of rows in neighborhood patch.\n",
    "    img_cols : int\n",
    "        Number of columns in neighborhood patch.\n",
    "    img_channels : int\n",
    "        Number of spectral bands.\n",
    "    nb_classes : int\n",
    "        Number of label categories.\n",
    "    lr : float\n",
    "        Learning rate for the model\n",
    "    momentum : float\n",
    "        Momentum value for optimizer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Model\n",
    "        A keras API model of the constructed ML network.\n",
    "    \"\"\"\n",
    "\n",
    "    model_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "    conv_layer = Conv2D(nb_filters, (patch_size, patch_size),\n",
    "                        strides=(1, 1),name='2d_convolution_layer', padding='same',\n",
    "                        kernel_regularizer=regularizers.l2(0.01))(model_input)\n",
    "    activation_layer = Activation('relu', name='activation_layer')(conv_layer)\n",
    "    max_pool_layer = MaxPooling2D(pool_size=(2, 2), name='2d_max_pooling_layer', padding='same')(activation_layer)\n",
    "    flatten_layer = Flatten(name='flatten_layer')(max_pool_layer)\n",
    "    dense_layer = Dense(units=nb_classes, name='dense_layer')(flatten_layer)\n",
    "    classifier_layer = Activation('softmax', name='classifier_layer')(dense_layer)\n",
    "\n",
    "    model = Model(model_input, classifier_layer, name='baseline_cnn_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3NSAp3mlW_x"
   },
   "source": [
    "## 8.7) Fusion Fully Connected Network (FCN) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XwZdnGRqlW_x"
   },
   "outputs": [],
   "source": [
    "#@title Fusion FCN Model Builder and Assignment\n",
    "def fusion_fcn_model(branch_1_shape, branch_2_shape, branch_3_shape, nb_classes):\n",
    "\n",
    "    # Initialize Inputs\n",
    "    branch_1_input = Input(shape=branch_1_shape)\n",
    "    branch_2_input = Input(shape=branch_2_shape)\n",
    "    branch_3_input = Input(shape=branch_3_shape)\n",
    "\n",
    "    print(f'Branch 1 shape: {branch_1_input.shape}')\n",
    "    print(f'Branch 2 shape: {branch_2_input.shape}')\n",
    "    print(f'Branch 3 shape: {branch_3_input.shape}')\n",
    "\n",
    "    # Set channel axis\n",
    "    channel_axis = len(branch_1_input.shape) - 1 if K.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    # First branch\n",
    "    branch_1_a = fusion_fcn_conv_block(branch_1_input, 1, 1)\n",
    "    branch_1_b = fusion_fcn_conv_block(branch_1_a, 1, 2)\n",
    "    branch_1_c = fusion_fcn_conv_block(branch_1_b, 1, 3)\n",
    "\n",
    "    branch_1 = Add(name='Branch_1_Add')([branch_1_a, branch_1_b, branch_1_c])\n",
    "\n",
    "    # Second branch\n",
    "    branch_2_a = fusion_fcn_conv_block(branch_2_input, 2, 1)\n",
    "    branch_2_b = fusion_fcn_conv_block(branch_2_a, 2, 2)\n",
    "    branch_2_c = fusion_fcn_conv_block(branch_2_b, 2, 3)\n",
    "\n",
    "    branch_2 = Add(name='Branch_2_Add')([branch_2_a, branch_2_b, branch_2_c])\n",
    "\n",
    "    # Third branch\n",
    "    branch_3 = branch_3_input\n",
    "\n",
    "    # Branch fusion\n",
    "    fusion = Concatenate(axis=channel_axis, name='Fusion_Concatenate')([branch_1, branch_2, branch_3])\n",
    "    fusion = Conv2D(nb_classes, (1,1), strides=(1,1), padding='same',\n",
    "                        name='Fusion_Conv2D')(fusion)\n",
    "    fusion = Activation('relu', name='Fusion_ReLU')(fusion)\n",
    "    out = Activation('softmax', name='Fusion_Softmax')(fusion)\n",
    "\n",
    "    model = Model(inputs=[branch_1_input, branch_2_input, branch_3_input],\n",
    "                  outputs=out,\n",
    "                  name='Fusion-FCN')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XFQRAJBdlW_x"
   },
   "outputs": [],
   "source": [
    "#@title Fusion FCN Model (Version 2) Model Builder and Assignment\n",
    "def fusion_fcn_v2_model(branch_1_shape, branch_2_shape, branch_3_shape, nb_classes):\n",
    "\n",
    "    # Initialize Inputs\n",
    "    branch_1_input = Input(shape=branch_1_shape)\n",
    "    branch_2_input = Input(shape=branch_2_shape)\n",
    "    branch_3_input = Input(shape=branch_3_shape)\n",
    "\n",
    "    print(f'Branch 1 shape: {branch_1_input.shape}')\n",
    "    print(f'Branch 2 shape: {branch_2_input.shape}')\n",
    "    print(f'Branch 3 shape: {branch_3_input.shape}')\n",
    "\n",
    "    # Set channel axis\n",
    "    channel_axis = len(branch_1_input.shape) - 1 if K.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    # First branch\n",
    "    branch_1_a = fusion_fcn_conv_block(branch_1_input, 1, 1)\n",
    "    branch_1_b = fusion_fcn_conv_block(branch_1_a, 1, 2)\n",
    "    branch_1_c = fusion_fcn_conv_block(branch_1_b, 1, 3)\n",
    "\n",
    "    branch_1 = Add(name='Branch_1_Add')([branch_1_a, branch_1_b, branch_1_c])\n",
    "\n",
    "    # Second branch\n",
    "    branch_2_a = fusion_fcn_conv_block(branch_2_input, 2, 1)\n",
    "    branch_2_b = fusion_fcn_conv_block(branch_2_a, 2, 2)\n",
    "    branch_2_c = fusion_fcn_conv_block(branch_2_b, 2, 3)\n",
    "\n",
    "    branch_2 = Add(name='Branch_2_Add')([branch_2_a, branch_2_b, branch_2_c])\n",
    "\n",
    "    # Third branch\n",
    "    branch_3_a = fusion_fcn_conv_block(branch_3_input, 3, 1)\n",
    "    branch_3_b = fusion_fcn_conv_block(branch_3_a, 3, 2)\n",
    "    branch_3_c = fusion_fcn_conv_block(branch_3_b, 3, 3)\n",
    "\n",
    "    branch_3 = Add(name='Branch_3_Add')([branch_3_a, branch_3_b, branch_3_c])\n",
    "\n",
    "    # Branch fusion\n",
    "    fusion = Concatenate(axis=channel_axis, name='Fusion_Concatenate')([branch_1, branch_2, branch_3])\n",
    "    fusion = Conv2D(fusion.shape[channel_axis], (1,1), strides=(1,1), padding='same',\n",
    "                        name='Fusion_Conv2D')(fusion)\n",
    "    fusion = Activation('relu', name='Fusion_ReLU')(fusion)\n",
    "    fusion = Flatten(name='Fusion_Flatten')(fusion)\n",
    "    fusion = Dense(units=nb_classes, name='Fusion_Dense')(fusion)\n",
    "    out = Activation('softmax', name='Fusion_Softmax')(fusion)\n",
    "\n",
    "    model = Model(inputs=[branch_1_input, branch_2_input, branch_3_input],\n",
    "                  outputs=out,\n",
    "                  name='Fusion-FCN-V2')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU-CyUmClW_x"
   },
   "source": [
    "## 8.8) 2D Densenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fAkHbWjQlW_x"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 2D Builder Class\n",
    "class Densenet2DBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        print('original input shape:', input_shape)\n",
    "        _handle_dim_ordering()\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, kernel_dim1, kernel_dim2)\")\n",
    "\n",
    "        print('original input shape:', input_shape)\n",
    "\n",
    "        print(f'Image data format: {K.image_data_format()}')\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
    "        print('change input shape:', input_shape)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        # 2D Convolution and pooling\n",
    "        conv1 = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='SAME', kernel_initializer='he_normal')(\n",
    "            input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv1)\n",
    "\n",
    "        # Dense Block1\n",
    "        x = dense_block_2d(pool1, 6, name='conv1')\n",
    "        x = transition_block_2d(x, 0.5, name='pool1')\n",
    "        x = dense_block_2d(x, 6, name='conv2')\n",
    "        x = transition_block_2d(x, 0.5, name='pool2')\n",
    "        x = dense_block_2d(x, 6, name='conv3')\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "        # Classifier block\n",
    "        dense = Dense(units=num_outputs, activation=\"softmax\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense, name='2D-DenseNet')\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_8(input_shape, num_outputs):\n",
    "        return Densenet2DBuilder.build(input_shape, num_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XIsaD2wolW_y"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 2D Model Assignment\n",
    "def densenet_2d_model(img_rows, img_cols, img_channels, nb_classes):\n",
    "\n",
    "    model = Densenet2DBuilder.build_resnet_8(\n",
    "        (img_channels, img_rows, img_cols), nb_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDyXsHihlW_y"
   },
   "source": [
    "## 8.9) Network-in-Network (NiN) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GlkH1oRQlW_y"
   },
   "outputs": [],
   "source": [
    "#@title Network-in-Network Model Builder and Assignment\n",
    "def nin_model(img_rows, img_cols, img_channels, num_classes, num_mlp_layers=2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model_input = Input(shape=(img_rows, img_cols, img_channels))\n",
    "\n",
    "    # Convolution block 1\n",
    "    x = nin_block(model_input, img_channels, (5,5), 1, num_mlp_layers=num_mlp_layers)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same',\n",
    "                     name='Spatial_Pooling_1')(x)\n",
    "\n",
    "    # Convolution block 2\n",
    "    x = nin_block(x, img_channels, (3,3), 2, num_mlp_layers=num_mlp_layers)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same',\n",
    "                     name='Spatial_Pooling_1')(x)\n",
    "\n",
    "    # Convolution block 3\n",
    "    x = nin_block(x, num_classes, (3,3), 3, num_mlp_layers=num_mlp_layers)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same',\n",
    "                     name='Spatial_Pooling_1')(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D(name='Global_Average_Pooling')(x)\n",
    "    x = Activation('softmax', name='Softmax_Classification')(x)\n",
    "\n",
    "    # Model creation\n",
    "    model = Model(model_input, x, name='nin_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "b1CKnuDXlW_y"
   },
   "outputs": [],
   "source": [
    "#@title NiN Band Selection Model Builder and Assignment\n",
    "def nin_band_selection_model(nb_channels, nb_classes, nb_layers=2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model_input = Input(shape=(1, 1, nb_channels))\n",
    "    x = model_input\n",
    "\n",
    "    for layer in range(nb_layers):\n",
    "        x = Conv2D(nb_channels, kernel_size=(1,1), strides=(1,1), name=f'mlp_conv_{layer}',\n",
    "               padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D(name='global_average_pooling')(x)\n",
    "    x = Activation('softmax', name='softmax_classification')(x)\n",
    "\n",
    "\n",
    "    model = Model(model_input, x, name='nin_band_selection_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Saat71NblW_y"
   },
   "source": [
    "## 8.10) Band Selection 3D Densenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sHD6PokBlW_y"
   },
   "outputs": [],
   "source": [
    "#@title Band Selection Densenet 3D Builder Class\n",
    "class BSDensenet3DBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        print('original input shape:', input_shape)\n",
    "        if len(input_shape) != 4:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, kernel_dim1, kernel_dim2, kernel_dim3)\")\n",
    "\n",
    "        print('original input shape:', input_shape)\n",
    "        # orignal input shape: 1,7,7,200\n",
    "\n",
    "        print(f'Image data format: {K.image_data_format()}')\n",
    "        channels = input_shape[3]\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            input_shape = (input_shape[1], input_shape[2], input_shape[3], input_shape[0])\n",
    "        print('change input shape:', input_shape)\n",
    "\n",
    "        # Set input\n",
    "        input = Input(shape=input_shape)\n",
    "\n",
    "        # 3D Convolution and pooling\n",
    "        x = Conv2D(channels, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initialize='he_normal')(input)\n",
    "        x = Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='SAME', kernel_initializer='he_normal')(x)\n",
    "        x = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same')(x)\n",
    "\n",
    "        # Dense Block1\n",
    "        x = dense_block_3d(x, 6, name='conv1')\n",
    "        x = transition_block_3d(x, 0.5, name='pool1')\n",
    "        x = dense_block_3d(x, 6, name='conv2')\n",
    "        x = transition_block_3d(x, 0.5, name='pool2')\n",
    "        x = dense_block_3d(x, 6, name='conv3')\n",
    "        print(x.shape)\n",
    "        x = GlobalAveragePooling3D(name='avg_pool')(x)\n",
    "        print(x.shape)\n",
    "        # x = Dense(16, activation='softmax')(x)\n",
    "\n",
    "        # \n",
    "        # Classifier block\n",
    "        output = Dense(units=num_outputs, activation=\"softmax\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "        model = Model(inputs=input, outputs=output, name='Band Section 3D-DenseNet')\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs):\n",
    "        return BSDensenet3DBuilder.build(input_shape, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hX64xaKQlW_z"
   },
   "outputs": [],
   "source": [
    "#@title Band Selection 3D Densenet Model Assignment\n",
    "def bs_3d_densenet_model(img_rows, img_cols, img_channels, nb_classes):\n",
    "\n",
    "    model = BSDensenet3DBuilder.build(\n",
    "        (1, img_rows, img_cols, img_channels), nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQIeBt6olW_z"
   },
   "source": [
    "## 8.11) Densenet 3D Fusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eum7M_EHlW_z"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 3D Fusion Model Builder and Assignment\n",
    "def densenet_3d_fusion_model(img_rows, img_cols, img_channels_list,\n",
    "                             nb_classes, num_dense_blocks=3):\n",
    "\n",
    "    # Note - normal num_dense_blocks is 6\n",
    "\n",
    "    branch_shapes = []\n",
    "\n",
    "    # Initialize shapes\n",
    "    for img_channels in img_channels_list:\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            branch_shapes.append((img_rows, img_cols, img_channels))\n",
    "        else:\n",
    "            branch_shapes.append((img_channels, img_rows, img_cols))\n",
    "\n",
    "    # Initialize inputs\n",
    "    branch_inputs = [Input(shape=shape) for shape in branch_shapes]\n",
    "\n",
    "    # Print input shapes\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        print(f'Branch {index+1} input shape: {branch_input.shape}')\n",
    "\n",
    "\n",
    "    # Set up branches\n",
    "    branches = []\n",
    "\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        num_channels = img_channels_list[index]\n",
    "        branch_num = index + 1\n",
    "        x = branch_input\n",
    "\n",
    "        # x = Conv2D(num_channels, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "        #             kernel_initializer='he_normal', name=f'Branch_{branch_num}_Conv1x1_1')(x)\n",
    "        # x = Conv2D(num_channels, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "        #             kernel_initializer='he_normal', name=f'Branch_{branch_num}_Conv1x1_2')(x)\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            x = Reshape((*branch_input.shape[1:], 1), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        else:\n",
    "            x = Reshape((1, *branch_input.shape[1:]), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        x = Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_3DConv')(x)\n",
    "        x = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same',\n",
    "                    name=f'Branch_{branch_num}_MaxPooling3D')(x)\n",
    "\n",
    "        # Dense Blocks\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv1')\n",
    "        x = transition_block_3d(x, 0.5, name=f'Branch_{branch_num}__pool1')\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv2')\n",
    "        x = transition_block_3d(x, 0.5, name=f'Branch_{branch_num}__pool2')\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv3')\n",
    "\n",
    "        x = GlobalAveragePooling3D(name=f'Branch_{branch_num}__avg_pool')(x)\n",
    "\n",
    "        branches.append(x)\n",
    "\n",
    "    # Print the output shape of the branches\n",
    "    for index, branch in enumerate(branches):\n",
    "        print(f'Branch {index+1} output shape: {branch.shape}')\n",
    "\n",
    "\n",
    "    # Branch fusion\n",
    "    fusion = Concatenate(axis=1, name='Fusion_Concatenate')(branches)\n",
    "    print(f'Shape after concatenation: {fusion.shape}')\n",
    "    fusion = Dense(units=fusion.shape[-1], kernel_initializer='he_normal', name='Fusion_Dense_1')(fusion)\n",
    "    fusion = Activation('relu', name='Fusion_ReLU')(fusion)\n",
    "    fusion = Dense(units=nb_classes, kernel_initializer='he_normal', name='Fusion_Dense_2')(fusion)\n",
    "    out = Activation('softmax', name='Fusion_Softmax')(fusion)\n",
    "\n",
    "    print(f'Fusion output shape: {out.shape}')\n",
    "\n",
    "    model = Model(inputs=branch_inputs,\n",
    "                  outputs=out,\n",
    "                  name='3D-Densenet-Fusion')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "aFHI_Y0hlW_z"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 3D Fusion Model (Version 2) Builder and Assignment\n",
    "def densenet_3d_fusion_model2(img_rows, img_cols, img_channels_list,\n",
    "                             nb_classes, num_dense_blocks=3):\n",
    "\n",
    "    # Note - normal num_dense_blocks is 6\n",
    "\n",
    "    branch_shapes = []\n",
    "\n",
    "    # Initialize shapes\n",
    "    for img_channels in img_channels_list:\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            branch_shapes.append((img_rows, img_cols, img_channels))\n",
    "        else:\n",
    "            branch_shapes.append((img_channels, img_rows, img_cols))\n",
    "\n",
    "    # Initialize inputs\n",
    "    branch_inputs = [Input(shape=shape) for shape in branch_shapes]\n",
    "\n",
    "    # Print input shapes\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        print(f'Branch {index+1} input shape: {branch_input.shape}')\n",
    "\n",
    "\n",
    "    # Set up branches\n",
    "    branches = []\n",
    "\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        num_channels = img_channels_list[index]\n",
    "        branch_num = index + 1\n",
    "        x = branch_input\n",
    "\n",
    "        x = Conv2D(num_channels, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_Conv1x1_1')(x)\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            x = Reshape((*branch_input.shape[1:], 1), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        else:\n",
    "            x = Reshape((1, *branch_input.shape[1:]), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        x = Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_3DConv')(x)\n",
    "        x = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same',\n",
    "                    name=f'Branch_{branch_num}_MaxPooling3D')(x)\n",
    "\n",
    "        # Dense Blocks\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv1')\n",
    "        x = transition_block_3d(x, 0.5, name=f'Branch_{branch_num}__pool1')\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv2')\n",
    "        x = transition_block_3d(x, 0.5, name=f'Branch_{branch_num}__pool2')\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv3')\n",
    "\n",
    "        x = GlobalAveragePooling3D(name=f'Branch_{branch_num}__avg_pool')(x)\n",
    "\n",
    "        branches.append(x)\n",
    "\n",
    "    # Print the output shape of the branches\n",
    "    for index, branch in enumerate(branches):\n",
    "        print(f'Branch {index+1} output shape: {branch.shape}')\n",
    "\n",
    "\n",
    "    # Branch fusion\n",
    "    fusion = Concatenate(axis=1, name='Fusion_Concatenate')(branches)\n",
    "    print(f'Shape after concatenation: {fusion.shape}')\n",
    "    fusion = Dense(units=fusion.shape[-1], kernel_initializer='he_normal', name='Fusion_Dense_1')(fusion)\n",
    "    fusion = Activation('relu', name='Fusion_ReLU')(fusion)\n",
    "    fusion = Dense(units=nb_classes, kernel_initializer='he_normal', name='Fusion_Dense_2')(fusion)\n",
    "    out = Activation('softmax', name='Fusion_Softmax')(fusion)\n",
    "\n",
    "    print(f'Fusion output shape: {out.shape}')\n",
    "\n",
    "    model = Model(inputs=branch_inputs,\n",
    "                  outputs=out,\n",
    "                  name='3D-Densenet-Fusion')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rRsbqqTzlW_z"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 3D Fusion Model (Version 3) Builder and Assignment\n",
    "def densenet_3d_fusion_model3(img_rows, img_cols, img_channels_list,\n",
    "                             nb_classes, num_dense_blocks=3):\n",
    "\n",
    "    # Note - normal num_dense_blocks is 6\n",
    "\n",
    "    branch_shapes = []\n",
    "\n",
    "    # Initialize shapes\n",
    "    for img_channels in img_channels_list:\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            branch_shapes.append((img_rows, img_cols, img_channels))\n",
    "        else:\n",
    "            branch_shapes.append((img_channels, img_rows, img_cols))\n",
    "\n",
    "    # Initialize inputs\n",
    "    branch_inputs = [Input(shape=shape) for shape in branch_shapes]\n",
    "\n",
    "    # Print input shapes\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        print(f'Branch {index+1} input shape: {branch_input.shape}')\n",
    "\n",
    "\n",
    "    # Set up branches\n",
    "    branches = []\n",
    "\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        num_channels = img_channels_list[index]\n",
    "        branch_num = index + 1\n",
    "        x = branch_input\n",
    "\n",
    "        x = Conv2D(num_channels, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_Conv1x1_1')(x)\n",
    "        x = Conv2D(num_channels, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_Conv1x1_2')(x)\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            x = Reshape((*branch_input.shape[1:], 1), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        else:\n",
    "            x = Reshape((1, *branch_input.shape[1:]), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        x = Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_3DConv')(x)\n",
    "        x = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same',\n",
    "                    name=f'Branch_{branch_num}_MaxPooling3D')(x)\n",
    "\n",
    "        # Dense Blocks\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv1')\n",
    "        x = transition_block_3d(x, 0.5, name=f'Branch_{branch_num}__pool1')\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv2')\n",
    "        x = transition_block_3d(x, 0.5, name=f'Branch_{branch_num}__pool2')\n",
    "        x = dense_block_3d(x, num_dense_blocks, name=f'Branch_{branch_num}__conv3')\n",
    "\n",
    "        x = GlobalAveragePooling3D(name=f'Branch_{branch_num}__avg_pool')(x)\n",
    "\n",
    "        branches.append(x)\n",
    "\n",
    "    # Print the output shape of the branches\n",
    "    for index, branch in enumerate(branches):\n",
    "        print(f'Branch {index+1} output shape: {branch.shape}')\n",
    "\n",
    "\n",
    "    # Branch fusion\n",
    "    fusion = Concatenate(axis=1, name='Fusion_Concatenate')(branches)\n",
    "    print(f'Shape after concatenation: {fusion.shape}')\n",
    "    fusion = Dense(units=fusion.shape[-1], kernel_initializer='he_normal', name='Fusion_Dense_1')(fusion)\n",
    "    fusion = Activation('relu', name='Fusion_ReLU')(fusion)\n",
    "    fusion = Dense(units=nb_classes, kernel_initializer='he_normal', name='Fusion_Dense_2')(fusion)\n",
    "    out = Activation('softmax', name='Fusion_Softmax')(fusion)\n",
    "\n",
    "    print(f'Fusion output shape: {out.shape}')\n",
    "\n",
    "    model = Model(inputs=branch_inputs,\n",
    "                  outputs=out,\n",
    "                  name='3D-Densenet-Fusion')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au7iKPxJlW_z"
   },
   "source": [
    "## 8.12) Densenet 3D Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3Zz6RRbslW_0"
   },
   "outputs": [],
   "source": [
    "#@title Densenet 3D Modified Model Builder And Assignment\n",
    "def densenet_3d_modified_model(img_rows, img_cols, img_channels_list, nb_classes,\n",
    "                               num_dense_blocks=3,\n",
    "                               growth_rate=32,\n",
    "                               num_1x1_convs=0,\n",
    "                               first_conv_filters=64,\n",
    "                               first_conv_kernel=(3,3,3),\n",
    "                               dropout_1=0.5,\n",
    "                               dropout_2=0.5,\n",
    "                               activation='relu'):\n",
    "\n",
    "    branch_shapes = []\n",
    "\n",
    "    # Initialize shapes\n",
    "    for img_channels in img_channels_list:\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            branch_shapes.append((img_rows, img_cols, img_channels))\n",
    "        else:\n",
    "            branch_shapes.append((img_channels, img_rows, img_cols))\n",
    "\n",
    "    # Initialize inputs\n",
    "    branch_inputs = [Input(shape=shape) for shape in branch_shapes]\n",
    "\n",
    "    # Print input shapes\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        print(f'Branch {index+1} input shape: {branch_input.shape}')\n",
    "\n",
    "\n",
    "    # Set up branches\n",
    "    branches = []\n",
    "\n",
    "    for index, branch_input in enumerate(branch_inputs):\n",
    "        num_channels = img_channels_list[index]\n",
    "        branch_num = index + 1\n",
    "        x = branch_input\n",
    "\n",
    "        for conv_1x1_num in range(num_1x1_convs):\n",
    "            x = Conv2D(num_channels, kernel_size=(1, 1), strides=(1, 1), padding='valid',\n",
    "                        kernel_initializer='he_normal', name=f'Branch_{branch_num}_Conv1x1_{conv_1x1_num}')(x)\n",
    "            x = Activation(activation, name=f'Branch_{branch_num}_Conv1x1_{activation}_{conv_1x1_num}')(x)\n",
    "\n",
    "        if K.image_data_format() == 'channels_last':\n",
    "            x = Reshape((*branch_input.shape[1:], 1), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "        else:\n",
    "            x = Reshape((1, *branch_input.shape[1:]), name=f'Branch_{branch_num}_Reshape')(x)\n",
    "\n",
    "\n",
    "        x = Conv3D(first_conv_filters, kernel_size=first_conv_kernel, strides=(1, 1, 1), padding='same',\n",
    "                    kernel_initializer='he_normal', name=f'Branch_{branch_num}_3DConv')(x)\n",
    "        x = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same',\n",
    "                    name=f'Branch_{branch_num}_MaxPooling3D')(x)\n",
    "\n",
    "        # Dense Blocks\n",
    "        x = dense_block_3d(x, num_dense_blocks, growth_rate=growth_rate, activation=activation, name=f'Branch_{branch_num}__conv1')\n",
    "        x = transition_block_3d(x, dropout_1, activation=activation, name=f'Branch_{branch_num}__pool1')\n",
    "        x = dense_block_3d(x, num_dense_blocks, growth_rate=growth_rate, activation=activation, name=f'Branch_{branch_num}__conv2')\n",
    "        x = transition_block_3d(x, dropout_2, activation=activation, name=f'Branch_{branch_num}__pool2')\n",
    "        x = dense_block_3d(x, num_dense_blocks, growth_rate=growth_rate, activation=activation, name=f'Branch_{branch_num}__conv3')\n",
    "\n",
    "        x = GlobalAveragePooling3D(name=f'Branch_{branch_num}__avg_pool')(x)\n",
    "\n",
    "        branches.append(x)\n",
    "\n",
    "    # Print the output shape of the branches\n",
    "    for index, branch in enumerate(branches):\n",
    "        print(f'Branch {index+1} output shape: {branch.shape}')\n",
    "\n",
    "\n",
    "    if len(img_channels_list) > 1:\n",
    "    # Branch fusion\n",
    "        fusion = Concatenate(axis=1, name='Fusion_Concatenate')(branches)\n",
    "        print(f'Shape after concatenation: {fusion.shape}')\n",
    "        fusion = Dense(units=fusion.shape[-1], kernel_initializer='he_normal', name='Fusion_Dense_1')(fusion)\n",
    "        fusion = Activation(activation, name=f'Fusion_{activation}')(fusion)\n",
    "        fusion = Dense(units=nb_classes, kernel_initializer='he_normal', name='Fusion_Dense_2')(fusion)\n",
    "        out = Activation('softmax', name='Fusion_Softmax')(fusion)\n",
    "\n",
    "        print(f'Fusion output shape: {out.shape}')\n",
    "\n",
    "        model_name = '3D-Densenet-Fusion'\n",
    "    else:\n",
    "        # x = Dense(units=x.shape[-1], kernel_initializer='he_normal', name='Dense_1')(x)\n",
    "        # x = Activation(activation, name=f'Dense_1_{activation}')(x)\n",
    "        x = Dense(units=nb_classes, kernel_initializer='he_normal', name='Dense_2')(x)\n",
    "        out = Activation('softmax', name='Softmax')(x)\n",
    "\n",
    "        print(f'Output shape: {out.shape}')\n",
    "\n",
    "        model_name = '3D-Densenet'\n",
    "\n",
    "    model = Model(inputs=branch_inputs,\n",
    "                  outputs=out,\n",
    "                  name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dAEbx0plW_0"
   },
   "source": [
    "# 9) Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GtJn5VGElW_0"
   },
   "outputs": [],
   "source": [
    "#@title Training Summary Plot Creation Function\n",
    "def create_training_summary_plot(history, experiment_name, output_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print('Creating training summary plots...')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Sparse Categorical Accuracy')\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'], color='blue', label='train')\n",
    "    if 'val_sparse_categorical_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_sparse_categorical_accuracy'], color='orange', label='test')\n",
    "\n",
    "    # save plot to file\n",
    "    print('Saving training summary plot to file...')\n",
    "    filename = os.path.join(output_path, f'{experiment_name}_training_summary_plot.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nsidn3nalW_0"
   },
   "outputs": [],
   "source": [
    "#@title Model Training Function\n",
    "def train_model(model, train_dataset, val_dataset,\n",
    "                iteration = None, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Initialize variables from the hyperparameters\n",
    "    experiment_name = hyperparams['experiment_name']\n",
    "    epochs = hyperparams['epochs']\n",
    "    epochs_before_decay = hyperparams['epochs_before_decay']\n",
    "    lr_decay_rate = hyperparams['lr_decay_rate']\n",
    "    patience = hyperparams['patience']\n",
    "    loss = hyperparams['loss']\n",
    "    model_metrics = hyperparams['metrics']\n",
    "    output_path = hyperparams['output_path']\n",
    "    model_save_period = hyperparams['model_save_period']\n",
    "    optimizer = get_optimizer(**hyperparams)\n",
    "    callbacks = []\n",
    "\n",
    "    # Determine ID string for experiment\n",
    "    if experiment_name is not None:\n",
    "        experiment_id = experiment_name\n",
    "    elif iteration is not None:\n",
    "        experiment_id = f'experiment_{iteration+1}'\n",
    "    else:\n",
    "        experiment_id = 'experiment'\n",
    "\n",
    "    # Create best weights path filename\n",
    "    best_weights_path = os.path.join(output_path,\n",
    "        f'{model.name}_best_weights_{experiment_id}.hdf5')\n",
    "\n",
    "    checkpoint_path_prefix = os.path.join(output_path, f'{experiment_id}_checkpoint_')\n",
    "\n",
    "    if model_save_period is not None:\n",
    "        # Create callback to save model weights every 'period' number\n",
    "        # of epochs\n",
    "        cb_periodic_model_checkpoint = ModelCheckpoint(checkpoint_path_prefix + '{epoch:08d}.hdf5', period=model_save_period)\n",
    "        callbacks.append(cb_periodic_model_checkpoint)\n",
    "\n",
    "    if patience is not None:\n",
    "        # Create callback to stop training early if metrics don't improve\n",
    "        cb_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=patience,\n",
    "                                        verbose=1,\n",
    "                                        mode='auto',\n",
    "                                        restore_best_weights=True)\n",
    "        callbacks.append(cb_early_stopping)\n",
    "\n",
    "    if val_dataset is not None:\n",
    "        # Create callback to save model weights if the model performs\n",
    "        # better than the previously trained models\n",
    "        cb_save_best_model = ModelCheckpoint(best_weights_path,\n",
    "                                            monitor='val_loss',\n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            mode='auto')\n",
    "        callbacks.append(cb_save_best_model)\n",
    "\n",
    "    if lr_decay_rate is not None and epochs_before_decay is not None:\n",
    "        # This function keeps the initial learning rate for a set number of epochs\n",
    "        # and reduces it at decay rate after that\n",
    "        def scheduler(epoch, lr):\n",
    "            if epoch < epochs_before_decay:\n",
    "                return lr\n",
    "            else:\n",
    "                print(f'Learning rate reduced from {lr} to {lr*lr_decay_rate}...')\n",
    "                return lr * lr_decay_rate\n",
    "                # return lr * tf.math.exp(-0.1)\n",
    "\n",
    "        # Create learning rate scheduler callback for learning rate decay\n",
    "        cb_lr_decay = LearningRateScheduler(scheduler)\n",
    "\n",
    "        callbacks.append(cb_lr_decay)\n",
    "\n",
    "    # Compile the model with the appropriate loss function, optimizer,\n",
    "    # and metrics\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=model_metrics,\n",
    "                  loss_weights=None,\n",
    "                  weighted_metrics=None,\n",
    "                  run_eagerly=None,\n",
    "                  )\n",
    "\n",
    "    # Display a summary of the model being trained\n",
    "    model.summary()\n",
    "\n",
    "    # Record start time for model training\n",
    "    model_train_start = time.process_time()\n",
    "\n",
    "    # Train the model\n",
    "    model_history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    # Record end time for model training\n",
    "    model_train_end = time.process_time()\n",
    "\n",
    "    # Calculate training and testing times\n",
    "    model_train_time = datetime.timedelta(seconds=(model_train_end - model_train_start))\n",
    "\n",
    "    # Write model history to file\n",
    "    with open(os.path.join(output_path,\n",
    "         f'{experiment_id}_training_history.txt'), 'w') as hf:\n",
    "\n",
    "        hf.write(f'EXPERIMENT #{iteration+1} MODEL HISTORY:\\n')\n",
    "        hf.write('-----------------------------------------------\\n')\n",
    "        hf.write(f'MODEL: {model.name}\\n')\n",
    "        hf.write('-----------------------------------------------\\n')\n",
    "\n",
    "        # Save model summary to file as well\n",
    "        model.summary(print_fn=lambda x: hf.write(x + '\\n'))\n",
    "\n",
    "        # Show epoch with best validation value\n",
    "        if patience is not None:\n",
    "            hf.write(f'Best Epoch: {cb_early_stopping.best_epoch}\\n')\n",
    "\n",
    "        # Get number of epochs model actually ran for\n",
    "        ran_epochs = model_history.params['epochs']\n",
    "        if patience is not None:\n",
    "            if cb_early_stopping.stopped_epoch > 0:\n",
    "                ran_epochs = cb_early_stopping.stopped_epoch\n",
    "\n",
    "        # Save info from each epoch to file\n",
    "        for epoch in range(ran_epochs):\n",
    "            hf.write(f'EPOCH: {epoch+1}\\n')\n",
    "            for key in model_history.history.keys():\n",
    "                hf.write(f'  {key}: {model_history.history[key][epoch]}\\n')\n",
    "\n",
    "    create_training_summary_plot(model_history, experiment_id, output_path)\n",
    "\n",
    "    # Save final weights\n",
    "    # final_weights_path = os.path.join(output_path,\n",
    "    #     f'{model.name}_final_weights_{experiment_id}.hdf5')\n",
    "    # model.save_weights(final_weights_path)\n",
    "\n",
    "    return model, model_train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gux0XElRlW_0"
   },
   "source": [
    "# 10) Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JAG9dOQKlW_0"
   },
   "outputs": [],
   "source": [
    "#@title Model Testing Function\n",
    "def test_model(model, test_dataset, **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Testing {model.name} with test dataset...')\n",
    "\n",
    "    # Record start time for model evaluation\n",
    "    model_test_start = time.process_time()\n",
    "\n",
    "    # Get prediction values for test dataset\n",
    "    pred_test = model.predict(test_dataset,\n",
    "                              verbose=1,\n",
    "                              ).argmax(axis=1)\n",
    "\n",
    "    # Record end time for model evaluation\n",
    "    model_test_end = time.process_time()\n",
    "\n",
    "    # Get time elapsed for testing model\n",
    "    model_test_time = datetime.timedelta(seconds=(model_test_end - model_test_start))\n",
    "\n",
    "    print('Testing completed!')\n",
    "\n",
    "\n",
    "    return pred_test, model_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "G1nZS60qlW_1"
   },
   "outputs": [],
   "source": [
    "#@title Model Statistics Calculation Function\n",
    "def calculate_model_statistics(pred_test, target_test, labels,\n",
    "                               **hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    labels = hyperparams['all_class_labels']\n",
    "\n",
    "    overall_acc = metrics.accuracy_score(target_test, pred_test)\n",
    "    precision = metrics.precision_score(target_test, pred_test, average='micro')\n",
    "    recall = metrics.recall_score(target_test, pred_test, average='micro')\n",
    "    kappa = metrics.cohen_kappa_score(target_test, pred_test)\n",
    "    confusion_matrix = metrics.confusion_matrix(target_test, pred_test, labels=range(len(labels)))\n",
    "\n",
    "    # Supress/hide invalid value warning\n",
    "    # np.seterr(invalid='ignore')\n",
    "\n",
    "    # Calculate average accuracy and per-class accuracies\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "\n",
    "    # Get classification report\n",
    "    classification_report = metrics.classification_report(target_test,\n",
    "                                                          pred_test,\n",
    "                                                          labels=range(len(labels)),\n",
    "                                                          target_names=labels,\n",
    "                                                          digits=3)\n",
    "\n",
    "    results = {\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'average_accuracy': average_acc,\n",
    "        'precision_score': precision,\n",
    "        'recall_score': recall,\n",
    "        'cohen_kappa_score': kappa,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'per_class_accuracies': each_acc,\n",
    "        'labels': labels,\n",
    "        'classification_report': classification_report,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZPEycBA3lW_1"
   },
   "outputs": [],
   "source": [
    "#@title Confusion Matrix Plot Creation Function\n",
    "def create_confusion_matrix_plot(confusion_matrix, labels, model_name,\n",
    "                                 output_path='./', iteration=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Create filename for confusion matrix image file\n",
    "    if iteration is not None:\n",
    "        filename = f'experiment_{iteration+1}_{model_name}_confusion_matrix.png'\n",
    "    else:\n",
    "        filename = f'experiment_{model_name}_confusion_matrix.png'\n",
    "\n",
    "    # Create full file name for confusion matrix image file\n",
    "    cm_filename = os.path.join(output_path, filename)\n",
    "\n",
    "    # Create annotations for confusion matrix\n",
    "    print('Creating confusion matrix annotations...')\n",
    "    cm_sum = np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "    cm_perc = confusion_matrix / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(confusion_matrix).astype(str)\n",
    "    nrows, ncols = confusion_matrix.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = confusion_matrix[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "\n",
    "\n",
    "    # Create confusion matrix dataframe\n",
    "    print('Creating confusion matrix plot...')\n",
    "    cm = pd.DataFrame(confusion_matrix, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=(30,30))\n",
    "    sns.heatmap(cm, annot=annot, fmt='', ax=ax)\n",
    "\n",
    "    if iteration is not None:\n",
    "        plt.title(f'Experiment #{iteration+1} {model_name} Confusion Matrix')\n",
    "    else:\n",
    "        plt.title(f'Experiment w/ {model_name} Confusion Matrix')\n",
    "\n",
    "    print('Saving confusion matrix plot...')\n",
    "    plt.savefig(cm_filename)\n",
    "\n",
    "    # Clear plot data for next plot\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TJPIP-5ulW_1"
   },
   "outputs": [],
   "source": [
    "#@title Experiment Results Output Function\n",
    "def output_experiment_results(experiment_info):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Get variables from dictionary\n",
    "    experiment_name = experiment_info['experiment_name']\n",
    "    model_name = experiment_info['model_name']\n",
    "    model_train_time = experiment_info['model_train_time']\n",
    "    model_test_time = experiment_info['model_test_time']\n",
    "    overall_acc = experiment_info['overall_accuracy']\n",
    "    average_acc = experiment_info['average_accuracy']\n",
    "    per_class_accuracies = experiment_info['per_class_accuracies']\n",
    "    precision = experiment_info['precision_score']\n",
    "    recall = experiment_info['recall_score']\n",
    "    kappa = experiment_info['cohen_kappa_score']\n",
    "    labels = experiment_info['labels']\n",
    "    classification_report = experiment_info['classification_report']\n",
    "\n",
    "    # Print results\n",
    "    print('---------------------------------------------------')\n",
    "    if experiment_name is None:\n",
    "        print('             MODEL EXPERIMENT RESULTS              ')\n",
    "    else:\n",
    "        print(f'          \"{experiment_name}\" RESULTS              ')\n",
    "    print('---------------------------------------------------')\n",
    "    print(f' MODEL NAME: {model_name}')\n",
    "    print('---------------------------------------------------')\n",
    "    print(f'{model_name} train time: {model_train_time}')\n",
    "    print(f'{model_name} test time:  {model_test_time}')\n",
    "    print('...................................................')\n",
    "    print(f'{model_name} overall accuracy:  {overall_acc}')\n",
    "    print(f'{model_name} average accuracy:  {average_acc}')\n",
    "    print(f'{model_name} precision score:   {precision}')\n",
    "    print(f'{model_name} recall score:      {recall}')\n",
    "    print(f'{model_name} cohen kappa score: {kappa}')\n",
    "    print('...................................................')\n",
    "    print(f'{model_name} Per-class accuracies:')\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f'{label}: {per_class_accuracies[i]}')\n",
    "    print('---------------------------------------------------')\n",
    "    print('              CLASSIFICATION REPORT                ')\n",
    "    print('...................................................')\n",
    "    print(classification_report)\n",
    "    print('---------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIFpsxQClW_1"
   },
   "source": [
    "# 11) Test Harness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mw08-hBqlW_1"
   },
   "outputs": [],
   "source": [
    "#@title Experiment Parameter List\n",
    "PARAMETER_LIST = (\n",
    "    \"experiment_name\",\n",
    "    \"experiment_number\",\n",
    "    \"cuda\",\n",
    "    \"restore\",\n",
    "    \"output_path\",\n",
    "    \"dataset\",\n",
    "    \"path_to_dataset\",\n",
    "    \"reuse_last_dataset\",\n",
    "    \"predict_only\",\n",
    "    \"skip_data_preprocessing\",\n",
    "    \"skip_band_selection\",\n",
    "    \"skip_data_postprocessing\",\n",
    "    \"model_id\",\n",
    "    \"add_branch\",\n",
    "    \"random_seed\",\n",
    "    \"epochs\",\n",
    "    \"epochs_before_decay\",\n",
    "    \"batch_size\",\n",
    "    \"patch_size\",\n",
    "    \"center_pixel\",\n",
    "    \"train_split\",\n",
    "    \"split_mode\",\n",
    "    \"class_balancing\",\n",
    "    \"iterations\",\n",
    "    \"patience\",\n",
    "    \"model_save_period\",\n",
    "    \"optimizer\",\n",
    "    \"lr\",\n",
    "    \"lr_decay_rate\",\n",
    "    \"momentum\",\n",
    "    \"epsilon\",\n",
    "    \"initial_accumulator_value\",\n",
    "    \"beta\",\n",
    "    \"beta_1\",\n",
    "    \"beta_2\",\n",
    "    \"amsgrad\",\n",
    "    \"rho\",\n",
    "    \"centered\",\n",
    "    \"nesterov\",\n",
    "    \"learning_rate_power\",\n",
    "    \"l1_regularization_strength\",\n",
    "    \"l2_regularization_strength\",\n",
    "    \"l2_shrinkage_regularization_strength\",\n",
    "    \"flip_augmentation\",\n",
    "    \"radiation_augmentation\",\n",
    "    \"mixture_augmentation\",\n",
    "    \"use_hs_data\",\n",
    "    \"use_lidar_ms_data\",\n",
    "    \"use_lidar_ndsm_data\",\n",
    "    \"use_vhr_data\",\n",
    "    \"use_all_data\",\n",
    "    \"normalize_hs_data\",\n",
    "    \"normalize_lidar_ms_data\",\n",
    "    \"normalize_lidar_ndsm_data\",\n",
    "    \"normalize_vhr_data\",\n",
    "    \"hs_resampling\",\n",
    "    \"lidar_ms_resampling\",\n",
    "    \"lidar_ndsm_resampling\",\n",
    "    \"vhr_resampling\",\n",
    "    \"hs_histogram_equalization\",\n",
    "    \"lidar_ms_histogram_equalization\",\n",
    "    \"lidar_dsm_histogram_equalization\",\n",
    "    \"lidar_dem_histogram_equalization\",\n",
    "    \"lidar_ndsm_histogram_equalization\",\n",
    "    \"vhr_histogram_equalization\",\n",
    "    \"hs_data_filter\",\n",
    "    \"lidar_ms_data_filter\",\n",
    "    \"lidar_dsm_data_filter\",\n",
    "    \"lidar_dem_data_filter\",\n",
    "    \"vhr_data_filter\",\n",
    "    \"band_reduction_method\",\n",
    "    \"n_components\",\n",
    "    \"selected_bands\",\n",
    "    \"select_only_hs_bands\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ECC74w6DG8Ii"
   },
   "outputs": [],
   "source": [
    "#@title Test Harness Runner Function\n",
    "def run_test_harness(**hyperparams):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_name = hyperparams['experiment_name']\n",
    "\n",
    "    # Get output path\n",
    "    if hyperparams['experiment_number'] < 1:\n",
    "        experiment_number = 1\n",
    "    else:\n",
    "        experiment_number = hyperparams['experiment_number']\n",
    "\n",
    "    # Get output path\n",
    "    if hyperparams['output_path'] is not None:\n",
    "        output_path = hyperparams['output_path']\n",
    "    else:\n",
    "        output_path = './'\n",
    "\n",
    "    # Get hyperparam derived variable values\n",
    "    if 'experiments_json' in hyperparams and hyperparams['experiments_json'] is not None:\n",
    "        # Transpose the json dataframe, since the experiments are read\n",
    "        # in as columns instead of rows\n",
    "        experiments = pd.read_json(hyperparams['experiments_json']).T\n",
    "        iterations = experiments.shape[0]\n",
    "        outfile_prefix = Path(hyperparams['experiments_json']).stem\n",
    "    elif 'experiments_csv' in hyperparams and hyperparams['experiments_csv'] is not None:\n",
    "        experiments = pd.read_csv(hyperparams['experiments_csv'])\n",
    "        iterations = experiments.shape[0]\n",
    "        outfile_prefix = Path(hyperparams['experiments_csv']).stem\n",
    "    else:\n",
    "        experiments = None\n",
    "        iterations = hyperparams['iterations']\n",
    "        if iterations is None:\n",
    "            iterations = 1\n",
    "        if hyperparams['experiment_name'] is None:\n",
    "            outfile_prefix = 'experiment'\n",
    "        else:\n",
    "            outfile_prefix = hyperparams['experiment_name']\n",
    "\n",
    "    # Get model name of CPU\n",
    "    cpu_name = cpuinfo.get_cpu_info()['brand_raw']\n",
    "\n",
    "    # Get model names of all GPUs on system\n",
    "    gpu_names = []\n",
    "    for gpu in tf.config.list_physical_devices(device_type = 'GPU'):\n",
    "        gpu_names.append(tf.config.experimental.get_device_details(gpu)['device_name'])\n",
    "\n",
    "    # Initialize data list variables for CSV output at end of program\n",
    "    experiment_data_list = []\n",
    "    per_class_data_lists = {}\n",
    "    per_class_selected_band_lists = {}\n",
    "\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('BEGINNING EXPERIMENTS...')\n",
    "    print('vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv')\n",
    "    print()\n",
    "\n",
    "    # Make a generater to create prime number random seeds\n",
    "    primes = prime_generator()\n",
    "\n",
    "    # Set variables that carry state over experiments to None\n",
    "    dataset_choice = None\n",
    "    data = None\n",
    "    train_gt = None\n",
    "    test_gt = None\n",
    "    dataset_info = {\n",
    "        'name': None,\n",
    "        'num_classes': None,\n",
    "        'ignored_labels': None,\n",
    "        'class_labels': None,\n",
    "        'label_mapping': None,\n",
    "    }\n",
    "    train_dataset = None\n",
    "    val_dataset = None\n",
    "    test_dataset = None\n",
    "    target_test = None\n",
    "    band_selection_time = None\n",
    "    bands_selected = None\n",
    "\n",
    "    # Go through experiment iterations\n",
    "    for iteration in range(experiment_number - 1, experiment_number - 1 + iterations):\n",
    "\n",
    "        # Clean memory in each iteration (otherwise the machine may\n",
    "        # randomly run out of memory if it is being pushed to its\n",
    "        # limit)\n",
    "        gc.collect()\n",
    "\n",
    "        print('*******************************************************')\n",
    "        print(f'<<< EXPERIMENT #{iteration+1}  STARTING >>>')\n",
    "        print('*******************************************************')\n",
    "        print()\n",
    "\n",
    "        experiment_data = {\n",
    "            'experiment_number': iteration + 1,\n",
    "            'experiment_name': experiment_name,\n",
    "            'success': False,\n",
    "            'random_seed': None,\n",
    "            'dataset': None,\n",
    "            'band_reduction_method': None,\n",
    "            'band_selection_time': None,\n",
    "            'bands_selected': None,\n",
    "            'channels': None,\n",
    "            'model': None,\n",
    "            'device': None,\n",
    "            'epochs': None,\n",
    "            'batch_size': None,\n",
    "            'patch_size': None,\n",
    "            'train_split': None,\n",
    "            'optimizer': None,\n",
    "            'learning_rate': None,\n",
    "            'loss': None,\n",
    "            'train_time': 0.0,\n",
    "            'test_time': 0.0,\n",
    "            'overall_accuracy': 0.0,\n",
    "            'average_accuracy': 0.0,\n",
    "            'precision_score': 0.0,\n",
    "            'recall_score': 0.0,\n",
    "            'cohen_kappa_score': 0.0,\n",
    "        }\n",
    "\n",
    "        per_class_data = {\n",
    "            'experiment_number': iteration + 1,\n",
    "            'experiment_name': experiment_name,\n",
    "            'random_seed': None,\n",
    "            'band_reduction_method': None,\n",
    "            'band_selection_time': None,\n",
    "            'bands_selected': None,\n",
    "            'model': None,\n",
    "            'train_time': 0.0,\n",
    "            'test_time': 0.0,\n",
    "            'overall_accuracy': 0.0,\n",
    "            'average_accuracy': 0.0,\n",
    "            'precision_score': 0.0,\n",
    "            'recall_score': 0.0,\n",
    "            'cohen_kappa_score': 0.0,\n",
    "        }\n",
    "\n",
    "        per_class_selected_bands = None\n",
    "\n",
    "        experiments_results_file = f'{outfile_prefix}_results.csv'\n",
    "        class_results_file = f'{outfile_prefix}__{dataset_choice}__class_results.csv'\n",
    "        selected_bands_file = f'{outfile_prefix}__{dataset_choice}__selected_bands.csv'\n",
    "\n",
    "        # Experiment has begun, so make sure to catch any failures that\n",
    "        # may occur\n",
    "        try:\n",
    "\n",
    "            # If loading experiments from a file, get new set of hyperparams\n",
    "            if experiments is not None:\n",
    "                # Get hyperparameters from dictionary\n",
    "                hyperparams = experiments.iloc[iteration].to_dict()\n",
    "\n",
    "                experiment_name = experiments.index[iteration]\n",
    "\n",
    "                hyperparams['experiment_name'] = experiment_name\n",
    "                experiment_data['experiment_name'] = experiment_name\n",
    "                per_class_data['experiment_name'] = experiment_name\n",
    "\n",
    "                # Fill in any missing parameters with None\n",
    "                for param in PARAMETER_LIST:\n",
    "                    if param not in hyperparams:\n",
    "                        hyperparams[param] = None\n",
    "\n",
    "                if hyperparams['experiment_number'] > 0:\n",
    "                    iteration = hyperparams['experiment_number'] - 1\n",
    "                    experiment_data['experiment_number'] = hyperparams['experiment_number']\n",
    "                    per_class_data['experiment_number'] = hyperparams['experiment_number']\n",
    "\n",
    "                # Ignore the output path in the experiments, use the path\n",
    "                # from command line arguments\n",
    "                hyperparams['output_path'] = output_path\n",
    "\n",
    "\n",
    "                print('<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~>')\n",
    "                print(f'EXPERIMENT NAME: {experiments.index[iteration]}')\n",
    "                print('<~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~>')\n",
    "                print()\n",
    "            else:\n",
    "                # Fill in any missing parameters with None\n",
    "                for param in PARAMETER_LIST:\n",
    "                    if param not in hyperparams:\n",
    "                        hyperparams[param] = None\n",
    "\n",
    "                experiment_name = hyperparams['experiment_name']\n",
    "\n",
    "            # Save hyperparameters to experiment file if argument\n",
    "            # has been given\n",
    "            if hyperparams['save_experiment_path'] is not None:\n",
    "                extension = Path(hyperparams['save_experiment_path']).suffix\n",
    "                if extension == '.json':\n",
    "                    experiments_params_df = pd.DataFrame.from_dict({experiment_name:hyperparams,}, orient='index')\n",
    "                    experiments_params_df.to_json(hyperparams['save_experiment_path'], orient='index', indent=4)\n",
    "                elif extension == '.csv':\n",
    "                    # experiments_params_df = pd.DataFrame.from_dict(hyperparams)\n",
    "                    experiments_params_df = pd.DataFrame.from_dict({experiment_name:hyperparams,}, orient='index')\n",
    "                    # experiments_params_df.to_csv(hyperparams['save_experiment_path'])\n",
    "                    experiments_params_df.to_csv(hyperparams['save_experiment_path'])\n",
    "                else:\n",
    "                    #TODO\n",
    "                    pass\n",
    "\n",
    "\n",
    "            # Print out parameters for experiment\n",
    "            print('.......................................................')\n",
    "            print('EXPERIMENT PARAMETERS')\n",
    "            print('.......................................................')\n",
    "            header = '{:<40} | {:<40}'.format('PARAMETER', 'VALUE')\n",
    "            print(header)\n",
    "            print('=' * len(header))\n",
    "            for key in hyperparams:\n",
    "                print('{:<40} | {:<40}'.format(key, str(hyperparams[key])))\n",
    "                print('-' * len(header))\n",
    "            print('-' * len(header))\n",
    "            print('.......................................................')\n",
    "\n",
    "\n",
    "            # Model checks\n",
    "            if (hyperparams['model_id'] == 'fusion-fcn'\n",
    "                and hyperparams['dataset'] != 'grss_dfc_2018'):\n",
    "                print('Cannot use fusion-fcn model without the grss_dfc_2018 dataset!')\n",
    "                exit(1)\n",
    "            elif (hyperparams['model_id'] == 'fusion-fcn-v2'\n",
    "                and hyperparams['dataset'] != 'grss_dfc_2018'):\n",
    "                print('Cannot use fusion-fcn-v2 model without the grss_dfc_2018 dataset!')\n",
    "                exit(1)\n",
    "            elif (hyperparams['model_id'] == '3d-densenet-fusion'\n",
    "                and hyperparams['dataset'] != 'grss_dfc_2018'):\n",
    "                print('Cannot use 3d-densenet-fusion model without the grss_dfc_2018 dataset!')\n",
    "                exit(1)\n",
    "\n",
    "            # Initialize random seed for sampling function\n",
    "            # Each random seed is a prime number, in order\n",
    "            if hyperparams['random_seed'] is not None:\n",
    "                seed = hyperparams['random_seed']\n",
    "            else:\n",
    "                seed = next(primes)\n",
    "            print(f'< Iteration #{iteration} random seed: {seed} >')\n",
    "            print()\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Choose the appropriate device from the hyperparameters\n",
    "            device = get_device(hyperparams['cuda'])\n",
    "\n",
    "            if 'CPU' in device:\n",
    "                device_name = cpu_name\n",
    "            else:\n",
    "                gpu_num = int(device.split(':')[-1])\n",
    "                device_name = gpu_names[gpu_num]\n",
    "                gpu = tf.config.list_physical_devices('GPU')[gpu_num]\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                # tf.config.experimental.set_virtual_device_configuration(\n",
    "                #     gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]\n",
    "                # )\n",
    "\n",
    "            # Device has been selected, so do all possible computation with device\n",
    "            with tf.device(device):\n",
    "                reuse_last_dataset = hyperparams['reuse_last_dataset']\n",
    "                if reuse_last_dataset and dataset_choice is not None:\n",
    "                    print()\n",
    "                    print(f'< Reusing last dataset: {dataset_choice} >')\n",
    "                else:\n",
    "\n",
    "                    reuse_last_dataset = False\n",
    "\n",
    "                    print()\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print('LOADING DATASET...')\n",
    "                    print('vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv')\n",
    "\n",
    "                    # Get dataset choice parameter\n",
    "                    dataset_choice = hyperparams['dataset']\n",
    "                    print()\n",
    "                    print(f' < Dataset Chosen: {dataset_choice} >')\n",
    "                    print()\n",
    "\n",
    "                    # Make sure dataset is in per-class data list dictionary\n",
    "                    if dataset_choice not in per_class_data_lists:\n",
    "                        per_class_data_lists[dataset_choice] = []\n",
    "\n",
    "                    # Make sure dataset is in per-class data list dictionary\n",
    "                    if dataset_choice not in per_class_selected_band_lists:\n",
    "                        per_class_selected_band_lists[dataset_choice] = []\n",
    "\n",
    "                    # Get selected dataset\n",
    "                    if not reuse_last_dataset:\n",
    "                        if dataset_choice == 'grss_dfc_2018':\n",
    "                            # Determine what parts of dataset to use\n",
    "                            if (not hyperparams['use_hs_data']\n",
    "                                and not hyperparams['use_lidar_ms_data']\n",
    "                                and not hyperparams['use_lidar_ndsm_data']\n",
    "                                and not hyperparams['use_vhr_data']\n",
    "                                and not hyperparams['use_all_data']):\n",
    "\n",
    "                                print('<!> No specific data selected, defaulting to using only hyperspectral data... <!>')\n",
    "                                hyperparams['use_hs_data'] = True\n",
    "\n",
    "                            data, train_gt, test_gt, dataset_info = load_grss_dfc_2018_uh_dataset(**hyperparams)\n",
    "                        elif dataset_choice == 'indian_pines':\n",
    "                            # data, train_gt, test_gt, dataset_info = load_indian_pines_dataset(**hyperparams)\n",
    "                            print(\"Indian Pines dataset currently unimplemented\")\n",
    "                            exit(1)\n",
    "                        elif dataset_choice == 'pavia_center':\n",
    "                            # data, train_gt, test_gt, dataset_info = load_pavia_center_dataset(**hyperparams)\n",
    "                            print(\"Pavia Center dataset currently unimplemented\")\n",
    "                            exit(1)\n",
    "                        elif dataset_choice == 'university_of_pavia':\n",
    "                            # data, train_gt, test_gt, dataset_info = load_university_of_pavia_dataset(**hyperparams)\n",
    "                            print(\"University of Pavia dataset currently unimplemented\")\n",
    "                            exit(1)\n",
    "                        else:\n",
    "                            print('No dataset chosen! Defaulting to only hyperspectral bands of grss_dfc_2018...')\n",
    "                            dataset_choice = 'grss_dfc_2018'\n",
    "                            hyperparams['use_hs_data'] = True\n",
    "                            data, train_gt, test_gt, dataset_info = load_grss_dfc_2018_uh_dataset(**hyperparams)\n",
    "                    print(f\"Dataset Info: {dataset_info}\")\n",
    "\n",
    "                    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "                    print('DATASET LOADED!')\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print()\n",
    "\n",
    "                    if not hyperparams['skip_data_preprocessing']:\n",
    "                        print('-------------------------------------------------------------------')\n",
    "                        print('PREPROCESS THE DATA')\n",
    "                        print('-------------------------------------------------------------------')\n",
    "                        data = preprocess_data(data, **hyperparams)\n",
    "                        print('-------------------------------------------------------------------')\n",
    "                        print()\n",
    "\n",
    "                    if not hyperparams['skip_band_selection']:\n",
    "                        print('-------------------------------------------------------------------')\n",
    "                        print('RUN BAND SELECTION ALGORITHM')\n",
    "                        print('-------------------------------------------------------------------')\n",
    "                        #TODO - allow per-modality band selection\n",
    "\n",
    "                        if hyperparams['select_only_hs_bands']:\n",
    "                            hs_channels = dataset_info['hs_channels']\n",
    "                            if hs_channels is not None:\n",
    "                                # Get the non-hyperspectral data so it\n",
    "                                # can be appended to the reduced\n",
    "                                # hyperspectral data later\n",
    "                                non_hs_channels = [channel for channel in range(data.shape[-1]) if channel not in hs_channels]\n",
    "                                non_hs_data = data[..., non_hs_channels]\n",
    "                                data = data[..., hs_channels]\n",
    "\n",
    "                                hs_channel_labels = [label for channel, label in enumerate(dataset_info['channel_labels']) if channel in hs_channels]\n",
    "                                non_hs_channel_labels = [label for channel, label in enumerate(dataset_info['channel_labels']) if channel not in hs_channels]\n",
    "\n",
    "                                data, band_selection_time, bands_selected = band_selection(data, train_gt, **hyperparams)\n",
    "\n",
    "                                # Update the channel indices to reflect\n",
    "                                # reduced HS data\n",
    "                                num_hs_channels = data.shape[-1]\n",
    "                                dataset_info['hs_channels'] = range(num_hs_channels)\n",
    "                                dataset_info['lidar_ms_channels'] = [new_channel + num_hs_channels for new_channel, channel in enumerate(non_hs_channels) if channel in dataset_info['lidar_ms_channels']]\n",
    "                                dataset_info['lidar_ndsm_channels'] = [new_channel + num_hs_channels for new_channel, channel in enumerate(non_hs_channels) if channel in dataset_info['lidar_ndsm_channels']]\n",
    "                                dataset_info['vhr_rgb_channels'] = [new_channel + num_hs_channels for new_channel, channel in enumerate(non_hs_channels) if channel in dataset_info['vhr_rgb_channels']]\n",
    "\n",
    "                                # Stack the non hyperspectral data onto\n",
    "                                # the data array\n",
    "                                data = np.dstack((data, non_hs_data))\n",
    "\n",
    "                                if bands_selected is not None:\n",
    "                                    # Initialize dictionary of band selection\n",
    "                                    # data to output to file\n",
    "                                    per_class_selected_bands = {\n",
    "                                        'experiment_number': iteration + 1,\n",
    "                                        'experiment_name': experiment_name,\n",
    "                                        'random_seed': seed,\n",
    "                                        'band_reduction_method': hyperparams['band_reduction_method'],\n",
    "                                        'band_selection_time': band_selection_time,\n",
    "                                        'model': None,\n",
    "                                        'overall_accuracy': 0.0,\n",
    "                                        'average_accuracy': 0.0,\n",
    "                                        'precision_score': 0.0,\n",
    "                                        'recall_score': 0.0,\n",
    "                                        'cohen_kappa_score': 0.0,\n",
    "                                    }\n",
    "\n",
    "\n",
    "                                    # Write selected bands to band selection\n",
    "                                    # dictionary\n",
    "                                    for channel, label in enumerate(hs_channel_labels):\n",
    "                                        if bands_selected is not None and channel in bands_selected:\n",
    "                                            per_class_selected_bands[f'{label} (channel {channel})'] = True\n",
    "                                        else:\n",
    "                                            per_class_selected_bands[f'{label} (channel {channel})'] = False\n",
    "\n",
    "                                    for channel, label in enumerate(non_hs_channel_labels):\n",
    "                                        per_class_selected_bands[f'{label} (channel {channel + len(hs_channels)})'] = True\n",
    "\n",
    "                            else:\n",
    "                                print('There are no hyperspectral channels in this experiment! Skipping band selection...')\n",
    "\n",
    "                        else:\n",
    "                            data, band_selection_time, bands_selected = band_selection(data, train_gt, **hyperparams)\n",
    "\n",
    "                            if bands_selected is not None:\n",
    "                                per_class_selected_bands = {\n",
    "                                    'experiment_number': iteration + 1,\n",
    "                                    'experiment_name': experiment_name,\n",
    "                                    'random_seed': seed,\n",
    "                                    'band_reduction_method': hyperparams['band_reduction_method'],\n",
    "                                    'band_selection_time': band_selection_time,\n",
    "                                    'model': None,\n",
    "                                    'overall_accuracy': 0.0,\n",
    "                                    'average_accuracy': 0.0,\n",
    "                                    'precision_score': 0.0,\n",
    "                                    'recall_score': 0.0,\n",
    "                                    'cohen_kappa_score': 0.0,\n",
    "                                }\n",
    "\n",
    "                                if dataset_info['channel_labels'] is not None:\n",
    "                                    for channel, label in enumerate(dataset_info['channel_labels']):\n",
    "                                        if bands_selected is not None and channel in bands_selected:\n",
    "                                            per_class_selected_bands[f'{label} (channel {channel})'] = True\n",
    "                                        else:\n",
    "                                            per_class_selected_bands[f'{label} (channel {channel})'] = False\n",
    "\n",
    "                                            # Remove any channels from\n",
    "                                            # modality channel lists if they\n",
    "                                            # are not selected\n",
    "                                            if channel in dataset_info['hs_channels']:\n",
    "                                                dataset_info['hs_channels'].remove(channel)\n",
    "                                            if channel in dataset_info['lidar_ms_channels']:\n",
    "                                                dataset_info['lidar_ms_channels'].remove(channel)\n",
    "                                            if channel in dataset_info['lidar_ndsm_channels']:\n",
    "                                                dataset_info['lidar_ndsm_channels'].remove(channel)\n",
    "                                            if channel in dataset_info['vhr_rgb_channels']:\n",
    "                                                dataset_info['vhr_rgb_channels'].remove(channel)\n",
    "\n",
    "                        print('-------------------------------------------------------------------')\n",
    "                        print()\n",
    "\n",
    "                # Set dataset variables\n",
    "                dataset_name = dataset_info['name']\n",
    "                num_classes = dataset_info['num_classes']\n",
    "                hs_channels = dataset_info['hs_channels']\n",
    "                lidar_ms_channels = dataset_info['lidar_ms_channels']\n",
    "                lidar_ndsm_channels = dataset_info['lidar_ndsm_channels']\n",
    "                vhr_rgb_channels = dataset_info['vhr_rgb_channels']\n",
    "                ignored_labels = dataset_info['ignored_labels']\n",
    "                all_class_labels = dataset_info['class_labels']\n",
    "                valid_class_labels = [label for index, label in enumerate(all_class_labels)\n",
    "                                        if index not in ignored_labels]\n",
    "\n",
    "\n",
    "                epochs = hyperparams['epochs']\n",
    "                supervision = 'full'\n",
    "                batch_size = hyperparams['batch_size']\n",
    "                patch_size = hyperparams['patch_size']\n",
    "                train_split = hyperparams['train_split']\n",
    "                optimizer = hyperparams['optimizer']\n",
    "                learning_rate = hyperparams['lr']\n",
    "                loss = 'sparse_categorical_crossentropy'\n",
    "                img_channels = data.shape[-1]\n",
    "                img_rows = patch_size\n",
    "                img_cols = patch_size\n",
    "\n",
    "                if (hyperparams['model_id'] == 'fusion-fcn'\n",
    "                    or hyperparams['model_id'] == 'fusion-fcn-v2'):\n",
    "                    branch_1_channels = (*vhr_rgb_channels, *lidar_ms_channels)\n",
    "                    branch_2_channels = (*lidar_ndsm_channels,)\n",
    "                    branch_3_channels = (*hs_channels,)\n",
    "                    input_channels = (branch_1_channels, branch_2_channels, branch_3_channels)\n",
    "                    input_sizes = [len(input_channel) for input_channel in input_channels]\n",
    "                elif (hyperparams['model_id'] == '3d-densenet-fusion'\n",
    "                      or hyperparams['model_id'] == '3d-densenet-fusion2'\n",
    "                      or hyperparams['model_id'] == '3d-densenet-fusion3'\n",
    "                      or hyperparams['model_id'] == '3d-densenet-fusion4'):\n",
    "                    # branch_1_channels = (*vhr_rgb_channels, )\n",
    "                    # branch_2_channels = (*lidar_ms_channels, *lidar_ndsm_channels, )\n",
    "                    # branch_3_channels = (*hs_channels, )\n",
    "                    # input_channels = (branch_1_channels, branch_2_channels, branch_3_channels)\n",
    "                    branch_1_channels = (*hs_channels, )\n",
    "                    branch_2_channels = (*lidar_ms_channels, )\n",
    "                    branch_3_channels = (*lidar_ndsm_channels, )\n",
    "                    branch_4_channels = (*vhr_rgb_channels, )\n",
    "                    input_channels = (branch_1_channels, branch_2_channels, branch_3_channels, branch_4_channels)\n",
    "                    input_sizes = [len(input_channel) for input_channel in input_channels]\n",
    "                elif hyperparams['model_id'] == '3d-densenet-modified':\n",
    "                    if hyperparams['add_branch'] is not None:\n",
    "                        input_channels = []\n",
    "                        branch_list = []\n",
    "                        for branch in hyperparams['add_branch']:\n",
    "                            branch_channels = []\n",
    "                            branch_modalities = []\n",
    "                            for modality in str(branch).split(','):\n",
    "                                if modality == 'hs' and (hyperparams['use_hs_data'] or hyperparams['use_all_data']):\n",
    "                                    branch_channels += [*hs_channels, ]\n",
    "                                elif modality == 'lidar_ms' and (hyperparams['use_lidar_ms_data'] or hyperparams['use_all_data']):\n",
    "                                    branch_channels += [*lidar_ms_channels, ]\n",
    "                                elif modality == 'lidar_ndsm' and (hyperparams['use_lidar_ndsm_data'] or hyperparams['use_all_data']):\n",
    "                                    branch_channels += [*lidar_ndsm_channels, ]\n",
    "                                elif modality == 'vhr_rgb' and (hyperparams['use_vhr_data'] or hyperparams['use_all_data']):\n",
    "                                    branch_channels += [*vhr_rgb_channels, ]\n",
    "\n",
    "                                branch_modalities.append(modality)\n",
    "                            branch_list.append(branch_modalities)\n",
    "                            if len(branch_channels) > 0:\n",
    "                                input_channels.append(branch_channels)\n",
    "\n",
    "                        for index, modalities in enumerate(branch_list):\n",
    "                            print(f'Branch {index} modalities: {modalities}')\n",
    "\n",
    "                        if len(input_channels) > 0:\n",
    "                            input_sizes = [len(input_channel) for input_channel in input_channels]\n",
    "                        else:\n",
    "                            input_channels = None\n",
    "                            input_sizes = [img_channels]\n",
    "\n",
    "                    else:\n",
    "                        input_channels = None\n",
    "                        input_sizes = [img_channels]\n",
    "\n",
    "                else:\n",
    "                    input_channels = None\n",
    "                    input_sizes = None\n",
    "\n",
    "                # Check to see if model uses 3d convolutions - if so\n",
    "                # then the input dimensions will need to be expanded\n",
    "                # to include the 'planes' dimension\n",
    "                if (hyperparams['model_id'] == '3d-densenet'\n",
    "                    # or hyperparams['model_id'] == '3d-densenet-fusion'\n",
    "                    or hyperparams['model_id'] == '3d-cnn'):\n",
    "                    expand_dims = True\n",
    "                else:\n",
    "                    expand_dims = False\n",
    "\n",
    "                # Add and update hyperparameters for model training\n",
    "                hyperparams.update(\n",
    "                    {\n",
    "                        'random_seed': seed,\n",
    "                        'n_classes': num_classes,\n",
    "                        'n_bands': img_channels,\n",
    "                        'all_class_labels': all_class_labels,\n",
    "                        'ignored_labels': ignored_labels,\n",
    "                        'device': device,\n",
    "                        'supervision': supervision,\n",
    "                        'center_pixel': True,\n",
    "                        'one_hot_encoding': True,\n",
    "                        'metrics': ['sparse_categorical_accuracy'],\n",
    "                        'loss': loss,\n",
    "                        'input_channels': input_channels,\n",
    "                        'expand_dims': expand_dims,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Update experiment data\n",
    "                experiment_data.update({\n",
    "                    'random_seed': seed,\n",
    "                    'dataset': dataset_name,\n",
    "                    'band_reduction_method': hyperparams['band_reduction_method'],\n",
    "                    'band_selection_time': band_selection_time,\n",
    "                    'bands_selected': bands_selected,\n",
    "                    'channels': img_channels,\n",
    "                    'device': device_name,\n",
    "                    'epochs': epochs,\n",
    "                    'batch_size': batch_size,\n",
    "                    'patch_size': patch_size,\n",
    "                    'train_split': train_split,\n",
    "                    'optimizer': optimizer,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'loss': loss,\n",
    "                })\n",
    "\n",
    "                # Update per-class data for experiment\n",
    "                per_class_data.update({\n",
    "                    'random_seed': seed,\n",
    "                    'band_reduction_method': hyperparams['band_reduction_method'],\n",
    "                    'band_selection_time': band_selection_time,\n",
    "                    'bands_selected': bands_selected,\n",
    "                })\n",
    "                for label in all_class_labels:\n",
    "                    per_class_data[label] = 0.0\n",
    "\n",
    "                if not reuse_last_dataset:\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print('SPLIT DATA FOR TRAINING, VALIDATION, AND TESTING')\n",
    "                    print('-------------------------------------------------------------------')\n",
    "\n",
    "                    print('Breaking down image into data patches and splitting data into train, validation, and test sets...')\n",
    "                    train_dataset, val_dataset, test_dataset = create_datasets(data, train_gt, test_gt, **hyperparams)\n",
    "                    target_test = np.array(test_dataset.labels)\n",
    "                    # datasets = create_datasets_v2(data, train_gt, test_gt, **hyperparams)\n",
    "\n",
    "                    # train_dataset = (datasets['train_dataset'], datasets['train_steps'])\n",
    "                    # val_dataset = (datasets['val_dataset'], datasets['val_steps'])\n",
    "                    # test_dataset = (datasets['test_dataset'], datasets['test_steps'])\n",
    "                    # target_test = datasets['target_test']\n",
    "\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print()\n",
    "\n",
    "\n",
    "                print('-------------------------------------------------------------------')\n",
    "                print('CREATE MODEL')\n",
    "                print('-------------------------------------------------------------------')\n",
    "\n",
    "                # Create specified model\n",
    "                if hyperparams['model_id'] == '2d-densenet':\n",
    "                    model = densenet_2d_model(img_rows=img_rows,\n",
    "                                    img_cols=img_cols,\n",
    "                                    img_channels=img_channels,\n",
    "                                    nb_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == '2d-densenet-multi':\n",
    "                    pass    #TODO\n",
    "                elif hyperparams['model_id'] == '3d-densenet':\n",
    "                    model = densenet_3d_model(img_rows=img_rows,\n",
    "                                    img_cols=img_cols,\n",
    "                                    img_channels=img_channels,\n",
    "                                    nb_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == '3d-densenet-modified':\n",
    "                    model = densenet_3d_modified_model(img_rows=img_rows,\n",
    "                                                       img_cols=img_cols,\n",
    "                                                       img_channels_list=input_sizes,\n",
    "                                                       nb_classes=num_classes,\n",
    "                                                       num_dense_blocks=3,\n",
    "                                                       growth_rate=32,\n",
    "                                                       num_1x1_convs=0,\n",
    "                                                       first_conv_filters=64,\n",
    "                                                       first_conv_kernel=(5,5,5),\n",
    "                                                       dropout_1=0.5,\n",
    "                                                       dropout_2=0.5,\n",
    "                                                       activation='leaky_relu')\n",
    "                elif hyperparams['model_id'] == '3d-densenet-fusion':\n",
    "                    model = densenet_3d_modified_model(img_rows=img_rows,\n",
    "                                                       img_cols=img_cols,\n",
    "                                                       img_channels_list=input_sizes,\n",
    "                                                       nb_classes=num_classes,\n",
    "                                                       num_dense_blocks=3,\n",
    "                                                       growth_rate=32,\n",
    "                                                       num_1x1_convs=0,\n",
    "                                                       first_conv_filters=64,\n",
    "                                                       first_conv_kernel=(5,5,5),\n",
    "                                                       dropout_1=0.5,\n",
    "                                                       dropout_2=0.5,\n",
    "                                                       activation='leaky_relu')\n",
    "                elif hyperparams['model_id'] == '3d-densenet-fusion2':\n",
    "                    model = densenet_3d_fusion_model2(img_rows=img_rows,\n",
    "                                                     img_cols=img_cols,\n",
    "                                                     img_channels_list=[\n",
    "                                                            len(hs_channels),\n",
    "                                                            len(lidar_ms_channels),\n",
    "                                                            len(lidar_ndsm_channels),\n",
    "                                                            len(vhr_rgb_channels),\n",
    "                                                     ],\n",
    "                                                     nb_classes=num_classes,\n",
    "                                                     num_dense_blocks=3)\n",
    "                elif hyperparams['model_id'] == '3d-densenet-fusion3':\n",
    "                    model = densenet_3d_fusion_model3(img_rows=img_rows,\n",
    "                                                     img_cols=img_cols,\n",
    "                                                     img_channels_list=[\n",
    "                                                            len(hs_channels),\n",
    "                                                            len(lidar_ms_channels),\n",
    "                                                            len(lidar_ndsm_channels),\n",
    "                                                            len(vhr_rgb_channels),\n",
    "                                                     ],\n",
    "                                                     nb_classes=num_classes,\n",
    "                                                     num_dense_blocks=3)\n",
    "                # elif hyperparams['model_id'] == '3d-densenet-fusion':\n",
    "                #     model = densenet_3d_fusion_model(img_rows=img_rows,\n",
    "                #                                      img_cols=img_cols,\n",
    "                #                                      img_channels_1=len(vhr_rgb_channels),\n",
    "                #                                      img_channels_2=len(lidar_ms_channels) + len(lidar_ndsm_channels),\n",
    "                #                                      img_channels_3=len(hs_channels),\n",
    "                #                                      nb_classes=num_classes,\n",
    "                #                                      num_dense_blocks=3)\n",
    "                # elif hyperparams['model_id'] == '3d-densenet-fusion2':\n",
    "                #     model = densenet_3d_fusion_model2(img_rows=img_rows,\n",
    "                #                                      img_cols=img_cols,\n",
    "                #                                      img_channels_1=len(vhr_rgb_channels),\n",
    "                #                                      img_channels_2=len(lidar_ms_channels) + len(lidar_ndsm_channels),\n",
    "                #                                      img_channels_3=len(hs_channels),\n",
    "                #                                      nb_classes=num_classes,\n",
    "                #                                      num_dense_blocks=3)\n",
    "                # elif hyperparams['model_id'] == '3d-densenet-fusion3':\n",
    "                #     model = densenet_3d_fusion_model3(img_rows=img_rows,\n",
    "                #                                      img_cols=img_cols,\n",
    "                #                                      img_channels_1=len(vhr_rgb_channels),\n",
    "                #                                      img_channels_2=len(lidar_ms_channels) + len(lidar_ndsm_channels),\n",
    "                #                                      img_channels_3=len(hs_channels),\n",
    "                #                                      nb_classes=num_classes,\n",
    "                #                                      num_dense_blocks=3)\n",
    "                # elif hyperparams['model_id'] == '3d-densenet-fusion4':\n",
    "                #     model = densenet_3d_fusion_model4(img_rows=img_rows,\n",
    "                #                                      img_cols=img_cols,\n",
    "                #                                      img_channels_1=len(vhr_rgb_channels),\n",
    "                #                                      img_channels_2=len(lidar_ms_channels) + len(lidar_ndsm_channels),\n",
    "                #                                      img_channels_3=len(hs_channels),\n",
    "                #                                      nb_classes=num_classes,\n",
    "                #                                      num_dense_blocks=3)\n",
    "                elif hyperparams['model_id'] == '2d-cnn':\n",
    "                    model = cnn_2d_model(img_rows=img_rows,\n",
    "                                    img_cols=img_cols,\n",
    "                                    img_channels=img_channels,\n",
    "                                    nb_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == '3d-cnn':\n",
    "                    model = cnn_3d_model(img_rows=img_rows,\n",
    "                                    img_cols=img_cols,\n",
    "                                    img_channels=img_channels,\n",
    "                                    nb_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == 'cnn-baseline':\n",
    "                    filter_size = patch_size // 2 + 1\n",
    "                    num_filters = img_channels * 2\n",
    "                    model = baseline_cnn_model(img_rows=img_rows,\n",
    "                                            img_cols=img_cols,\n",
    "                                            img_channels=img_channels,\n",
    "                                            patch_size=filter_size,\n",
    "                                            nb_filters=num_filters,\n",
    "                                            nb_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == 'nin':\n",
    "                    model = nin_model(img_rows=img_rows,\n",
    "                                    img_cols=img_cols,\n",
    "                                    img_channels=img_channels,\n",
    "                                    num_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == 'fusion-fcn':\n",
    "                    branch_1_shape = (img_rows, img_cols,\n",
    "                                len(lidar_ms_channels) + len(vhr_rgb_channels))\n",
    "                    branch_2_shape = (img_rows, img_cols,\n",
    "                                len(lidar_ndsm_channels))\n",
    "                    branch_3_shape = (img_rows, img_cols, len(hs_channels))\n",
    "                    model = fusion_fcn_model(\n",
    "                                    branch_1_shape=branch_1_shape,\n",
    "                                    branch_2_shape=branch_2_shape,\n",
    "                                    branch_3_shape=branch_3_shape,\n",
    "                                    nb_classes=num_classes)\n",
    "                elif hyperparams['model_id'] == 'fusion-fcn-v2':\n",
    "                    branch_1_shape = (img_rows, img_cols,\n",
    "                                len(lidar_ms_channels) + len(vhr_rgb_channels))\n",
    "                    branch_2_shape = (img_rows, img_cols,\n",
    "                                len(lidar_ndsm_channels))\n",
    "                    branch_3_shape = (img_rows, img_cols, len(hs_channels))\n",
    "                    model = fusion_fcn_v2_model(\n",
    "                                    branch_1_shape=branch_1_shape,\n",
    "                                    branch_2_shape=branch_2_shape,\n",
    "                                    branch_3_shape=branch_3_shape,\n",
    "                                    nb_classes=num_classes)\n",
    "                else:\n",
    "                    print('<!> No model specified, defaulting to 3d-densenet <!>')\n",
    "                    model = densenet_3d_model(img_rows=img_rows,\n",
    "                                    img_cols=img_cols,\n",
    "                                    img_channels=img_channels,\n",
    "                                    nb_classes=num_classes)\n",
    "\n",
    "                # Record model name for output\n",
    "                experiment_data['model'] = model.name\n",
    "                per_class_data['model'] = model.name\n",
    "                if per_class_selected_bands is not None:\n",
    "                    per_class_selected_bands['model'] = model.name\n",
    "\n",
    "                if hyperparams['restore'] is not None:\n",
    "                    print(f'Restoring {model.name} weights from {hyperparams[\"restore\"]}')\n",
    "                    model.load_weights(hyperparams['restore'])\n",
    "\n",
    "\n",
    "                print('-------------------------------------------------------------------')\n",
    "                print()\n",
    "\n",
    "                if not hyperparams['predict_only'] or hyperparams['predict_only'] is None:\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print('TRAIN MODEL')\n",
    "                    print('-------------------------------------------------------------------')\n",
    "\n",
    "                    # Run experiment on model\n",
    "                    model, model_train_time = train_model(model=model,\n",
    "                                                        train_dataset=train_dataset,\n",
    "                                                        val_dataset=val_dataset,\n",
    "                                                        iteration=iteration,\n",
    "                                                        **hyperparams)\n",
    "                else:\n",
    "                    model_train_time = None\n",
    "\n",
    "                print('-------------------------------------------------------------------')\n",
    "                print('TEST MODEL')\n",
    "                print('-------------------------------------------------------------------')\n",
    "\n",
    "                pred_test, model_test_time = test_model(model=model,\n",
    "                                                        test_dataset=test_dataset,\n",
    "                                                        **hyperparams)\n",
    "\n",
    "                if not hyperparams['skip_data_postprocessing']:\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print('POSTPROCESS THE TEST RESULTS')\n",
    "                    print('-------------------------------------------------------------------')\n",
    "\n",
    "                    # Check whether pred_test is the right size\n",
    "                    print(f'pred_test shape: {pred_test.shape}')\n",
    "                    print(f'pred_test size:  {pred_test.size}')\n",
    "                    print(f'test_gt size:    {test_gt.size}')\n",
    "                    if pred_test.size != test_gt.size:\n",
    "                        print('Error! pred_test and test_gt do not have same number of elements!')\n",
    "                        print(f'       pred_test delta: {pred_test.size - test_gt.size} more elements')\n",
    "\n",
    "                    # Reshape pred_test to original gt image size so that\n",
    "                    # postprocessing can occur\n",
    "                    pred_test = np.reshape(pred_test, test_gt.shape)\n",
    "                    print(f'reshaped pred_test shape: {pred_test.shape}')\n",
    "                    print(f'test_gt shape:            {test_gt.shape}')\n",
    "\n",
    "                    pred_test = postprocess_data(pred_test, **hyperparams)\n",
    "                    print('-------------------------------------------------------------------')\n",
    "                    print()\n",
    "\n",
    "                    # Remove ignored labels from target and predicted data\n",
    "                    target_test, pred_test = filter_pred_results(test_gt, pred_test, ignored_labels)\n",
    "\n",
    "\n",
    "\n",
    "                # Calculate the model performance statistics\n",
    "                experiment_results = calculate_model_statistics(pred_test, target_test, all_class_labels, **hyperparams)\n",
    "                experiment_results.update({\n",
    "                    'experiment_name': experiment_name,\n",
    "                    'model_name': model.name,\n",
    "                    'model_train_time': model_train_time,\n",
    "                    'model_test_time': model_test_time,\n",
    "                })\n",
    "\n",
    "                # Copy results to output data\n",
    "                experiment_data['train_time'] = experiment_results['model_train_time']\n",
    "                experiment_data['test_time'] = experiment_results['model_test_time']\n",
    "                experiment_data['overall_accuracy'] = experiment_results['overall_accuracy']\n",
    "                experiment_data['average_accuracy'] = experiment_results['average_accuracy']\n",
    "                experiment_data['precision_score'] = experiment_results['precision_score']\n",
    "                experiment_data['recall_score'] = experiment_results['recall_score']\n",
    "                experiment_data['cohen_kappa_score'] = experiment_results['cohen_kappa_score']\n",
    "\n",
    "                per_class_data['train_time'] = model_train_time\n",
    "                per_class_data['test_time'] = model_test_time\n",
    "                per_class_data['overall_accuracy'] = experiment_results['overall_accuracy']\n",
    "                per_class_data['average_accuracy'] = experiment_results['average_accuracy']\n",
    "                per_class_data['precision_score'] = experiment_results['precision_score']\n",
    "                per_class_data['recall_score'] = experiment_results['recall_score']\n",
    "                per_class_data['cohen_kappa_score'] = experiment_results['cohen_kappa_score']\n",
    "\n",
    "                for index, acc in enumerate(experiment_results['per_class_accuracies']):\n",
    "                    per_class_data[experiment_results['labels'][index]] = acc\n",
    "\n",
    "                if per_class_selected_bands is not None:\n",
    "                    per_class_selected_bands['overall_accuracy'] = experiment_results['overall_accuracy']\n",
    "                    per_class_selected_bands['average_accuracy'] = experiment_results['average_accuracy']\n",
    "                    per_class_selected_bands['precision_score'] = experiment_results['precision_score']\n",
    "                    per_class_selected_bands['recall_score'] = experiment_results['recall_score']\n",
    "                    per_class_selected_bands['cohen_kappa_score'] = experiment_results['cohen_kappa_score']\n",
    "\n",
    "                # Output experimental results\n",
    "                output_experiment_results(experiment_results)\n",
    "\n",
    "                # Save image of confusion matrix\n",
    "                create_confusion_matrix_plot(experiment_results['confusion_matrix'],\n",
    "                                             all_class_labels,\n",
    "                                             model.name,\n",
    "                                             output_path = output_path,\n",
    "                                             iteration=iteration)\n",
    "\n",
    "                print('-------------------------------------------------------------------')\n",
    "                print()\n",
    "\n",
    "                experiment_data['success'] = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print()\n",
    "            print('###################################################')\n",
    "            print('!!! EXCEPTION OCCURRED !!!')\n",
    "            print('###################################################')\n",
    "            print(f'Exception Type: {type(e)}')\n",
    "            print(f'Exception Line: {e.__traceback__.tb_lineno}')\n",
    "            print(f'Exception Desc: {e}')\n",
    "            print()\n",
    "            print('---------------------------------------------------')\n",
    "            print('** Full Traceback **')\n",
    "            print()\n",
    "            # Print full exception\n",
    "            traceback.print_exc()\n",
    "            print('###################################################')\n",
    "            print()\n",
    "\n",
    "            # Write exception to file\n",
    "            with open(os.path.join(output_path, f'experiment_{iteration+1}_exception.log'),'w') as ef:\n",
    "                ef.write('\\n')\n",
    "                ef.write('###################################################\\n')\n",
    "                ef.write('!!! EXCEPTION OCCURRED !!!\\n')\n",
    "                ef.write('###################################################\\n')\n",
    "                ef.write(f'Exception Type: {type(e)}\\n')\n",
    "                ef.write(f'Exception Line: {e.__traceback__.tb_lineno}\\n')\n",
    "                ef.write(f'Exception Desc: {e}\\n')\n",
    "                ef.write('\\n')\n",
    "                ef.write('---------------------------------------------------\\n')\n",
    "                ef.write('** Full Traceback **\\n')\n",
    "                ef.write('\\n')\n",
    "                # Print full exception\n",
    "                ef.write(f'{traceback.format_exc()}\\n')\n",
    "                ef.write('###################################################\\n')\n",
    "                ef.write('\\n')\n",
    "\n",
    "            print(f'Experiment #{iteration+1} crashed and thus failed!')\n",
    "\n",
    "        experiment_data_list.append(experiment_data)\n",
    "        per_class_data_lists[dataset_choice].append(per_class_data)\n",
    "        if per_class_selected_bands is not None:\n",
    "            per_class_selected_band_lists[dataset_choice].append(per_class_selected_bands)\n",
    "\n",
    "        print()\n",
    "        print('-------------------------------------------------------------------')\n",
    "        print('SAVING RESULTS...')\n",
    "\n",
    "        experiment_results = pd.DataFrame(experiment_data_list)\n",
    "        experiment_results.set_index('experiment_number', inplace=True)\n",
    "        experiment_results.to_csv(os.path.join(output_path, experiments_results_file))\n",
    "\n",
    "        print('  >>> Experiment results saved!')\n",
    "\n",
    "        for dataset_choice in per_class_data_lists:\n",
    "            if len(per_class_data_lists[dataset_choice]) > 0:\n",
    "                file_name = f'{outfile_prefix}__{dataset_choice}__class_results.csv'\n",
    "                per_class_data_results = pd.DataFrame(per_class_data_lists[dataset_choice])\n",
    "                per_class_data_results.set_index('experiment_number', inplace=True)\n",
    "                per_class_data_results.to_csv(os.path.join(output_path, file_name))\n",
    "                print(f'  >>> {dataset_choice} per-class results saved!')\n",
    "\n",
    "        for dataset_choice in per_class_selected_band_lists:\n",
    "            if len(per_class_selected_band_lists[dataset_choice]) > 0:\n",
    "                file_name = f'{outfile_prefix}__{dataset_choice}__selected_band_results.csv'\n",
    "                per_class_selected_band_results = pd.DataFrame(per_class_selected_band_lists[dataset_choice])\n",
    "                per_class_selected_band_results.set_index('experiment_number', inplace=True)\n",
    "                per_class_selected_band_results.to_csv(os.path.join(output_path, file_name))\n",
    "                print(f'  >>> {dataset_choice} per-class selected band results saved!')\n",
    "\n",
    "        print('RESULTS SAVED!')\n",
    "        print('-------------------------------------------------------------------')\n",
    "\n",
    "        print()\n",
    "        print('*******************************************************')\n",
    "        print(f'<<< EXPERIMENT #{iteration+1}  COMPLETE! >>>')\n",
    "        print('*******************************************************')\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "    print('EXPERIMENTS COMPLETE!')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv7rQP7WlW_3"
   },
   "source": [
    "# 12) Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-8NIAzxlW_3"
   },
   "source": [
    "## 12.1) Set Parameters For Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Yr868K0plW_3"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#@title EXPERIMENT HYPERPARAMETERS\n",
    "################################################################\n",
    "hyperparameters = {}\n",
    "\n",
    "# Name to use for the experiment and output files\n",
    "hyperparameters[\"experiment_name\"] = None\n",
    "\n",
    "# The numerical identifier of this experiment (i.e. the sequence number of this experiment)\n",
    "hyperparameters[\"experiment_number\"] = 1\n",
    "\n",
    "# File path to save the experimental parameters to\n",
    "hyperparameters[\"save_experiment_path\"] = None\n",
    "\n",
    "# Specify CUDA device (-1 learns on CPU)\n",
    "hyperparameters[\"cuda\"] = 0\n",
    "\n",
    "# Number of runs of experiment\n",
    "hyperparameters[\"runs\"] = 1\n",
    "\n",
    "# Path to file containing weights to use for initialization, e.g. a checkpoint\n",
    "hyperparameters[\"restore\"] = None\n",
    "\n",
    "# Path to where output files should be created\n",
    "hyperparameters[\"output_path\"] = \"./\"\n",
    "\n",
    "# The hyperspectral or data fusion dataset to use for experiments\n",
    "hyperparameters[\"dataset\"] = 'grss_dfc_2018'\n",
    "\n",
    "# The path to the dataset directory\n",
    "hyperparameters[\"path_to_dataset\"] = \"../datasets/grss_dfc_2018\"\n",
    "\n",
    "# Reuse the last dataset generator\n",
    "hyperparameters[\"reuse_last_dataset\"] = False\n",
    "\n",
    "# Skip training and only do prediction on the model\n",
    "hyperparameters[\"predict_only\"] = False\n",
    "\n",
    "# Skip the data preprocessing step\n",
    "hyperparameters[\"skip_data_preprocessing\"] = False\n",
    "\n",
    "# Skip the band selection step\n",
    "hyperparameters[\"skip_band_selection\"] = True\n",
    "\n",
    "# Skip the data postprocessing step\n",
    "hyperparameters[\"skip_data_postprocessing\"] = True\n",
    "\n",
    "# The identifier for the machine learning model to use on the dataset\n",
    "# (2d-cnn, 3d-cnn, cnn-baseline,\n",
    "#  2d-densenet, 2d-densenet-multi, 3d-densenet, 3d-densenet-modified,\n",
    "#  3d-densenet-fusion, 3d-densenet-fusion2, 3d-densenet-fusion3, 3d-densenet-fusion4,\n",
    "#  nin, fusion-fcn, fusion-fcn-v2, )\n",
    "hyperparameters[\"model_id\"] = \"3d-densenet\"\n",
    "\n",
    "# Add a branch to the machine learning model, with the branch modalities\n",
    "# as a comma-separated string after the argument (ex. hs,vhr_rgb)\n",
    "# [modalities: hs, lidar_ms, lidar_ndsm, vhr_rgb]\n",
    "hyperparameters['add_branch'] = [\"hs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "V0GonpYylW_3"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#@title TRAINING OPTION PARAMETERS\n",
    "################################################################\n",
    "\n",
    "# Random number generator seed.\n",
    "hyperparameters[\"random_seed\"] = 123\n",
    "\n",
    "# Training epochs\n",
    "hyperparameters[\"epochs\"] = 10\n",
    "\n",
    "# Number of training epochs to pass before learning rate decay\n",
    "hyperparameters[\"epochs_before_decay\"] = 10\n",
    "\n",
    "# Batch size\n",
    "hyperparameters[\"batch_size\"] = 32\n",
    "\n",
    "# Size of the spatial neighborhood [e.g. patch_size X patch_size square]\n",
    "hyperparameters[\"patch_size\"] = 15\n",
    "\n",
    "# Uses the label of the center pixel when training\n",
    "hyperparameters[\"center_pixel\"] = True\n",
    "\n",
    "# The amount of samples set aside for training during validation split\n",
    "hyperparameters[\"train_split\"] = 0.60\n",
    "\n",
    "# The mode by which to split datasets (random, fixed, or disjoint)\n",
    "hyperparameters[\"split_mode\"] = \"fixed\"\n",
    "\n",
    "# Inverse median frequency class balancing\n",
    "hyperparameters[\"class_balancing\"] = False\n",
    "\n",
    "# Number of iterations to run the model for\n",
    "hyperparameters[\"iterations\"] = None\n",
    "\n",
    "# Number of epochs without improvement before stopping training\n",
    "hyperparameters[\"patience\"] = 3\n",
    "\n",
    "# The number of epochs to pass before saving model again\n",
    "hyperparameters[\"model_save_period\"] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9TBw0Pt_lW_3"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#@title MODEL OPTIMIZER PARAMETERS\n",
    "################################################################\n",
    "\n",
    "# The optimizer used by the machine learning model\n",
    "hyperparameters[\"optimizer\"] = \"nadam\"\n",
    "\n",
    "# The model's learning rate\n",
    "hyperparameters[\"lr\"] = 0.00005\n",
    "\n",
    "# The percentage rate at which the model's learning rate decays\n",
    "hyperparameters[\"lr_decay_rate\"] = 0.95\n",
    "\n",
    "# The optimizer's momentum, if applicable\n",
    "hyperparameters[\"momentum\"] = None\n",
    "\n",
    "# The optimizer's epsilon value, if applicable\n",
    "hyperparameters[\"epsilon\"] = None\n",
    "\n",
    "# The optimizer's initial_accumulator_value value, if applicable\n",
    "hyperparameters[\"initial_accumulator_value\"] = None\n",
    "\n",
    "# The optimizer's beta value, if applicable (Ftrl only)\n",
    "hyperparameters[\"beta\"] = None\n",
    "\n",
    "# The optimizer's beta_1 value, if applicable\n",
    "hyperparameters[\"beta_1\"] = None\n",
    "\n",
    "# The optimizer's beta_2 value, if applicable\n",
    "hyperparameters[\"beta_2\"] = None\n",
    "\n",
    "# The optimizer's amsgrad value, if applicable\n",
    "hyperparameters[\"amsgrad\"] = None\n",
    "\n",
    "# The optimizer's rho value, if applicable\n",
    "hyperparameters[\"rho\"] = None\n",
    "\n",
    "# The optimizer's centered value, if applicable\n",
    "hyperparameters[\"centered\"] = None\n",
    "\n",
    "# The optimizer's nesterov value, if applicable\n",
    "hyperparameters[\"nesterov\"] = None\n",
    "\n",
    "# The optimizer's learning_rate_power value, if applicable\n",
    "hyperparameters[\"learning_rate_power\"] = None\n",
    "\n",
    "# The optimizer's l1_regularization_strength value, if applicable\n",
    "hyperparameters[\"l1_regularization_strength\"] = None\n",
    "\n",
    "# The optimizer's l2_regularization_strength value, if applicable\n",
    "hyperparameters[\"l2_regularization_strength\"] = None\n",
    "\n",
    "# The optimizer's l2_shrinkage_regularization_strength value, if applicable\n",
    "hyperparameters[\"l2_shrinkage_regularization_strength\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XGaTQzk-lW_3"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#@title DATA AUGMENTATION PARAMETERS\n",
    "################################################################\n",
    "\n",
    "# Random flips (if patch_size > 1)\n",
    "hyperparameters[\"flip_augmentation\"] = False\n",
    "\n",
    "# Random radiation noise (illumination)\n",
    "hyperparameters[\"radiation_augmentation\"] = False\n",
    "\n",
    "# Random mixes between spectra\n",
    "hyperparameters[\"mixture_augmentation\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "R_By1QAMlW_3"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#@title GRSS DATA FUSION CONTEST 2018 DATASET PARAMETERS\n",
    "################################################################\n",
    "\n",
    "# Load Hyperspectral data for this experiment\n",
    "hyperparameters[\"use_hs_data\"] = True\n",
    "\n",
    "# Load lidar multispectral intensity data for this experiment\n",
    "hyperparameters[\"use_lidar_ms_data\"] = True\n",
    "\n",
    "# Load lidar NDSM data for this experiment\n",
    "hyperparameters[\"use_lidar_ndsm_data\"] = True\n",
    "\n",
    "# Load very high resolution RGB data for this experiment\n",
    "hyperparameters[\"use_vhr_data\"] = True\n",
    "\n",
    "# Load all data sources for this experiment\n",
    "hyperparameters[\"use_all_data\"] = False\n",
    "\n",
    "# Normalize hyperspectral data\n",
    "hyperparameters[\"normalize_hs_data\"] = False\n",
    "\n",
    "# Normalize LiDAR multispectral data\n",
    "hyperparameters[\"normalize_lidar_ms_data\"] = False\n",
    "\n",
    "# Normalize LiDAR NDSM data\n",
    "hyperparameters[\"normalize_lidar_ndsm_data\"] = False\n",
    "\n",
    "# Normalize VHR RGB data\n",
    "hyperparameters[\"normalize_vhr_data\"] = False\n",
    "\n",
    "# Resampling method to use on the grss_dfc_2018 hyperspectral image\n",
    "# (nearest, bilinear, cubic, cubic_spline, lanczos, average, mode,\n",
    "#  gauss, max, min, med, q1, q3, rms)\n",
    "hyperparameters[\"hs_resampling\"] = \"average\"\n",
    "\n",
    "# Resampling method to use on the grss_dfc_2018 LiDAR multispectral image\n",
    "# (nearest, bilinear, cubic, cubic_spline, lanczos, average, mode,\n",
    "#  gauss, max, min, med, q1, q3, rms)\n",
    "hyperparameters[\"lidar_ms_resampling\"] = \"average\"\n",
    "\n",
    "# Resampling method to use on the grss_dfc_2018 LiDAR NDSM image\n",
    "# (nearest, bilinear, cubic, cubic_spline, lanczos, average, mode,\n",
    "#  gauss, max, min, med, q1, q3, rms)\n",
    "hyperparameters[\"lidar_ndsm_resampling\"] = \"average\"\n",
    "\n",
    "# Resampling method to use on the grss_dfc_2018 VHR RGB image\n",
    "# (nearest, bilinear, cubic, cubic_spline, lanczos, average, mode,\n",
    "#  gauss, max, min, med, q1, q3, rms)\n",
    "hyperparameters[\"vhr_resampling\"] = \"cubic_spline\"\n",
    "\n",
    "# Perform data equalization based on layer histogram for grss_dfc_2018 hyperspectral image\n",
    "hyperparameters[\"hs_histogram_equalization\"] = False\n",
    "\n",
    "# Perform data equalization based on layer histogram for grss_dfc_2018 LiDAR multispectral image\n",
    "hyperparameters[\"lidar_ms_histogram_equalization\"] = False\n",
    "\n",
    "# Perform data equalization based on layer histogram for grss_dfc_2018 LiDAR DSM image\n",
    "hyperparameters[\"lidar_dsm_histogram_equalization\"] = False\n",
    "\n",
    "# Perform data equalization based on layer histogram for grss_dfc_2018 LiDAR DEM image\n",
    "hyperparameters[\"lidar_dem_histogram_equalization\"] = False\n",
    "\n",
    "# Perform data equalization based on layer histogram for grss_dfc_2018 LiDAR NDSM image\n",
    "hyperparameters[\"lidar_ndsm_histogram_equalization\"] = False\n",
    "\n",
    "# Perform data equalization based on layer histogram for grss_dfc_2018 VHR RGB image\n",
    "hyperparameters[\"vhr_histogram_equalization\"] = False\n",
    "\n",
    "# Filtering method to use on the grss_dfc_2018 hyperspectral image\n",
    "# (median, gaussian)\n",
    "hyperparameters[\"hs_data_filter\"] = None\n",
    "\n",
    "# Filtering method to use on the grss_dfc_2018 LiDAR multispectral image\n",
    "# (median, gaussian)\n",
    "hyperparameters[\"lidar_ms_data_filter\"] = \"median\"\n",
    "\n",
    "# Filtering method to use on the grss_dfc_2018 LiDAR DSM image\n",
    "# (median, gaussian)\n",
    "hyperparameters[\"lidar_dsm_data_filter\"] = \"gaussian\"\n",
    "\n",
    "# Filtering method to use on the grss_dfc_2018 LiDAR DEM image\n",
    "# (median, gaussian)\n",
    "hyperparameters[\"lidar_dem_data_filter\"] = \"gaussian\"\n",
    "\n",
    "# Filtering method to use on the grss_dfc_2018 VHR RGB image\n",
    "# (median, gaussian)\n",
    "hyperparameters[\"vhr_data_filter\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GlqzboUWlW_4"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "#@title BAND SELECTION PARAMETERS\n",
    "################################################################\n",
    "\n",
    "# The band dimensionality reduction method to be used\n",
    "hyperparameters[\"band_reduction_method\"] = None\n",
    "\n",
    "# The number of components to be used with the band reduction method\n",
    "hyperparameters[\"n_components\"] = None\n",
    "\n",
    "# A list of channels indices for manual band selection (each channel separated by spaces)\n",
    "hyperparameters[\"selected_bands\"] = None\n",
    "\n",
    "# Only perform bands selection on the hyperspectral data\n",
    "hyperparameters[\"select_only_hs_bands\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvLM0OsOlW_4"
   },
   "source": [
    "## 12.2) Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQOLRQ2jHd4R"
   },
   "outputs": [],
   "source": [
    "# Start timing experiments\n",
    "test_harness_start = time.time()\n",
    "\n",
    "# Run the test harness\n",
    "run_test_harness(**hyperparameters)\n",
    "\n",
    "test_harness_end = time.time()\n",
    "test_harness_runtime = datetime.timedelta(seconds=(test_harness_end - test_harness_start))\n",
    "\n",
    "print(f' < Total Test Harness Runtime: {test_harness_runtime} >')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyL3xpwAlW_4"
   },
   "source": [
    "# References and Citations\n",
    "1. Fernandez-Diaz, Juan Carlos, and Ramesh L. Shrestha. Data Collection &amp; Processing Report. 2018 IEEE GRSS Data Fusion Challenge  Fusion of Multispectral LiDAR and Hyperspectral Data, University of Houston, 2017, https://hyperspectral.ee.uh.edu/2018IEEEDocs/DataReport.pdf.\n",
    "\n",
    "2. Saurabh Prasad, Bertrand Le Saux, Naoto Yokoya, Ronny Hansch, December 18, 2020, \"2018 IEEE GRSS Data Fusion Challenge  Fusion of Multispectral LiDAR and Hyperspectral Data\", IEEE Dataport, doi: https://dx.doi.org/10.21227/jnh9-nz89.\n",
    "\n",
    "3. Zhang C, Li G, Du S, et al. Three-dimensional densely connected convolutional network for hyperspectral remote sensing image classification[J]. Journal of Applied Remote Sensing, 2019, 13(1): 016519.\n",
    "\n",
    "4. Leeguandong. Leeguandong/3D-Densenet-for-HSI: PaperThree-Dimensional Densely Connected Convolutional Network for Hyperspectral Remote Sensing Image Classification. GitHub, Journal of Applied Remote Sensing, 13(1), 3 Feb. 2019, https://github.com/leeguandong/3D-DenseNet-for-HSI."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fZceZKzllW_U",
    "Gfd_4UvJlW_W",
    "IRCNnZsUlW_W",
    "NuU02ktqlW_Y",
    "TEGnHgeolW_Z",
    "p_EZWrpClW_Z",
    "Om2S3912lW_a",
    "DRb5YllXlW_a",
    "JtSTIz1RlW_b",
    "JtdrWu4_lW_e",
    "s_TeBG0GlW_f",
    "kP1NPLxNlW_f",
    "QFQOYSe1lW_g",
    "trK_uc8MlW_m",
    "yQGGpaDolW_m",
    "23DZLfpIlW_n",
    "lm3X_DnSlW_o",
    "X3jT4AHqlW_p",
    "5XZGyfWplW_p",
    "blN8c0KelW_q",
    "pGvyd3IalW_q",
    "MCKXAMXblW_q",
    "JNzMcw2flW_r",
    "4oh1noshlW_r",
    "Rh03k6IQlW_s",
    "O5QvARL2lW_s",
    "nHjv8dDLlW_t",
    "-FpFhE_KlW_t",
    "-GKhhyHylW_t",
    "Tu_-QIf8lW_v",
    "BmLPbxtglW_v",
    "Fg8wrwhElW_w",
    "rJctrIFhlW_w",
    "g3NSAp3mlW_x",
    "CU-CyUmClW_x",
    "TDyXsHihlW_y",
    "Saat71NblW_y",
    "zQIeBt6olW_z",
    "4dAEbx0plW_0",
    "Gux0XElRlW_0",
    "g-8NIAzxlW_3",
    "uvLM0OsOlW_4"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
